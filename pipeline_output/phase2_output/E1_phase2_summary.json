{
  "league_code": "E1",
  "pipeline_metrics": {
    "total_components": 5,
    "successful_components": 4,
    "pipeline_success": true,
    "best_model": "ensemble",
    "best_score": 1.0,
    "recommendations": [
      "Phase 2 pipeline completed successfully",
      "Proceed to Phase 3: Validation and Testing"
    ]
  },
  "component_results": {
    "model_architecture": {
      "base_models": {
        "xgboost": "XGBClassifier(base_score=None, booster=None, callbacks=None,\n              colsample_bylevel=None, colsample_bynode=None,\n              colsample_bytree=0.8, device=None, early_stopping_rounds=None,\n              enable_categorical=False, eval_metric='mlogloss',\n              feature_types=None, feature_weights=None, gamma=None,\n              grow_policy=None, importance_type=None,\n              interaction_constraints=None, learning_rate=0.05, max_bin=None,\n              max_cat_threshold=None, max_cat_to_onehot=None,\n              max_delta_step=None, max_depth=6, max_leaves=None,\n              min_child_weight=3, missing=nan, monotone_constraints=None,\n              multi_strategy=None, n_estimators=200, n_jobs=None,\n              num_parallel_tree=None, ...)",
        "lightgbm": "LGBMClassifier(bagging_fraction=0.8, bagging_freq=5, feature_fraction=0.8,\n               lambda_l1=0.1, lambda_l2=1.0, learning_rate=0.05, max_depth=6,\n               metric='multi_logloss', min_data_in_leaf=20, n_estimators=200,\n               num_class=3, num_leaves=50, objective='multiclass',\n               random_state=42, verbose=-1)",
        "random_forest": "RandomForestClassifier(class_weight='balanced', max_depth=10,\n                       min_samples_leaf=2, min_samples_split=5,\n                       random_state=42)",
        "logistic_regression": "LogisticRegression(class_weight='balanced', max_iter=1000, random_state=42)"
      },
      "training_results": {
        "xgboost": {
          "model": null,
          "success": false,
          "error": "XGBClassifier.fit() got an unexpected keyword argument 'early_stopping_rounds'"
        },
        "lightgbm": {
          "model": "LGBMClassifier(bagging_fraction=0.8, bagging_freq=5, feature_fraction=0.8,\n               lambda_l1=0.1, lambda_l2=1.0, learning_rate=0.05, max_depth=6,\n               metric='multi_logloss', min_data_in_leaf=20, n_estimators=200,\n               num_class=3, num_leaves=50, objective='multiclass',\n               random_state=42, verbose=-1)",
          "train_accuracy": 1.0,
          "val_accuracy": 1.0,
          "train_logloss": 0.001691006074833698,
          "val_logloss": 0.0016853465085957856,
          "success": true
        },
        "random_forest": {
          "model": "RandomForestClassifier(class_weight='balanced', max_depth=10,\n                       min_samples_leaf=2, min_samples_split=5,\n                       random_state=42)",
          "train_accuracy": 1.0,
          "val_accuracy": 1.0,
          "train_logloss": 0.02071131740615101,
          "val_logloss": 0.03281625546475826,
          "success": true
        },
        "logistic_regression": {
          "model": "LogisticRegression(class_weight='balanced', max_iter=1000, random_state=42)",
          "train_accuracy": 1.0,
          "val_accuracy": 1.0,
          "train_logloss": 0.003896055205470742,
          "val_logloss": 0.005840027222712557,
          "success": true
        }
      },
      "ensemble_model": "VotingClassifier(estimators=[('lightgbm',\n                              LGBMClassifier(bagging_fraction=0.8,\n                                             bagging_freq=5,\n                                             feature_fraction=0.8,\n                                             lambda_l1=0.1, lambda_l2=1.0,\n                                             learning_rate=0.05, max_depth=6,\n                                             metric='multi_logloss',\n                                             min_data_in_leaf=20,\n                                             n_estimators=200, num_class=3,\n                                             num_leaves=50,\n                                             objective='multiclass',\n                                             random_state=42, verbose=-1)),\n                             ('random_forest',\n                              RandomForestClassifier(class_weight='balanced',\n                                                     max_depth=10,\n                                                     min_samples_leaf=2,\n                                                     min_samples_split=5,\n                                                     random_state=42)),\n                             ('logistic_regression',\n                              LogisticRegression(class_weight='balanced',\n                                                 max_iter=1000,\n                                                 random_state=42))],\n                 voting='soft')",
      "ensemble_results": {
        "model": "VotingClassifier(estimators=[('lightgbm',\n                              LGBMClassifier(bagging_fraction=0.8,\n                                             bagging_freq=5,\n                                             feature_fraction=0.8,\n                                             lambda_l1=0.1, lambda_l2=1.0,\n                                             learning_rate=0.05, max_depth=6,\n                                             metric='multi_logloss',\n                                             min_data_in_leaf=20,\n                                             n_estimators=200, num_class=3,\n                                             num_leaves=50,\n                                             objective='multiclass',\n                                             random_state=42, verbose=-1)),\n                             ('random_forest',\n                              RandomForestClassifier(class_weight='balanced',\n                                                     max_depth=10,\n                                                     min_samples_leaf=2,\n                                                     min_samples_split=5,\n                                                     random_state=42)),\n                             ('logistic_regression',\n                              LogisticRegression(class_weight='balanced',\n                                                 max_iter=1000,\n                                                 random_state=42))],\n                 voting='soft')",
        "train_accuracy": 1.0,
        "val_accuracy": 1.0,
        "train_logloss": 0.008679161968775422,
        "val_logloss": 0.01323423233117615,
        "success": true
      },
      "calibrated_models": {
        "lightgbm": "CalibratedClassifierCV(cv=3,\n                       estimator=LGBMClassifier(bagging_fraction=0.8,\n                                                bagging_freq=5,\n                                                feature_fraction=0.8,\n                                                lambda_l1=0.1, lambda_l2=1.0,\n                                                learning_rate=0.05, max_depth=6,\n                                                metric='multi_logloss',\n                                                min_data_in_leaf=20,\n                                                n_estimators=200, num_class=3,\n                                                num_leaves=50,\n                                                objective='multiclass',\n                                                random_state=42, verbose=-1),\n                       method='isotonic')",
        "random_forest": "CalibratedClassifierCV(cv=3,\n                       estimator=RandomForestClassifier(class_weight='balanced',\n                                                        max_depth=10,\n                                                        min_samples_leaf=2,\n                                                        min_samples_split=5,\n                                                        random_state=42),\n                       method='isotonic')",
        "logistic_regression": "CalibratedClassifierCV(cv=3,\n                       estimator=LogisticRegression(class_weight='balanced',\n                                                    max_iter=1000,\n                                                    random_state=42),\n                       method='isotonic')",
        "ensemble": "CalibratedClassifierCV(cv=3,\n                       estimator=VotingClassifier(estimators=[('lightgbm',\n                                                               LGBMClassifier(bagging_fraction=0.8,\n                                                                              bagging_freq=5,\n                                                                              feature_fraction=0.8,\n                                                                              lambda_l1=0.1,\n                                                                              lambda_l2=1.0,\n                                                                              learning_rate=0.05,\n                                                                              max_depth=6,\n                                                                              metric='multi_logloss',\n                                                                              min_data_in_leaf=20,\n                                                                              n_estimators=200,\n                                                                              num_class=3,\n                                                                              num_leaves=50,\n                                                                              objective='multiclass',\n                                                                              random_state=42,\n                                                                              verbose=-1)),\n                                                              ('random_forest',\n                                                               RandomForestClassifier(class_weight='balanced',\n                                                                                      max_depth=10,\n                                                                                      min_samples_leaf=2,\n                                                                                      min_samples_split=5,\n                                                                                      random_state=42)),\n                                                              ('logistic_regression',\n                                                               LogisticRegression(class_weight='balanced',\n                                                                                  max_iter=1000,\n                                                                                  random_state=42))],\n                                                  voting='soft'),\n                       method='isotonic')"
      },
      "evaluation_results": {
        "xgboost": {
          "error": "need to call fit or load_model beforehand"
        },
        "lightgbm": {
          "accuracy": 1.0,
          "log_loss": 0.0016979705812201252,
          "predictions": "[1 2 0 2 2 1 2 2 2 1 0 2 1 0 0 2 2 1 2 2 0 1 2 0 2 0 1 2 1 0 0 2 2 2 2 2 1\n 0 2 0 2 1 2 2 0 2 2 1 0 2 2 1 1 0 2 2 0 1 0 2 1 2 0 1 2 1 1 1 1 2 2 0 2 2\n 1 2 1 1 0 2 2 1 2 0 2 1 1 0 1 2 1 1 1 0 2 2 2 1 2 2 0 0 2 2 1 0 1 1 1 2 0\n 1 0 2]",
          "probabilities": "[[8.75935590e-04 9.98233431e-01 8.90633577e-04]\n [8.12518249e-04 8.07554169e-04 9.98379928e-01]\n [9.98313738e-01 8.33271383e-04 8.52990932e-04]\n [8.12500726e-04 8.29103460e-04 9.98358396e-01]\n [8.12500726e-04 8.29103460e-04 9.98358396e-01]\n [8.75935590e-04 9.98233431e-01 8.90633577e-04]\n [8.12500726e-04 8.29103460e-04 9.98358396e-01]\n [8.12318309e-04 1.05343066e-03 9.98134251e-01]\n [8.12518249e-04 8.07554169e-04 9.98379928e-01]\n [8.57716335e-04 9.98270175e-01 8.72108607e-04]\n [9.98291522e-01 8.55506341e-04 8.52971950e-04]\n [8.12514905e-04 8.11666877e-04 9.98375818e-01]\n [8.75935590e-04 9.98233431e-01 8.90633577e-04]\n [9.98309498e-01 8.37514954e-04 8.52987310e-04]\n [9.98313738e-01 8.33271383e-04 8.52990932e-04]\n [8.12518249e-04 8.07554169e-04 9.98379928e-01]\n [8.12514905e-04 8.11666877e-04 9.98375818e-01]\n [8.57716335e-04 9.98270175e-01 8.72108607e-04]\n [8.12514905e-04 8.11666877e-04 9.98375818e-01]\n [8.12514905e-04 8.11666877e-04 9.98375818e-01]\n [9.98309498e-01 8.37514954e-04 8.52987310e-04]\n [8.75935590e-04 9.98233431e-01 8.90633577e-04]\n [8.12518249e-04 8.07554169e-04 9.98379928e-01]\n [9.98309498e-01 8.37514954e-04 8.52987310e-04]\n [8.12514905e-04 8.11666877e-04 9.98375818e-01]\n [9.98313738e-01 8.33271383e-04 8.52990932e-04]\n [8.57716335e-04 9.98270175e-01 8.72108607e-04]\n [8.12518249e-04 8.07554169e-04 9.98379928e-01]\n [8.57716335e-04 9.98270175e-01 8.72108607e-04]\n [9.98309498e-01 8.37514954e-04 8.52987310e-04]\n [9.98313738e-01 8.33271383e-04 8.52990932e-04]\n [8.12500726e-04 8.29103460e-04 9.98358396e-01]\n [8.12514905e-04 8.11666877e-04 9.98375818e-01]\n [8.12514905e-04 8.11666877e-04 9.98375818e-01]\n [8.12514905e-04 8.11666877e-04 9.98375818e-01]\n [8.12514905e-04 8.11666877e-04 9.98375818e-01]\n [8.57716335e-04 9.98270175e-01 8.72108607e-04]\n [9.98313738e-01 8.33271383e-04 8.52990932e-04]\n [8.12518249e-04 8.07554169e-04 9.98379928e-01]\n [9.98313738e-01 8.33271383e-04 8.52990932e-04]\n [8.12518249e-04 8.07554169e-04 9.98379928e-01]\n [8.57716335e-04 9.98270175e-01 8.72108607e-04]\n [8.12518249e-04 8.07554169e-04 9.98379928e-01]\n [8.12318309e-04 1.05343066e-03 9.98134251e-01]\n [9.98313738e-01 8.33271383e-04 8.52990932e-04]\n [8.12518249e-04 8.07554169e-04 9.98379928e-01]\n [8.12500726e-04 8.29103460e-04 9.98358396e-01]\n [8.57716335e-04 9.98270175e-01 8.72108607e-04]\n [9.98291522e-01 8.55506341e-04 8.52971950e-04]\n [8.12514905e-04 8.11666877e-04 9.98375818e-01]\n [8.12500726e-04 8.29103460e-04 9.98358396e-01]\n [8.57716335e-04 9.98270175e-01 8.72108607e-04]\n [8.57716335e-04 9.98270175e-01 8.72108607e-04]\n [9.98313738e-01 8.33271383e-04 8.52990932e-04]\n [8.12318309e-04 1.05343066e-03 9.98134251e-01]\n [8.12518249e-04 8.07554169e-04 9.98379928e-01]\n [9.98313738e-01 8.33271383e-04 8.52990932e-04]\n [8.57716335e-04 9.98270175e-01 8.72108607e-04]\n [9.98309498e-01 8.37514954e-04 8.52987310e-04]\n [8.12518249e-04 8.07554169e-04 9.98379928e-01]\n [8.75935590e-04 9.98233431e-01 8.90633577e-04]\n [8.12500726e-04 8.29103460e-04 9.98358396e-01]\n [9.98291522e-01 8.55506341e-04 8.52971950e-04]\n [1.22073351e-03 9.97888940e-01 8.90326219e-04]\n [8.12518249e-04 8.07554169e-04 9.98379928e-01]\n [8.75935590e-04 9.98233431e-01 8.90633577e-04]\n [8.57716335e-04 9.98270175e-01 8.72108607e-04]\n [8.57716335e-04 9.98270175e-01 8.72108607e-04]\n [8.57716335e-04 9.98270175e-01 8.72108607e-04]\n [8.12500726e-04 8.29103460e-04 9.98358396e-01]\n [8.12518249e-04 8.07554169e-04 9.98379928e-01]\n [9.98313738e-01 8.33271383e-04 8.52990932e-04]\n [8.12518249e-04 8.07554169e-04 9.98379928e-01]\n [8.12514905e-04 8.11666877e-04 9.98375818e-01]\n [9.32226105e-04 9.98119905e-01 9.47868633e-04]\n [8.12514905e-04 8.11666877e-04 9.98375818e-01]\n [8.57716335e-04 9.98270175e-01 8.72108607e-04]\n [8.75935590e-04 9.98233431e-01 8.90633577e-04]\n [9.98313738e-01 8.33271383e-04 8.52990932e-04]\n [8.12518249e-04 8.07554169e-04 9.98379928e-01]\n [8.12518249e-04 8.07554169e-04 9.98379928e-01]\n [8.75935590e-04 9.98233431e-01 8.90633577e-04]\n [8.12318309e-04 1.05343066e-03 9.98134251e-01]\n [9.98313738e-01 8.33271383e-04 8.52990932e-04]\n [8.12518249e-04 8.07554169e-04 9.98379928e-01]\n [1.22073351e-03 9.97888940e-01 8.90326219e-04]\n [8.57716335e-04 9.98270175e-01 8.72108607e-04]\n [9.98309498e-01 8.37514954e-04 8.52987310e-04]\n [8.57716335e-04 9.98270175e-01 8.72108607e-04]\n [8.12500726e-04 8.29103460e-04 9.98358396e-01]\n [8.57716335e-04 9.98270175e-01 8.72108607e-04]\n [8.57716335e-04 9.98270175e-01 8.72108607e-04]\n [8.57716335e-04 9.98270175e-01 8.72108607e-04]\n [9.98309498e-01 8.37514954e-04 8.52987310e-04]\n [8.12514905e-04 8.11666877e-04 9.98375818e-01]\n [8.12518249e-04 8.07554169e-04 9.98379928e-01]\n [8.12514905e-04 8.11666877e-04 9.98375818e-01]\n [8.57716335e-04 9.98270175e-01 8.72108607e-04]\n [8.12500726e-04 8.29103460e-04 9.98358396e-01]\n [8.12514905e-04 8.11666877e-04 9.98375818e-01]\n [9.98309498e-01 8.37514954e-04 8.52987310e-04]\n [9.98313738e-01 8.33271383e-04 8.52990932e-04]\n [8.12435362e-04 9.09483992e-04 9.98278081e-01]\n [8.12514905e-04 8.11666877e-04 9.98375818e-01]\n [8.57716335e-04 9.98270175e-01 8.72108607e-04]\n [9.98313738e-01 8.33271383e-04 8.52990932e-04]\n [8.57716335e-04 9.98270175e-01 8.72108607e-04]\n [8.57716335e-04 9.98270175e-01 8.72108607e-04]\n [8.57716335e-04 9.98270175e-01 8.72108607e-04]\n [8.12318309e-04 1.05343066e-03 9.98134251e-01]\n [9.98291522e-01 8.55506341e-04 8.52971950e-04]\n [8.75935590e-04 9.98233431e-01 8.90633577e-04]\n [9.98313738e-01 8.33271383e-04 8.52990932e-04]\n [8.12518249e-04 8.07554169e-04 9.98379928e-01]]"
        },
        "random_forest": {
          "accuracy": 1.0,
          "log_loss": 0.04099639582662607,
          "predictions": "[1 2 0 2 2 1 2 2 2 1 0 2 1 0 0 2 2 1 2 2 0 1 2 0 2 0 1 2 1 0 0 2 2 2 2 2 1\n 0 2 0 2 1 2 2 0 2 2 1 0 2 2 1 1 0 2 2 0 1 0 2 1 2 0 1 2 1 1 1 1 2 2 0 2 2\n 1 2 1 1 0 2 2 1 2 0 2 1 1 0 1 2 1 1 1 0 2 2 2 1 2 2 0 0 2 2 1 0 1 1 1 2 0\n 1 0 2]",
          "probabilities": "[[0.02460465 0.94676035 0.028635  ]\n [0.01501959 0.01       0.97498041]\n [0.99435248 0.         0.00564752]\n [0.00562264 0.01351436 0.980863  ]\n [0.00562264 0.04       0.95437736]\n [0.02460465 0.9596893  0.01570605]\n [0.         0.01818681 0.98181319]\n [0.05260947 0.04730604 0.90008449]\n [0.01962874 0.03       0.95037126]\n [0.00719128 0.99280872 0.        ]\n [0.94664403 0.02240441 0.03095156]\n [0.01022729 0.01915323 0.97061948]\n [0.00464543 0.95729692 0.03805765]\n [0.98460465 0.00539535 0.01      ]\n [0.95719807 0.01       0.03280193]\n [0.00782152 0.         0.99217848]\n [0.01       0.         0.99      ]\n [0.         0.97832173 0.02167827]\n [0.         0.01600806 0.98399194]\n [0.00562264 0.00600806 0.98836929]\n [0.96703532 0.01539535 0.01756933]\n [0.0167221  0.92374457 0.05953333]\n [0.04805331 0.02113846 0.93080823]\n [0.90321823 0.05240441 0.04437736]\n [0.00562264 0.         0.99437736]\n [0.99       0.01       0.        ]\n [0.05299094 0.91052343 0.03648564]\n [0.01       0.01       0.98      ]\n [0.01630573 0.94264063 0.04105364]\n [0.98       0.02       0.        ]\n [0.94526287 0.02908962 0.02564752]\n [0.         0.0075063  0.9924937 ]\n [0.03673874 0.04598685 0.91727442]\n [0.00562264 0.00429395 0.99008341]\n [0.00658321 0.00600806 0.98740872]\n [0.01       0.         0.99      ]\n [0.03373109 0.94509768 0.02117123]\n [0.97       0.         0.03      ]\n [0.01       0.01       0.98      ]\n [0.99       0.01       0.        ]\n [0.00851541 0.02053651 0.97094807]\n [0.02       0.95832173 0.02167827]\n [0.01       0.02231366 0.96768634]\n [0.01204062 0.06486687 0.92309252]\n [0.96281243 0.02438563 0.01280193]\n [0.02587483 0.02600806 0.9481171 ]\n [0.         0.         1.        ]\n [0.01362822 0.95237984 0.03399194]\n [0.97486338 0.02075926 0.00437736]\n [0.         0.01030201 0.98969799]\n [0.01562264 0.0075063  0.97687106]\n [0.03414878 0.95941348 0.00643775]\n [0.         0.9975063  0.0024937 ]\n [0.95904044 0.01609724 0.02486232]\n [0.03362637 0.08272903 0.8836446 ]\n [0.01562264 0.02       0.96437736]\n [0.99       0.01       0.        ]\n [0.01384412 0.92858655 0.05756933]\n [0.96163997 0.0207907  0.01756933]\n [0.         0.         1.        ]\n [0.01       0.9608324  0.0291676 ]\n [0.02961148 0.0216338  0.94875472]\n [0.98       0.02       0.        ]\n [0.0543699  0.80059071 0.14503939]\n [0.         0.         1.        ]\n [0.02       0.96201613 0.01798387]\n [0.         0.99633331 0.00366669]\n [0.         0.99       0.01      ]\n [0.01181061 0.95411405 0.03407535]\n [0.01       0.01429395 0.97570605]\n [0.01562264 0.02582803 0.95854933]\n [0.96583914 0.00369427 0.0304666 ]\n [0.02110883 0.02231366 0.95657751]\n [0.         0.         1.        ]\n [0.00630573 0.97300081 0.02069346]\n [0.00562264 0.02       0.97437736]\n [0.03713037 0.78779009 0.17507954]\n [0.00630573 0.96639273 0.02730154]\n [0.94299799 0.01934887 0.03765314]\n [0.00719807 0.         0.99280193]\n [0.00793961 0.01       0.98206039]\n [0.04169054 0.93025228 0.02805717]\n [0.04362637 0.07349984 0.88287379]\n [0.95019606 0.02       0.02980394]\n [0.00562264 0.01351436 0.980863  ]\n [0.00742543 0.93963257 0.05294201]\n [0.01986407 0.92707129 0.05306464]\n [0.95299094 0.02700906 0.02      ]\n [0.01       0.98       0.01      ]\n [0.         0.01126418 0.98873582]\n [0.00759559 0.97240441 0.02      ]\n [0.         0.9875063  0.0124937 ]\n [0.01       0.98429395 0.00570605]\n [0.96150635 0.02240441 0.01608924]\n [0.         0.         1.        ]\n [0.01344416 0.00600806 0.98054777]\n [0.00562264 0.01       0.98437736]\n [0.00460465 0.98290165 0.0124937 ]\n [0.         0.01       0.99      ]\n [0.00562264 0.00600806 0.98836929]\n [0.950189   0.03076694 0.01904405]\n [0.92299094 0.03700906 0.04      ]\n [0.03790419 0.04021054 0.92188527]\n [0.         0.         1.        ]\n [0.00460465 0.97140341 0.02399194]\n [0.81788937 0.09992786 0.08218276]\n [0.         0.99231366 0.00768634]\n [0.01       0.96       0.03      ]\n [0.02561437 0.91513224 0.05925339]\n [0.02204062 0.06486687 0.91309252]\n [0.94460465 0.02539535 0.03      ]\n [0.00742543 0.93963257 0.05294201]\n [0.9515686  0.01278388 0.03564752]\n [0.01       0.         0.99      ]]"
        },
        "logistic_regression": {
          "accuracy": 1.0,
          "log_loss": 0.005428633659881864,
          "predictions": "[1 2 0 2 2 1 2 2 2 1 0 2 1 0 0 2 2 1 2 2 0 1 2 0 2 0 1 2 1 0 0 2 2 2 2 2 1\n 0 2 0 2 1 2 2 0 2 2 1 0 2 2 1 1 0 2 2 0 1 0 2 1 2 0 1 2 1 1 1 1 2 2 0 2 2\n 1 2 1 1 0 2 2 1 2 0 2 1 1 0 1 2 1 1 1 0 2 2 2 1 2 2 0 0 2 2 1 0 1 1 1 2 0\n 1 0 2]",
          "probabilities": "[[4.13935725e-03 9.94320908e-01 1.53973428e-03]\n [9.49530346e-04 1.00571595e-02 9.88993310e-01]\n [9.99991536e-01 7.26661600e-06 1.19722677e-06]\n [3.49082880e-04 3.99502718e-04 9.99251414e-01]\n [9.28896849e-06 2.03225894e-04 9.99787485e-01]\n [2.91366069e-03 9.95320547e-01 1.76579247e-03]\n [3.93112419e-06 1.13896172e-04 9.99882173e-01]\n [1.75967293e-03 3.62490641e-03 9.94615421e-01]\n [8.21061306e-05 7.39051711e-04 9.99178842e-01]\n [1.62960135e-03 9.96411675e-01 1.95872322e-03]\n [9.99458314e-01 4.08909585e-04 1.32776038e-04]\n [1.56346418e-03 1.13630531e-02 9.87073483e-01]\n [4.15408293e-03 9.92119183e-01 3.72673415e-03]\n [9.94398786e-01 4.00996967e-03 1.59124449e-03]\n [9.94646312e-01 4.37382125e-03 9.79866601e-04]\n [4.70120468e-05 3.51657782e-04 9.99601330e-01]\n [1.96815269e-03 9.29569747e-03 9.88736150e-01]\n [2.18351422e-03 9.92557764e-01 5.25872154e-03]\n [3.25453561e-04 2.54445573e-03 9.97130091e-01]\n [1.25084370e-04 3.42917642e-04 9.99531998e-01]\n [9.90847566e-01 7.03160468e-03 2.12082904e-03]\n [3.43197143e-03 9.92154416e-01 4.41361303e-03]\n [1.44947006e-03 2.82738572e-03 9.95723144e-01]\n [9.91682890e-01 5.55334722e-03 2.76376259e-03]\n [6.82715897e-04 3.76651830e-03 9.95550766e-01]\n [9.95871765e-01 2.98761093e-03 1.14062400e-03]\n [2.59370244e-03 9.95451432e-01 1.95486574e-03]\n [1.61872766e-03 5.55126187e-03 9.92830010e-01]\n [5.85433955e-03 9.91000292e-01 3.14536863e-03]\n [9.89266629e-01 9.34211915e-03 1.39125194e-03]\n [9.99989249e-01 1.05987321e-05 1.51908178e-07]\n [3.63925535e-04 5.98701323e-04 9.99037373e-01]\n [1.12889903e-03 6.53435571e-03 9.92336745e-01]\n [5.22279270e-03 7.75510420e-03 9.87022103e-01]\n [4.65818228e-08 6.56463429e-07 9.99999297e-01]\n [5.15387772e-04 9.27869730e-03 9.90205915e-01]\n [9.49129618e-03 9.85029040e-01 5.47966344e-03]\n [9.93794679e-01 3.91639663e-03 2.28892452e-03]\n [1.68336431e-03 1.43323347e-03 9.96883402e-01]\n [9.99925227e-01 6.80525118e-05 6.72066936e-06]\n [2.67999767e-04 4.23710938e-04 9.99308289e-01]\n [1.70829089e-03 9.96497862e-01 1.79384727e-03]\n [2.17400067e-03 9.97120473e-03 9.87854795e-01]\n [2.62166194e-03 5.09396198e-03 9.92284376e-01]\n [9.99992395e-01 7.29548659e-06 3.09711030e-07]\n [2.95400405e-04 1.22582815e-03 9.98478771e-01]\n [4.06074843e-06 1.63914790e-04 9.99832024e-01]\n [1.19553224e-03 9.95640353e-01 3.16411519e-03]\n [9.89912581e-01 7.16438808e-03 2.92303061e-03]\n [2.18368733e-04 1.54040369e-03 9.98241228e-01]\n [1.49569037e-03 4.32123506e-03 9.94183075e-01]\n [3.93230016e-03 9.93440012e-01 2.62768813e-03]\n [3.22848096e-03 9.93761768e-01 3.00975068e-03]\n [9.99496076e-01 4.31320666e-04 7.26031494e-05]\n [3.36487572e-03 6.45059518e-03 9.90184529e-01]\n [2.01718687e-03 3.38148411e-03 9.94601329e-01]\n [9.99994206e-01 5.61189110e-06 1.82596075e-07]\n [3.42662372e-03 9.93370958e-01 3.20241842e-03]\n [9.88671162e-01 9.49041456e-03 1.83842360e-03]\n [9.76100886e-04 7.01208448e-03 9.92011815e-01]\n [3.22679934e-03 9.94306329e-01 2.46687144e-03]\n [1.96991120e-03 1.16731484e-02 9.86356940e-01]\n [9.92205303e-01 6.50420173e-03 1.29049576e-03]\n [7.71470042e-03 9.86201792e-01 6.08350791e-03]\n [7.20839931e-05 3.91079581e-04 9.99536836e-01]\n [4.64737709e-03 9.92874273e-01 2.47834972e-03]\n [1.25555467e-03 9.96275355e-01 2.46909042e-03]\n [2.16107382e-03 9.91865746e-01 5.97318021e-03]\n [1.34463034e-03 9.96883101e-01 1.77226838e-03]\n [6.19987103e-04 4.68958292e-03 9.94690430e-01]\n [1.79226452e-04 7.35066363e-04 9.99085707e-01]\n [9.97783722e-01 1.84055890e-03 3.75719036e-04]\n [1.94870407e-03 4.55830021e-03 9.93492996e-01]\n [3.79085340e-06 6.06540420e-05 9.99935555e-01]\n [1.17461377e-03 9.96659796e-01 2.16559031e-03]\n [2.28496089e-03 8.79467918e-03 9.88920360e-01]\n [1.07084799e-03 9.94880648e-01 4.04850405e-03]\n [3.91141304e-03 9.91505015e-01 4.58357207e-03]\n [9.94356035e-01 4.07847483e-03 1.56549047e-03]\n [8.52686146e-04 3.77972871e-03 9.95367585e-01]\n [3.92026352e-03 5.59655425e-03 9.90483182e-01]\n [3.30472493e-03 9.92965213e-01 3.73006195e-03]\n [1.13071340e-03 5.88456174e-03 9.92984725e-01]\n [9.99992404e-01 7.24138202e-06 3.54162516e-07]\n [2.68311648e-04 3.00592979e-04 9.99431095e-01]\n [1.99383312e-03 9.95906942e-01 2.09922487e-03]\n [1.11833176e-02 9.84414267e-01 4.40241499e-03]\n [9.99853370e-01 1.23811469e-04 2.28188434e-05]\n [5.22049948e-03 9.91000338e-01 3.77916244e-03]\n [4.95991960e-05 8.76937792e-04 9.99073463e-01]\n [3.36142445e-03 9.95271310e-01 1.36726522e-03]\n [2.51122595e-03 9.94447113e-01 3.04166072e-03]\n [3.42854695e-03 9.94803748e-01 1.76770529e-03]\n [9.94239581e-01 3.82771181e-03 1.93270742e-03]\n [9.88892918e-07 1.60695874e-05 9.99982942e-01]\n [1.90965250e-04 9.63483623e-04 9.98845551e-01]\n [4.46570352e-03 5.16397568e-03 9.90370321e-01]\n [2.88187544e-03 9.95514156e-01 1.60396837e-03]\n [6.53775767e-06 7.12921074e-05 9.99922170e-01]\n [8.26905641e-06 1.49557963e-04 9.99842173e-01]\n [9.98712113e-01 1.18869507e-03 9.91915008e-05]\n [9.95177614e-01 3.23324031e-03 1.58914547e-03]\n [3.34610601e-05 1.26013350e-04 9.99840526e-01]\n [2.15865822e-05 1.13848823e-04 9.99864565e-01]\n [1.30377130e-03 9.96741888e-01 1.95434064e-03]\n [9.97819645e-01 8.14235609e-04 1.36611900e-03]\n [1.17777543e-03 9.97462403e-01 1.35982113e-03]\n [5.98927478e-03 9.88410277e-01 5.60044842e-03]\n [1.46746002e-02 9.70439010e-01 1.48863900e-02]\n [1.96895504e-03 3.90780950e-03 9.94123235e-01]\n [9.79682665e-01 1.55295680e-02 4.78776688e-03]\n [3.36949463e-03 9.91697007e-01 4.93349800e-03]\n [9.96626807e-01 2.89780424e-03 4.75388362e-04]\n [8.20147778e-05 2.61538363e-04 9.99656447e-01]]"
        },
        "ensemble": {
          "accuracy": 1.0,
          "log_loss": 0.015707224429754873,
          "predictions": "[1 2 0 2 2 1 2 2 2 1 0 2 1 0 0 2 2 1 2 2 0 1 2 0 2 0 1 2 1 0 0 2 2 2 2 2 1\n 0 2 0 2 1 2 2 0 2 2 1 0 2 2 1 1 0 2 2 0 1 0 2 1 2 0 1 2 1 1 1 1 2 2 0 2 2\n 1 2 1 1 0 2 2 1 2 0 2 1 1 0 1 2 1 1 1 0 2 2 2 1 2 2 0 0 2 2 1 0 1 1 1 2 0\n 1 0 2]",
          "probabilities": "[[9.89720350e-03 9.79698330e-01 1.04044669e-02]\n [5.56624084e-03 6.91732936e-03 9.87516430e-01]\n [9.97533284e-01 2.73066426e-04 2.19364963e-03]\n [2.23377472e-03 4.87068968e-03 9.92895536e-01]\n [2.12051008e-03 1.36338102e-02 9.84245680e-01]\n [9.48863798e-03 9.84341192e-01 6.17017048e-03]\n [2.44510295e-04 6.33297131e-03 9.93422518e-01]\n [1.83662305e-02 1.72323048e-02 9.64401465e-01]\n [6.81348161e-03 1.04779601e-02 9.82708558e-01]\n [3.25616225e-03 9.95744708e-01 9.99130109e-04]\n [9.81451411e-01 7.87639967e-03 1.06721897e-02]\n [4.17345220e-03 1.04048293e-02 9.85421718e-01]\n [3.24903966e-03 9.82476610e-01 1.42743505e-02]\n [9.92418443e-01 3.40706522e-03 4.17449181e-03]\n [9.83366737e-01 5.06191797e-03 1.15713445e-02]\n [2.86604553e-03 3.48828791e-04 9.96785126e-01]\n [4.23258415e-03 3.33130068e-03 9.92436115e-01]\n [1.04370544e-03 9.89631075e-01 9.32521984e-03]\n [3.51684441e-04 6.41690828e-03 9.93231407e-01]\n [2.15910855e-03 2.34972891e-03 9.95491163e-01]\n [9.85378259e-01 7.74761022e-03 6.87413038e-03]\n [7.03388967e-03 9.71304239e-01 2.16618710e-02]\n [1.67441275e-02 8.22022392e-03 9.75035649e-01]\n [9.64384337e-01 1.95912122e-02 1.60244507e-02]\n [2.34498572e-03 1.48824096e-03 9.96166773e-01]\n [9.94709199e-01 4.59984786e-03 6.90952874e-04]\n [1.88440804e-02 9.67996196e-01 1.31597237e-02]\n [4.11610996e-03 5.41536349e-03 9.90468527e-01]\n [7.70255804e-03 9.77218217e-01 1.50792247e-02]\n [9.89172840e-01 1.00526654e-02 7.74494295e-04]\n [9.81169316e-01 9.97071592e-03 8.85996786e-03]\n [3.64508432e-04 2.93440104e-03 9.96701091e-01]\n [1.28657450e-02 1.77398028e-02 9.69394452e-01]\n [3.85834466e-03 4.24908563e-03 9.91892570e-01]\n [2.43761898e-03 2.23564185e-03 9.95326739e-01]\n [3.74832918e-03 3.32563396e-03 9.92926037e-01]\n [1.47233304e-02 9.76046817e-01 9.22985229e-03]\n [9.87350171e-01 1.57610976e-03 1.10737197e-02]\n [4.13765551e-03 4.04268735e-03 9.91819657e-01]\n [9.96060353e-01 3.62666172e-03 3.12985097e-04]\n [3.17100477e-03 7.21835090e-03 9.89610644e-01]\n [7.55196433e-03 9.84277774e-01 8.17026176e-03]\n [4.30120097e-03 1.09932326e-02 9.84705566e-01]\n [5.13060937e-03 2.35755978e-02 9.71293793e-01]\n [9.87020221e-01 8.40162047e-03 4.57815890e-03]\n [8.96661176e-03 9.30957375e-03 9.81723814e-01]\n [2.44553503e-04 2.87373121e-04 9.99468073e-01]\n [5.25711797e-03 9.82011309e-01 1.27315726e-02]\n [9.87675952e-01 9.57984163e-03 2.74420668e-03]\n [3.15989498e-04 4.18020697e-03 9.95503804e-01]\n [5.94931055e-03 4.17524562e-03 9.89875444e-01]\n [1.30095591e-02 9.83622406e-01 3.36803483e-03]\n [1.39202768e-03 9.96427265e-01 2.18070685e-03]\n [9.85597448e-01 5.78016460e-03 8.62238697e-03]\n [1.25735999e-02 2.99818629e-02 9.57444537e-01]\n [6.12314354e-03 8.02543757e-03 9.85851419e-01]\n [9.96083346e-01 3.60584818e-03 3.10805740e-04]\n [6.07278304e-03 9.73323745e-01 2.06034722e-02]\n [9.82854342e-01 1.03656631e-02 6.77999523e-03]\n [5.68567704e-04 2.56897102e-03 9.96862461e-01]\n [4.72480048e-03 9.84384153e-01 1.08910469e-02]\n [1.07703304e-02 1.13350519e-02 9.77894618e-01]\n [9.90152398e-01 9.10669296e-03 7.40908901e-04]\n [2.11507224e-02 9.28101034e-01 5.07482440e-02]\n [2.67228739e-04 3.61969391e-04 9.99370802e-01]\n [8.53165973e-03 9.84301378e-01 7.16696260e-03]\n [7.34385587e-04 9.96874132e-01 2.39148202e-03]\n [1.03622530e-03 9.93293159e-01 5.67061577e-03]\n [4.70094703e-03 9.83003626e-01 1.22954271e-02]\n [3.78319562e-03 6.56057854e-03 9.89656226e-01]\n [5.51049006e-03 9.08597377e-03 9.85403536e-01]\n [9.87292897e-01 2.11558636e-03 1.05915170e-02]\n [7.92904503e-03 9.18893113e-03 9.82882024e-01]\n [2.44463538e-04 2.52952872e-04 9.99502584e-01]\n [2.80931612e-03 9.89224779e-01 7.96590494e-03]\n [2.87906739e-03 9.83096125e-03 9.87289971e-01]\n [1.30496075e-02 9.26894822e-01 6.00555704e-02]\n [3.72158254e-03 9.85303825e-01 1.09745929e-02]\n [9.78536618e-01 8.07975973e-03 1.33836218e-02]\n [2.92678533e-03 1.49151910e-03 9.95581696e-01]\n [4.19649166e-03 5.43046095e-03 9.90373047e-01]\n [1.53142900e-02 9.73743743e-01 1.09419673e-02]\n [1.51622124e-02 2.67167866e-02 9.58121001e-01]\n [9.82814764e-01 6.93972468e-03 1.02455111e-02]\n [2.20685179e-03 4.83659444e-03 9.92956554e-01]\n [3.59560819e-03 9.77683368e-01 1.87210235e-02]\n [1.06649969e-02 9.69833095e-01 1.95019079e-02]\n [9.83698733e-01 9.31625069e-03 6.98501660e-03]\n [5.38936719e-03 9.89671356e-01 4.93927652e-03]\n [2.59732985e-04 4.27977369e-03 9.95460493e-01]\n [3.96820475e-03 9.88563151e-01 7.46864411e-03]\n [1.15294268e-03 9.93322380e-01 5.52467687e-03]\n [4.79204968e-03 9.92370476e-01 2.83747476e-03]\n [9.84665941e-01 9.01600042e-03 6.31805907e-03]\n [2.43529551e-04 2.38091387e-04 9.99518379e-01]\n [4.78824376e-03 2.55545891e-03 9.92656297e-01]\n [3.60598160e-03 5.28739342e-03 9.91106625e-01]\n [2.81137623e-03 9.92143178e-01 5.04544608e-03]\n [2.45379173e-04 3.58983223e-03 9.96164789e-01]\n [2.12017011e-03 2.28527568e-03 9.95594554e-01]\n [9.82384337e-01 1.09238390e-02 6.69182445e-03]\n [9.72141461e-01 1.36847455e-02 1.41737934e-02]\n [1.28890826e-02 1.36792376e-02 9.73431680e-01]\n [2.50395448e-04 2.70684466e-04 9.99478920e-01]\n [2.28534152e-03 9.88719677e-01 8.99498108e-03]\n [9.37988284e-01 3.38513429e-02 2.81603728e-02]\n [7.08459176e-04 9.95929933e-01 3.36160788e-03]\n [5.64562562e-03 9.82141336e-01 1.22130385e-02]\n [1.37455230e-02 9.61194994e-01 2.50594834e-02]\n [8.24637374e-03 2.31802137e-02 9.68573413e-01]\n [9.74179736e-01 1.39135980e-02 1.19066659e-02]\n [3.91417439e-03 9.76447768e-01 1.96380574e-02]\n [9.82150413e-01 5.49787359e-03 1.23517133e-02]\n [3.60387233e-03 3.18788985e-04 9.96077339e-01]]"
        },
        "lightgbm_calibrated": {
          "accuracy": 1.0,
          "log_loss": 2.2204460492503136e-16,
          "predictions": "[1 2 0 2 2 1 2 2 2 1 0 2 1 0 0 2 2 1 2 2 0 1 2 0 2 0 1 2 1 0 0 2 2 2 2 2 1\n 0 2 0 2 1 2 2 0 2 2 1 0 2 2 1 1 0 2 2 0 1 0 2 1 2 0 1 2 1 1 1 1 2 2 0 2 2\n 1 2 1 1 0 2 2 1 2 0 2 1 1 0 1 2 1 1 1 0 2 2 2 1 2 2 0 0 2 2 1 0 1 1 1 2 0\n 1 0 2]",
          "probabilities": "[[0. 1. 0.]\n [0. 0. 1.]\n [1. 0. 0.]\n [0. 0. 1.]\n [0. 0. 1.]\n [0. 1. 0.]\n [0. 0. 1.]\n [0. 0. 1.]\n [0. 0. 1.]\n [0. 1. 0.]\n [1. 0. 0.]\n [0. 0. 1.]\n [0. 1. 0.]\n [1. 0. 0.]\n [1. 0. 0.]\n [0. 0. 1.]\n [0. 0. 1.]\n [0. 1. 0.]\n [0. 0. 1.]\n [0. 0. 1.]\n [1. 0. 0.]\n [0. 1. 0.]\n [0. 0. 1.]\n [1. 0. 0.]\n [0. 0. 1.]\n [1. 0. 0.]\n [0. 1. 0.]\n [0. 0. 1.]\n [0. 1. 0.]\n [1. 0. 0.]\n [1. 0. 0.]\n [0. 0. 1.]\n [0. 0. 1.]\n [0. 0. 1.]\n [0. 0. 1.]\n [0. 0. 1.]\n [0. 1. 0.]\n [1. 0. 0.]\n [0. 0. 1.]\n [1. 0. 0.]\n [0. 0. 1.]\n [0. 1. 0.]\n [0. 0. 1.]\n [0. 0. 1.]\n [1. 0. 0.]\n [0. 0. 1.]\n [0. 0. 1.]\n [0. 1. 0.]\n [1. 0. 0.]\n [0. 0. 1.]\n [0. 0. 1.]\n [0. 1. 0.]\n [0. 1. 0.]\n [1. 0. 0.]\n [0. 0. 1.]\n [0. 0. 1.]\n [1. 0. 0.]\n [0. 1. 0.]\n [1. 0. 0.]\n [0. 0. 1.]\n [0. 1. 0.]\n [0. 0. 1.]\n [1. 0. 0.]\n [0. 1. 0.]\n [0. 0. 1.]\n [0. 1. 0.]\n [0. 1. 0.]\n [0. 1. 0.]\n [0. 1. 0.]\n [0. 0. 1.]\n [0. 0. 1.]\n [1. 0. 0.]\n [0. 0. 1.]\n [0. 0. 1.]\n [0. 1. 0.]\n [0. 0. 1.]\n [0. 1. 0.]\n [0. 1. 0.]\n [1. 0. 0.]\n [0. 0. 1.]\n [0. 0. 1.]\n [0. 1. 0.]\n [0. 0. 1.]\n [1. 0. 0.]\n [0. 0. 1.]\n [0. 1. 0.]\n [0. 1. 0.]\n [1. 0. 0.]\n [0. 1. 0.]\n [0. 0. 1.]\n [0. 1. 0.]\n [0. 1. 0.]\n [0. 1. 0.]\n [1. 0. 0.]\n [0. 0. 1.]\n [0. 0. 1.]\n [0. 0. 1.]\n [0. 1. 0.]\n [0. 0. 1.]\n [0. 0. 1.]\n [1. 0. 0.]\n [1. 0. 0.]\n [0. 0. 1.]\n [0. 0. 1.]\n [0. 1. 0.]\n [1. 0. 0.]\n [0. 1. 0.]\n [0. 1. 0.]\n [0. 1. 0.]\n [0. 0. 1.]\n [1. 0. 0.]\n [0. 1. 0.]\n [1. 0. 0.]\n [0. 0. 1.]]"
        },
        "random_forest_calibrated": {
          "accuracy": 1.0,
          "log_loss": 0.0011258946918833164,
          "predictions": "[1 2 0 2 2 1 2 2 2 1 0 2 1 0 0 2 2 1 2 2 0 1 2 0 2 0 1 2 1 0 0 2 2 2 2 2 1\n 0 2 0 2 1 2 2 0 2 2 1 0 2 2 1 1 0 2 2 0 1 0 2 1 2 0 1 2 1 1 1 1 2 2 0 2 2\n 1 2 1 1 0 2 2 1 2 0 2 1 1 0 1 2 1 1 1 0 2 2 2 1 2 2 0 0 2 2 1 0 1 1 1 2 0\n 1 0 2]",
          "probabilities": "[[0.00000000e+00 1.00000000e+00 0.00000000e+00]\n [0.00000000e+00 0.00000000e+00 1.00000000e+00]\n [1.00000000e+00 0.00000000e+00 0.00000000e+00]\n [0.00000000e+00 0.00000000e+00 1.00000000e+00]\n [0.00000000e+00 0.00000000e+00 1.00000000e+00]\n [0.00000000e+00 1.00000000e+00 0.00000000e+00]\n [0.00000000e+00 0.00000000e+00 1.00000000e+00]\n [0.00000000e+00 0.00000000e+00 1.00000000e+00]\n [0.00000000e+00 0.00000000e+00 1.00000000e+00]\n [0.00000000e+00 1.00000000e+00 0.00000000e+00]\n [1.00000000e+00 0.00000000e+00 0.00000000e+00]\n [0.00000000e+00 0.00000000e+00 1.00000000e+00]\n [0.00000000e+00 1.00000000e+00 0.00000000e+00]\n [1.00000000e+00 0.00000000e+00 0.00000000e+00]\n [1.00000000e+00 0.00000000e+00 0.00000000e+00]\n [0.00000000e+00 0.00000000e+00 1.00000000e+00]\n [0.00000000e+00 0.00000000e+00 1.00000000e+00]\n [0.00000000e+00 1.00000000e+00 0.00000000e+00]\n [0.00000000e+00 0.00000000e+00 1.00000000e+00]\n [0.00000000e+00 0.00000000e+00 1.00000000e+00]\n [1.00000000e+00 0.00000000e+00 0.00000000e+00]\n [0.00000000e+00 1.00000000e+00 0.00000000e+00]\n [0.00000000e+00 0.00000000e+00 1.00000000e+00]\n [1.00000000e+00 0.00000000e+00 0.00000000e+00]\n [0.00000000e+00 0.00000000e+00 1.00000000e+00]\n [1.00000000e+00 0.00000000e+00 0.00000000e+00]\n [2.23620252e-02 9.77637975e-01 0.00000000e+00]\n [0.00000000e+00 0.00000000e+00 1.00000000e+00]\n [0.00000000e+00 1.00000000e+00 0.00000000e+00]\n [1.00000000e+00 0.00000000e+00 0.00000000e+00]\n [1.00000000e+00 0.00000000e+00 0.00000000e+00]\n [0.00000000e+00 0.00000000e+00 1.00000000e+00]\n [0.00000000e+00 0.00000000e+00 1.00000000e+00]\n [0.00000000e+00 0.00000000e+00 1.00000000e+00]\n [0.00000000e+00 0.00000000e+00 1.00000000e+00]\n [0.00000000e+00 0.00000000e+00 1.00000000e+00]\n [0.00000000e+00 1.00000000e+00 0.00000000e+00]\n [1.00000000e+00 0.00000000e+00 0.00000000e+00]\n [0.00000000e+00 0.00000000e+00 1.00000000e+00]\n [1.00000000e+00 0.00000000e+00 0.00000000e+00]\n [0.00000000e+00 0.00000000e+00 1.00000000e+00]\n [0.00000000e+00 1.00000000e+00 0.00000000e+00]\n [0.00000000e+00 0.00000000e+00 1.00000000e+00]\n [0.00000000e+00 0.00000000e+00 1.00000000e+00]\n [1.00000000e+00 0.00000000e+00 0.00000000e+00]\n [0.00000000e+00 0.00000000e+00 1.00000000e+00]\n [0.00000000e+00 0.00000000e+00 1.00000000e+00]\n [0.00000000e+00 1.00000000e+00 0.00000000e+00]\n [1.00000000e+00 0.00000000e+00 0.00000000e+00]\n [0.00000000e+00 0.00000000e+00 1.00000000e+00]\n [0.00000000e+00 0.00000000e+00 1.00000000e+00]\n [0.00000000e+00 1.00000000e+00 0.00000000e+00]\n [0.00000000e+00 1.00000000e+00 0.00000000e+00]\n [9.97948354e-01 2.05164623e-03 0.00000000e+00]\n [0.00000000e+00 2.64764686e-02 9.73523531e-01]\n [0.00000000e+00 0.00000000e+00 1.00000000e+00]\n [1.00000000e+00 0.00000000e+00 0.00000000e+00]\n [0.00000000e+00 1.00000000e+00 0.00000000e+00]\n [1.00000000e+00 0.00000000e+00 0.00000000e+00]\n [0.00000000e+00 0.00000000e+00 1.00000000e+00]\n [0.00000000e+00 1.00000000e+00 0.00000000e+00]\n [0.00000000e+00 0.00000000e+00 1.00000000e+00]\n [1.00000000e+00 0.00000000e+00 0.00000000e+00]\n [2.18762293e-04 9.87056119e-01 1.27251192e-02]\n [0.00000000e+00 0.00000000e+00 1.00000000e+00]\n [0.00000000e+00 1.00000000e+00 0.00000000e+00]\n [0.00000000e+00 1.00000000e+00 0.00000000e+00]\n [0.00000000e+00 1.00000000e+00 0.00000000e+00]\n [0.00000000e+00 1.00000000e+00 0.00000000e+00]\n [0.00000000e+00 0.00000000e+00 1.00000000e+00]\n [0.00000000e+00 0.00000000e+00 1.00000000e+00]\n [1.00000000e+00 0.00000000e+00 0.00000000e+00]\n [0.00000000e+00 0.00000000e+00 1.00000000e+00]\n [0.00000000e+00 0.00000000e+00 1.00000000e+00]\n [0.00000000e+00 1.00000000e+00 0.00000000e+00]\n [0.00000000e+00 0.00000000e+00 1.00000000e+00]\n [0.00000000e+00 9.66456831e-01 3.35431691e-02]\n [0.00000000e+00 1.00000000e+00 0.00000000e+00]\n [1.00000000e+00 0.00000000e+00 0.00000000e+00]\n [0.00000000e+00 0.00000000e+00 1.00000000e+00]\n [0.00000000e+00 0.00000000e+00 1.00000000e+00]\n [0.00000000e+00 1.00000000e+00 0.00000000e+00]\n [0.00000000e+00 4.40957251e-03 9.95590427e-01]\n [1.00000000e+00 0.00000000e+00 0.00000000e+00]\n [0.00000000e+00 0.00000000e+00 1.00000000e+00]\n [0.00000000e+00 1.00000000e+00 0.00000000e+00]\n [0.00000000e+00 9.97429598e-01 2.57040151e-03]\n [1.00000000e+00 0.00000000e+00 0.00000000e+00]\n [0.00000000e+00 1.00000000e+00 0.00000000e+00]\n [0.00000000e+00 0.00000000e+00 1.00000000e+00]\n [0.00000000e+00 1.00000000e+00 0.00000000e+00]\n [0.00000000e+00 1.00000000e+00 0.00000000e+00]\n [0.00000000e+00 1.00000000e+00 0.00000000e+00]\n [1.00000000e+00 0.00000000e+00 0.00000000e+00]\n [0.00000000e+00 0.00000000e+00 1.00000000e+00]\n [0.00000000e+00 0.00000000e+00 1.00000000e+00]\n [0.00000000e+00 0.00000000e+00 1.00000000e+00]\n [0.00000000e+00 1.00000000e+00 0.00000000e+00]\n [0.00000000e+00 0.00000000e+00 1.00000000e+00]\n [0.00000000e+00 0.00000000e+00 1.00000000e+00]\n [1.00000000e+00 0.00000000e+00 0.00000000e+00]\n [1.00000000e+00 0.00000000e+00 0.00000000e+00]\n [0.00000000e+00 2.65430225e-03 9.97345698e-01]\n [0.00000000e+00 0.00000000e+00 1.00000000e+00]\n [0.00000000e+00 1.00000000e+00 0.00000000e+00]\n [9.80148472e-01 1.58588934e-02 3.99263469e-03]\n [0.00000000e+00 1.00000000e+00 0.00000000e+00]\n [0.00000000e+00 1.00000000e+00 0.00000000e+00]\n [0.00000000e+00 1.00000000e+00 0.00000000e+00]\n [0.00000000e+00 0.00000000e+00 1.00000000e+00]\n [1.00000000e+00 0.00000000e+00 0.00000000e+00]\n [0.00000000e+00 1.00000000e+00 0.00000000e+00]\n [1.00000000e+00 0.00000000e+00 0.00000000e+00]\n [0.00000000e+00 0.00000000e+00 1.00000000e+00]]"
        },
        "logistic_regression_calibrated": {
          "accuracy": 1.0,
          "log_loss": 0.0006709265595441126,
          "predictions": "[1 2 0 2 2 1 2 2 2 1 0 2 1 0 0 2 2 1 2 2 0 1 2 0 2 0 1 2 1 0 0 2 2 2 2 2 1\n 0 2 0 2 1 2 2 0 2 2 1 0 2 2 1 1 0 2 2 0 1 0 2 1 2 0 1 2 1 1 1 1 2 2 0 2 2\n 1 2 1 1 0 2 2 1 2 0 2 1 1 0 1 2 1 1 1 0 2 2 2 1 2 2 0 0 2 2 1 0 1 1 1 2 0\n 1 0 2]",
          "probabilities": "[[0.         1.         0.        ]\n [0.         0.         1.        ]\n [1.         0.         0.        ]\n [0.         0.         1.        ]\n [0.         0.         1.        ]\n [0.         1.         0.        ]\n [0.         0.         1.        ]\n [0.         0.         1.        ]\n [0.         0.         1.        ]\n [0.         1.         0.        ]\n [1.         0.         0.        ]\n [0.         0.         1.        ]\n [0.         1.         0.        ]\n [1.         0.         0.        ]\n [1.         0.         0.        ]\n [0.         0.         1.        ]\n [0.         0.         1.        ]\n [0.         1.         0.        ]\n [0.         0.         1.        ]\n [0.         0.         1.        ]\n [1.         0.         0.        ]\n [0.         1.         0.        ]\n [0.         0.         1.        ]\n [1.         0.         0.        ]\n [0.         0.         1.        ]\n [1.         0.         0.        ]\n [0.         1.         0.        ]\n [0.         0.         1.        ]\n [0.         1.         0.        ]\n [1.         0.         0.        ]\n [1.         0.         0.        ]\n [0.         0.         1.        ]\n [0.         0.         1.        ]\n [0.         0.         1.        ]\n [0.         0.         1.        ]\n [0.         0.         1.        ]\n [0.03224123 0.96775877 0.        ]\n [1.         0.         0.        ]\n [0.         0.         1.        ]\n [1.         0.         0.        ]\n [0.         0.         1.        ]\n [0.         1.         0.        ]\n [0.         0.         1.        ]\n [0.         0.         1.        ]\n [1.         0.         0.        ]\n [0.         0.         1.        ]\n [0.         0.         1.        ]\n [0.         1.         0.        ]\n [1.         0.         0.        ]\n [0.         0.         1.        ]\n [0.         0.         1.        ]\n [0.01070763 0.98929237 0.        ]\n [0.         1.         0.        ]\n [1.         0.         0.        ]\n [0.         0.         1.        ]\n [0.         0.         1.        ]\n [1.         0.         0.        ]\n [0.         1.         0.        ]\n [1.         0.         0.        ]\n [0.         0.         1.        ]\n [0.         1.         0.        ]\n [0.         0.         1.        ]\n [1.         0.         0.        ]\n [0.         1.         0.        ]\n [0.         0.         1.        ]\n [0.         1.         0.        ]\n [0.         1.         0.        ]\n [0.         1.         0.        ]\n [0.         1.         0.        ]\n [0.         0.         1.        ]\n [0.         0.         1.        ]\n [1.         0.         0.        ]\n [0.         0.         1.        ]\n [0.         0.         1.        ]\n [0.         1.         0.        ]\n [0.         0.         1.        ]\n [0.         1.         0.        ]\n [0.         1.         0.        ]\n [1.         0.         0.        ]\n [0.         0.         1.        ]\n [0.         0.         1.        ]\n [0.         1.         0.        ]\n [0.         0.         1.        ]\n [1.         0.         0.        ]\n [0.         0.         1.        ]\n [0.         1.         0.        ]\n [0.03241096 0.96758904 0.        ]\n [1.         0.         0.        ]\n [0.         1.         0.        ]\n [0.         0.         1.        ]\n [0.         1.         0.        ]\n [0.         1.         0.        ]\n [0.         1.         0.        ]\n [1.         0.         0.        ]\n [0.         0.         1.        ]\n [0.         0.         1.        ]\n [0.         0.         1.        ]\n [0.         1.         0.        ]\n [0.         0.         1.        ]\n [0.         0.         1.        ]\n [1.         0.         0.        ]\n [1.         0.         0.        ]\n [0.         0.         1.        ]\n [0.         0.         1.        ]\n [0.         1.         0.        ]\n [1.         0.         0.        ]\n [0.         1.         0.        ]\n [0.         1.         0.        ]\n [0.         1.         0.        ]\n [0.         0.         1.        ]\n [1.         0.         0.        ]\n [0.         1.         0.        ]\n [1.         0.         0.        ]\n [0.         0.         1.        ]]"
        },
        "ensemble_calibrated": {
          "accuracy": 1.0,
          "log_loss": 0.00025607991992227506,
          "predictions": "[1 2 0 2 2 1 2 2 2 1 0 2 1 0 0 2 2 1 2 2 0 1 2 0 2 0 1 2 1 0 0 2 2 2 2 2 1\n 0 2 0 2 1 2 2 0 2 2 1 0 2 2 1 1 0 2 2 0 1 0 2 1 2 0 1 2 1 1 1 1 2 2 0 2 2\n 1 2 1 1 0 2 2 1 2 0 2 1 1 0 1 2 1 1 1 0 2 2 2 1 2 2 0 0 2 2 1 0 1 1 1 2 0\n 1 0 2]",
          "probabilities": "[[0.00000000e+00 1.00000000e+00 0.00000000e+00]\n [0.00000000e+00 0.00000000e+00 1.00000000e+00]\n [1.00000000e+00 0.00000000e+00 0.00000000e+00]\n [0.00000000e+00 0.00000000e+00 1.00000000e+00]\n [0.00000000e+00 0.00000000e+00 1.00000000e+00]\n [0.00000000e+00 1.00000000e+00 0.00000000e+00]\n [0.00000000e+00 0.00000000e+00 1.00000000e+00]\n [0.00000000e+00 0.00000000e+00 1.00000000e+00]\n [0.00000000e+00 0.00000000e+00 1.00000000e+00]\n [0.00000000e+00 1.00000000e+00 0.00000000e+00]\n [1.00000000e+00 0.00000000e+00 0.00000000e+00]\n [0.00000000e+00 0.00000000e+00 1.00000000e+00]\n [0.00000000e+00 1.00000000e+00 0.00000000e+00]\n [1.00000000e+00 0.00000000e+00 0.00000000e+00]\n [1.00000000e+00 0.00000000e+00 0.00000000e+00]\n [0.00000000e+00 0.00000000e+00 1.00000000e+00]\n [0.00000000e+00 0.00000000e+00 1.00000000e+00]\n [0.00000000e+00 1.00000000e+00 0.00000000e+00]\n [0.00000000e+00 0.00000000e+00 1.00000000e+00]\n [0.00000000e+00 0.00000000e+00 1.00000000e+00]\n [1.00000000e+00 0.00000000e+00 0.00000000e+00]\n [0.00000000e+00 1.00000000e+00 0.00000000e+00]\n [0.00000000e+00 0.00000000e+00 1.00000000e+00]\n [1.00000000e+00 0.00000000e+00 0.00000000e+00]\n [0.00000000e+00 0.00000000e+00 1.00000000e+00]\n [1.00000000e+00 0.00000000e+00 0.00000000e+00]\n [6.35091976e-03 9.93649080e-01 0.00000000e+00]\n [0.00000000e+00 0.00000000e+00 1.00000000e+00]\n [0.00000000e+00 1.00000000e+00 0.00000000e+00]\n [1.00000000e+00 0.00000000e+00 0.00000000e+00]\n [1.00000000e+00 0.00000000e+00 0.00000000e+00]\n [0.00000000e+00 0.00000000e+00 1.00000000e+00]\n [0.00000000e+00 0.00000000e+00 1.00000000e+00]\n [0.00000000e+00 0.00000000e+00 1.00000000e+00]\n [0.00000000e+00 0.00000000e+00 1.00000000e+00]\n [0.00000000e+00 0.00000000e+00 1.00000000e+00]\n [0.00000000e+00 1.00000000e+00 0.00000000e+00]\n [1.00000000e+00 0.00000000e+00 0.00000000e+00]\n [0.00000000e+00 0.00000000e+00 1.00000000e+00]\n [1.00000000e+00 0.00000000e+00 0.00000000e+00]\n [0.00000000e+00 0.00000000e+00 1.00000000e+00]\n [0.00000000e+00 1.00000000e+00 0.00000000e+00]\n [0.00000000e+00 0.00000000e+00 1.00000000e+00]\n [0.00000000e+00 9.07225896e-04 9.99092774e-01]\n [1.00000000e+00 0.00000000e+00 0.00000000e+00]\n [0.00000000e+00 0.00000000e+00 1.00000000e+00]\n [0.00000000e+00 0.00000000e+00 1.00000000e+00]\n [0.00000000e+00 1.00000000e+00 0.00000000e+00]\n [1.00000000e+00 0.00000000e+00 0.00000000e+00]\n [0.00000000e+00 0.00000000e+00 1.00000000e+00]\n [0.00000000e+00 0.00000000e+00 1.00000000e+00]\n [0.00000000e+00 1.00000000e+00 0.00000000e+00]\n [0.00000000e+00 1.00000000e+00 0.00000000e+00]\n [1.00000000e+00 0.00000000e+00 0.00000000e+00]\n [0.00000000e+00 7.16473563e-03 9.92835264e-01]\n [0.00000000e+00 0.00000000e+00 1.00000000e+00]\n [1.00000000e+00 0.00000000e+00 0.00000000e+00]\n [0.00000000e+00 1.00000000e+00 0.00000000e+00]\n [1.00000000e+00 0.00000000e+00 0.00000000e+00]\n [0.00000000e+00 0.00000000e+00 1.00000000e+00]\n [0.00000000e+00 1.00000000e+00 0.00000000e+00]\n [0.00000000e+00 0.00000000e+00 1.00000000e+00]\n [1.00000000e+00 0.00000000e+00 0.00000000e+00]\n [8.06981951e-05 9.97978366e-01 1.94093622e-03]\n [0.00000000e+00 0.00000000e+00 1.00000000e+00]\n [0.00000000e+00 1.00000000e+00 0.00000000e+00]\n [0.00000000e+00 1.00000000e+00 0.00000000e+00]\n [0.00000000e+00 1.00000000e+00 0.00000000e+00]\n [0.00000000e+00 1.00000000e+00 0.00000000e+00]\n [0.00000000e+00 0.00000000e+00 1.00000000e+00]\n [0.00000000e+00 0.00000000e+00 1.00000000e+00]\n [1.00000000e+00 0.00000000e+00 0.00000000e+00]\n [0.00000000e+00 0.00000000e+00 1.00000000e+00]\n [0.00000000e+00 0.00000000e+00 1.00000000e+00]\n [0.00000000e+00 1.00000000e+00 0.00000000e+00]\n [0.00000000e+00 0.00000000e+00 1.00000000e+00]\n [0.00000000e+00 9.92359087e-01 7.64091279e-03]\n [0.00000000e+00 1.00000000e+00 0.00000000e+00]\n [1.00000000e+00 0.00000000e+00 0.00000000e+00]\n [0.00000000e+00 0.00000000e+00 1.00000000e+00]\n [0.00000000e+00 0.00000000e+00 1.00000000e+00]\n [0.00000000e+00 1.00000000e+00 0.00000000e+00]\n [0.00000000e+00 1.25853349e-03 9.98741467e-01]\n [1.00000000e+00 0.00000000e+00 0.00000000e+00]\n [0.00000000e+00 0.00000000e+00 1.00000000e+00]\n [0.00000000e+00 1.00000000e+00 0.00000000e+00]\n [0.00000000e+00 1.00000000e+00 0.00000000e+00]\n [1.00000000e+00 0.00000000e+00 0.00000000e+00]\n [0.00000000e+00 1.00000000e+00 0.00000000e+00]\n [0.00000000e+00 0.00000000e+00 1.00000000e+00]\n [0.00000000e+00 1.00000000e+00 0.00000000e+00]\n [0.00000000e+00 1.00000000e+00 0.00000000e+00]\n [0.00000000e+00 1.00000000e+00 0.00000000e+00]\n [1.00000000e+00 0.00000000e+00 0.00000000e+00]\n [0.00000000e+00 0.00000000e+00 1.00000000e+00]\n [0.00000000e+00 0.00000000e+00 1.00000000e+00]\n [0.00000000e+00 0.00000000e+00 1.00000000e+00]\n [0.00000000e+00 1.00000000e+00 0.00000000e+00]\n [0.00000000e+00 0.00000000e+00 1.00000000e+00]\n [0.00000000e+00 0.00000000e+00 1.00000000e+00]\n [1.00000000e+00 0.00000000e+00 0.00000000e+00]\n [1.00000000e+00 0.00000000e+00 0.00000000e+00]\n [0.00000000e+00 4.87800596e-04 9.99512199e-01]\n [0.00000000e+00 0.00000000e+00 1.00000000e+00]\n [0.00000000e+00 1.00000000e+00 0.00000000e+00]\n [9.97298757e-01 2.70124336e-03 0.00000000e+00]\n [0.00000000e+00 1.00000000e+00 0.00000000e+00]\n [0.00000000e+00 1.00000000e+00 0.00000000e+00]\n [0.00000000e+00 1.00000000e+00 0.00000000e+00]\n [0.00000000e+00 5.77529306e-04 9.99422471e-01]\n [1.00000000e+00 0.00000000e+00 0.00000000e+00]\n [0.00000000e+00 1.00000000e+00 0.00000000e+00]\n [1.00000000e+00 0.00000000e+00 0.00000000e+00]\n [0.00000000e+00 0.00000000e+00 1.00000000e+00]]"
        }
      },
      "feature_importance": {
        "lightgbm": "                 feature  importance\n99                  draw         163\n100             away_win         162\n98              home_win         156\n101      goal_difference          51\n232        match_balance          29\n..                   ...         ...\n236   team_strength_diff           0\n237   home_team_win_rate           0\n238   away_team_win_rate           0\n239  home_team_avg_goals           0\n240  away_team_avg_goals           0\n\n[241 rows x 2 columns]",
        "random_forest": "                      feature  importance\n99                       draw    0.106844\n101           goal_difference    0.103877\n100                  away_win    0.095561\n104            home_advantage    0.088709\n232             match_balance    0.078629\n..                        ...         ...\n209      away_goal_diff_trend    0.000000\n211   away_clean_sheet_streak    0.000000\n210   home_clean_sheet_streak    0.000000\n214      home_position_change    0.000000\n225  transfer_learning_factor    0.000000\n\n[241 rows x 2 columns]"
      },
      "success": true
    },
    "transfer_learning": {
      "success": false,
      "error": "No source data files"
    },
    "hyperparameter_tuning": {
      "optimization_results": {
        "xgboost": {
          "best_params": {
            "n_estimators": 70,
            "max_depth": 7,
            "learning_rate": 0.0633962790098781,
            "subsample": 0.9600453213426712,
            "colsample_bytree": 0.6815642249743875,
            "min_child_weight": 1,
            "reg_alpha": 0.4212723787372108,
            "reg_lambda": 1.0430989234542152
          },
          "best_value": Infinity,
          "n_trials": 30,
          "study": "<optuna.study.study.Study object at 0x7f1718cb5a90>",
          "model_type": "xgboost"
        },
        "lightgbm": {
          "best_params": {
            "n_estimators": 180,
            "max_depth": 7,
            "learning_rate": 0.044155348585934756,
            "num_leaves": 69,
            "feature_fraction": 0.6454648841585607,
            "bagging_fraction": 0.7671828052679323,
            "bagging_freq": 5,
            "lambda_l1": 0.0009365388901337052,
            "lambda_l2": 0.010781998857585295,
            "min_data_in_leaf": 24
          },
          "best_value": 0.00010515328942091435,
          "n_trials": 30,
          "study": "<optuna.study.study.Study object at 0x7f1715163250>",
          "model_type": "lightgbm"
        },
        "random_forest": {
          "best_params": {
            "n_estimators": 161,
            "max_depth": 13,
            "min_samples_split": 17,
            "min_samples_leaf": 7,
            "max_features": null
          },
          "best_value": 2.2204460492503136e-16,
          "n_trials": 30,
          "study": "<optuna.study.study.Study object at 0x7f1718c61f90>",
          "model_type": "random_forest"
        },
        "logistic_regression": {
          "best_params": {
            "C": 9.999554314053057,
            "max_iter": 441,
            "solver": "lbfgs"
          },
          "best_value": 0.0032901862153515617,
          "n_trials": 30,
          "study": "<optuna.study.study.Study object at 0x7f1718c568b0>",
          "model_type": "logistic_regression"
        }
      },
      "validation_results": {
        "models": {
          "xgboost": {
            "validation_passed": false,
            "issues": [
              "No valid trials"
            ]
          },
          "lightgbm": {
            "validation_passed": true,
            "issues": []
          },
          "random_forest": {
            "validation_passed": true,
            "issues": []
          },
          "logistic_regression": {
            "validation_passed": true,
            "issues": []
          }
        },
        "overall": {
          "validation_passed": false,
          "issues": [
            "No valid trials"
          ]
        }
      },
      "comparison_results": {
        "models": {
          "xgboost": {
            "score": Infinity,
            "params": {
              "n_estimators": 70,
              "max_depth": 7,
              "learning_rate": 0.0633962790098781,
              "subsample": 0.9600453213426712,
              "colsample_bytree": 0.6815642249743875,
              "min_child_weight": 1,
              "reg_alpha": 0.4212723787372108,
              "reg_lambda": 1.0430989234542152
            }
          },
          "lightgbm": {
            "score": 0.00010515328942091435,
            "params": {
              "n_estimators": 180,
              "max_depth": 7,
              "learning_rate": 0.044155348585934756,
              "num_leaves": 69,
              "feature_fraction": 0.6454648841585607,
              "bagging_fraction": 0.7671828052679323,
              "bagging_freq": 5,
              "lambda_l1": 0.0009365388901337052,
              "lambda_l2": 0.010781998857585295,
              "min_data_in_leaf": 24
            }
          },
          "random_forest": {
            "score": 2.2204460492503136e-16,
            "params": {
              "n_estimators": 161,
              "max_depth": 13,
              "min_samples_split": 17,
              "min_samples_leaf": 7,
              "max_features": null
            }
          },
          "logistic_regression": {
            "score": 0.0032901862153515617,
            "params": {
              "C": 9.999554314053057,
              "max_iter": 441,
              "solver": "lbfgs"
            }
          }
        },
        "best_model": "random_forest",
        "best_score": 2.2204460492503136e-16
      },
      "evaluation_results": {
        "xgboost": {
          "error": "feature_names must be string, and may not contain [, ] or <"
        },
        "lightgbm": {
          "mean_score": 0.00010804107325590494,
          "std_score": 4.2306310975070135e-05,
          "scores": [
            0.0001668593064216939,
            8.812129321041772e-05,
            6.914262013560321e-05
          ],
          "model": "LGBMClassifier(bagging_fraction=0.7671828052679323, bagging_freq=5,\n               feature_fraction=0.6454648841585607,\n               lambda_l1=0.0009365388901337052, lambda_l2=0.010781998857585295,\n               learning_rate=0.044155348585934756, max_depth=7,\n               min_data_in_leaf=24, n_estimators=180, num_leaves=69,\n               verbose=-1)"
        },
        "random_forest": {
          "mean_score": 2.2204460492503136e-16,
          "std_score": 0.0,
          "scores": [
            2.2204460492503136e-16,
            2.2204460492503136e-16,
            2.2204460492503136e-16
          ],
          "model": "RandomForestClassifier(max_depth=13, max_features=None, min_samples_leaf=7,\n                       min_samples_split=17, n_estimators=161)"
        },
        "logistic_regression": {
          "mean_score": 0.0032901862153515617,
          "std_score": 0.0030568154897096327,
          "scores": [
            0.0076055423513776135,
            0.0013548985060320659,
            0.0009101177886450051
          ],
          "model": "LogisticRegression(C=9.999554314053057, max_iter=441)"
        }
      },
      "importance_results": {
        "lightgbm": {
          "importance": {
            "learning_rate": 0.3930246725438454,
            "n_estimators": 0.20383309805900338,
            "bagging_fraction": 0.17380651853620316,
            "lambda_l1": 0.09084117408925171,
            "lambda_l2": 0.042487812876528495,
            "num_leaves": 0.038741375976812764,
            "feature_fraction": 0.023493752352428274,
            "bagging_freq": 0.018455416831048285,
            "min_data_in_leaf": 0.008767620823085586,
            "max_depth": 0.0065485579117929935
          },
          "best_params": {
            "n_estimators": 180,
            "max_depth": 7,
            "learning_rate": 0.044155348585934756,
            "num_leaves": 69,
            "feature_fraction": 0.6454648841585607,
            "bagging_fraction": 0.7671828052679323,
            "bagging_freq": 5,
            "lambda_l1": 0.0009365388901337052,
            "lambda_l2": 0.010781998857585295,
            "min_data_in_leaf": 24
          },
          "best_value": 0.00010515328942091435
        },
        "random_forest": {
          "importance": {
            "max_features": 0.9310293155241881,
            "n_estimators": 0.03990405860427429,
            "max_depth": 0.01796635305556032,
            "min_samples_leaf": 0.007514584075825786,
            "min_samples_split": 0.0035856887401515426
          },
          "best_params": {
            "n_estimators": 161,
            "max_depth": 13,
            "min_samples_split": 17,
            "min_samples_leaf": 7,
            "max_features": null
          },
          "best_value": 2.2204460492503136e-16
        },
        "logistic_regression": {
          "importance": {
            "solver": 0.766952020402056,
            "C": 0.20142537809011923,
            "max_iter": 0.03162260150782479
          },
          "best_params": {
            "C": 9.999554314053057,
            "max_iter": 441,
            "solver": "lbfgs"
          },
          "best_value": 0.0032901862153515617
        }
      },
      "success": true
    },
    "ensemble": {
      "base_models": {
        "xgboost": "XGBClassifier(base_score=None, booster=None, callbacks=None,\n              colsample_bylevel=None, colsample_bynode=None,\n              colsample_bytree=0.8, device=None, early_stopping_rounds=None,\n              enable_categorical=False, eval_metric='mlogloss',\n              feature_types=None, feature_weights=None, gamma=None,\n              grow_policy=None, importance_type=None,\n              interaction_constraints=None, learning_rate=0.03, max_bin=None,\n              max_cat_threshold=None, max_cat_to_onehot=None,\n              max_delta_step=None, max_depth=5, max_leaves=None,\n              min_child_weight=5, missing=nan, monotone_constraints=None,\n              multi_strategy=None, n_estimators=150, n_jobs=None,\n              num_parallel_tree=None, ...)",
        "lightgbm": "LGBMClassifier(bagging_fraction=0.8, bagging_freq=5, feature_fraction=0.8,\n               lambda_l1=0.2, lambda_l2=1.5, learning_rate=0.03, max_depth=5,\n               metric='multi_logloss', min_data_in_leaf=10, n_estimators=150,\n               num_class=3, num_leaves=30, objective='multiclass',\n               random_state=42, verbose=-1)",
        "random_forest": "RandomForestClassifier(max_depth=8, min_samples_leaf=5, min_samples_split=10,\n                       random_state=42)",
        "logistic_regression": "LogisticRegression(C=0.1, max_iter=1000, random_state=42)"
      },
      "training_results": {
        "xgboost": {
          "model": null,
          "success": false,
          "error": "XGBClassifier.fit() got an unexpected keyword argument 'early_stopping_rounds'"
        },
        "lightgbm": {
          "model": "LGBMClassifier(bagging_fraction=0.8, bagging_freq=5, feature_fraction=0.8,\n               lambda_l1=0.2, lambda_l2=1.5, learning_rate=0.03, max_depth=5,\n               metric='multi_logloss', min_data_in_leaf=10, n_estimators=150,\n               num_class=3, num_leaves=30, objective='multiclass',\n               random_state=42, verbose=-1)",
          "train_accuracy": 1.0,
          "val_accuracy": 1.0,
          "train_logloss": 0.009364142424332455,
          "val_logloss": 0.009297147441430936,
          "overfitting": 0.0,
          "success": true
        },
        "random_forest": {
          "model": "RandomForestClassifier(max_depth=8, min_samples_leaf=5, min_samples_split=10,\n                       random_state=42)",
          "train_accuracy": 1.0,
          "val_accuracy": 1.0,
          "train_logloss": 0.0472352613668281,
          "val_logloss": 0.05806705514754749,
          "overfitting": 0.0,
          "success": true
        },
        "logistic_regression": {
          "model": "LogisticRegression(C=0.1, max_iter=1000, random_state=42)",
          "train_accuracy": 1.0,
          "val_accuracy": 1.0,
          "train_logloss": 0.026441957893574737,
          "val_logloss": 0.033263949896907535,
          "overfitting": 0.0,
          "success": true
        }
      },
      "conservative_ensemble": {
        "models": {
          "lightgbm": "LGBMClassifier(bagging_fraction=0.8, bagging_freq=5, feature_fraction=0.8,\n               lambda_l1=0.2, lambda_l2=1.5, learning_rate=0.03, max_depth=5,\n               metric='multi_logloss', min_data_in_leaf=10, n_estimators=150,\n               num_class=3, num_leaves=30, objective='multiclass',\n               random_state=42, verbose=-1)",
          "random_forest": "RandomForestClassifier(max_depth=8, min_samples_leaf=5, min_samples_split=10,\n                       random_state=42)",
          "logistic_regression": "LogisticRegression(C=0.1, max_iter=1000, random_state=42)"
        },
        "weights": {
          "lightgbm": 0.5881154594030743,
          "random_forest": 0.24587456303055302,
          "logistic_regression": 0.16600997756637276
        },
        "type": "conservative_weighted",
        "training_results": {
          "xgboost": {
            "model": null,
            "success": false,
            "error": "XGBClassifier.fit() got an unexpected keyword argument 'early_stopping_rounds'"
          },
          "lightgbm": {
            "model": "LGBMClassifier(bagging_fraction=0.8, bagging_freq=5, feature_fraction=0.8,\n               lambda_l1=0.2, lambda_l2=1.5, learning_rate=0.03, max_depth=5,\n               metric='multi_logloss', min_data_in_leaf=10, n_estimators=150,\n               num_class=3, num_leaves=30, objective='multiclass',\n               random_state=42, verbose=-1)",
            "train_accuracy": 1.0,
            "val_accuracy": 1.0,
            "train_logloss": 0.009364142424332455,
            "val_logloss": 0.009297147441430936,
            "overfitting": 0.0,
            "success": true
          },
          "random_forest": {
            "model": "RandomForestClassifier(max_depth=8, min_samples_leaf=5, min_samples_split=10,\n                       random_state=42)",
            "train_accuracy": 1.0,
            "val_accuracy": 1.0,
            "train_logloss": 0.0472352613668281,
            "val_logloss": 0.05806705514754749,
            "overfitting": 0.0,
            "success": true
          },
          "logistic_regression": {
            "model": "LogisticRegression(C=0.1, max_iter=1000, random_state=42)",
            "train_accuracy": 1.0,
            "val_accuracy": 1.0,
            "train_logloss": 0.026441957893574737,
            "val_logloss": 0.033263949896907535,
            "overfitting": 0.0,
            "success": true
          }
        }
      },
      "ensemble_results": {
        "val_accuracy": 1.0,
        "val_logloss": 0.024729120356356736,
        "success": true,
        "type": "conservative_weighted"
      },
      "calibrated_models": {
        "lightgbm": "CalibratedClassifierCV(cv=3,\n                       estimator=LGBMClassifier(bagging_fraction=0.8,\n                                                bagging_freq=5,\n                                                feature_fraction=0.8,\n                                                lambda_l1=0.2, lambda_l2=1.5,\n                                                learning_rate=0.03, max_depth=5,\n                                                metric='multi_logloss',\n                                                min_data_in_leaf=10,\n                                                n_estimators=150, num_class=3,\n                                                num_leaves=30,\n                                                objective='multiclass',\n                                                random_state=42, verbose=-1),\n                       method='isotonic')",
        "random_forest": "CalibratedClassifierCV(cv=3,\n                       estimator=RandomForestClassifier(max_depth=8,\n                                                        min_samples_leaf=5,\n                                                        min_samples_split=10,\n                                                        random_state=42),\n                       method='isotonic')",
        "logistic_regression": "CalibratedClassifierCV(cv=3,\n                       estimator=LogisticRegression(C=0.1, max_iter=1000,\n                                                    random_state=42),\n                       method='isotonic')"
      },
      "cv_results": {
        "mean_accuracy": 1.0,
        "std_accuracy": 0.0,
        "mean_log_loss": 0.043059929159830965,
        "std_log_loss": 0.008478994772180976,
        "n_folds": 2,
        "scores": {
          "accuracy": [
            1.0,
            1.0
          ],
          "log_loss": [
            0.05153892393201194,
            0.03458093438764999
          ],
          "folds": [
            1,
            2
          ]
        }
      },
      "evaluation_results": {
        "accuracy": 1.0,
        "log_loss": 0.036149904114264525,
        "mean_confidence": 0.9647072195018908,
        "classification_report": {
          "Away Win": {
            "precision": 1.0,
            "recall": 1.0,
            "f1-score": 1.0,
            "support": 27.0
          },
          "Draw": {
            "precision": 1.0,
            "recall": 1.0,
            "f1-score": 1.0,
            "support": 36.0
          },
          "Home Win": {
            "precision": 1.0,
            "recall": 1.0,
            "f1-score": 1.0,
            "support": 51.0
          },
          "accuracy": 1.0,
          "macro avg": {
            "precision": 1.0,
            "recall": 1.0,
            "f1-score": 1.0,
            "support": 114.0
          },
          "weighted avg": {
            "precision": 1.0,
            "recall": 1.0,
            "f1-score": 1.0,
            "support": 114.0
          }
        },
        "confusion_matrix": "[[27  0  0]\n [ 0 36  0]\n [ 0  0 51]]",
        "predictions": "[1 2 0 2 2 1 2 2 2 1 0 2 1 0 0 2 2 1 2 2 0 1 2 0 2 0 1 2 1 0 0 2 2 2 2 2 1\n 0 2 0 2 1 2 2 0 2 2 1 0 2 2 1 1 0 2 2 0 1 0 2 1 2 0 1 2 1 1 1 1 2 2 0 2 2\n 1 2 1 1 0 2 2 1 2 0 2 1 1 0 1 2 1 1 1 0 2 2 2 1 2 2 0 0 2 2 1 0 1 1 1 2 0\n 1 0 2]",
        "probabilities": "[[0.0264018  0.94579593 0.02780227]\n [0.0069763  0.01998724 0.97303645]\n [0.98955914 0.00483825 0.00560261]\n [0.01496618 0.02220628 0.96282754]\n [0.01104894 0.01699779 0.97195326]\n [0.02755149 0.94890332 0.02354518]\n [0.00810756 0.01547447 0.97641796]\n [0.01473321 0.02281502 0.96245177]\n [0.01206903 0.01456008 0.97337089]\n [0.01654932 0.96647308 0.0169776 ]\n [0.97729922 0.00896927 0.01373151]\n [0.01068388 0.01942254 0.96989358]\n [0.02777447 0.94438516 0.02784037]\n [0.96897253 0.01986719 0.01116028]\n [0.96848392 0.01759407 0.01392201]\n [0.01096895 0.00545642 0.98357463]\n [0.00887609 0.02254586 0.96857805]\n [0.01092519 0.96970841 0.0193664 ]\n [0.00935507 0.0218795  0.96876544]\n [0.01395792 0.02111103 0.96493105]\n [0.97207337 0.01748082 0.01044581]\n [0.04509557 0.90482302 0.05008141]\n [0.01021116 0.01445276 0.97533608]\n [0.93828625 0.04194053 0.01977322]\n [0.01191475 0.01330523 0.97478002]\n [0.98376066 0.01025349 0.00598586]\n [0.03029216 0.94972461 0.01998323]\n [0.00741341 0.01554514 0.97704145]\n [0.0311922  0.94196376 0.02684404]\n [0.96741865 0.02342665 0.0091547 ]\n [0.98412189 0.00942907 0.00644904]\n [0.01281634 0.02231675 0.96486691]\n [0.01909516 0.03192602 0.94897882]\n [0.01739415 0.02139089 0.96121496]\n [0.00366538 0.00380636 0.99252827]\n [0.013762   0.02789649 0.9583415 ]\n [0.01545582 0.97025707 0.01428712]\n [0.97465467 0.01072997 0.01461536]\n [0.00714254 0.00474372 0.98811374]\n [0.98669634 0.00662243 0.00668124]\n [0.01082891 0.01484999 0.9743211 ]\n [0.01828484 0.96380295 0.01791221]\n [0.00788993 0.03006467 0.96204541]\n [0.0150163  0.02179032 0.96319338]\n [0.9821881  0.01113392 0.00667798]\n [0.00697745 0.01377608 0.97924647]\n [0.00496333 0.00761513 0.98742154]\n [0.01062271 0.96361066 0.02576663]\n [0.96874914 0.01847604 0.01277482]\n [0.00834991 0.01367123 0.97797887]\n [0.00929469 0.0106388  0.98006651]\n [0.01246566 0.97377925 0.01375509]\n [0.01589766 0.96978828 0.01431406]\n [0.94193706 0.03068625 0.02737669]\n [0.02270414 0.03618538 0.94111048]\n [0.0071171  0.0113917  0.9814912 ]\n [0.98279527 0.01136454 0.0058402 ]\n [0.01954387 0.96505371 0.01540242]\n [0.96257538 0.02664502 0.0107796 ]\n [0.00423853 0.01411226 0.9816492 ]\n [0.0212949  0.95565028 0.02305482]\n [0.01384703 0.03167281 0.95448016]\n [0.96969517 0.01549871 0.01480612]\n [0.05033516 0.89109641 0.05856843]\n [0.00683734 0.00631271 0.98684995]\n [0.02654895 0.94871897 0.02473209]\n [0.01111838 0.97352421 0.0153574 ]\n [0.00973656 0.96875707 0.02150636]\n [0.01965889 0.94960675 0.03073436]\n [0.00754361 0.00858163 0.98387476]\n [0.00603995 0.01159345 0.9823666 ]\n [0.9352358  0.03557293 0.02919128]\n [0.01242682 0.01959393 0.96797925]\n [0.00461241 0.00678103 0.98860656]\n [0.01746561 0.96847631 0.01405808]\n [0.0134614  0.01606932 0.97046928]\n [0.02004937 0.9341563  0.04579433]\n [0.030177   0.93847485 0.03134815]\n [0.96543038 0.02080333 0.01376629]\n [0.00528666 0.00882359 0.98588975]\n [0.01506203 0.01986721 0.96507077]\n [0.0448446  0.92373321 0.03142219]\n [0.02126212 0.04029872 0.93843916]\n [0.9841201  0.01047107 0.00540883]\n [0.01015812 0.02189061 0.96795128]\n [0.04860066 0.90999784 0.0414015 ]\n [0.03079294 0.94007278 0.02913428]\n [0.98258513 0.00722992 0.01018495]\n [0.01529251 0.97352438 0.01118311]\n [0.00955755 0.01607774 0.97436472]\n [0.01970145 0.96511793 0.01518062]\n [0.01273363 0.97503121 0.01223516]\n [0.01778002 0.96637205 0.01584793]\n [0.96824738 0.01338332 0.01836931]\n [0.00578214 0.00772669 0.98649117]\n [0.00799943 0.00997503 0.98202554]\n [0.01740505 0.01818302 0.96441193]\n [0.01434765 0.9737848  0.01186755]\n [0.00941233 0.00878776 0.9817999 ]\n [0.0056219  0.00892302 0.98545508]\n [0.98223108 0.00685524 0.01091367]\n [0.96409996 0.01999051 0.01590953]\n [0.0215179  0.02666385 0.95181824]\n [0.00836042 0.01174548 0.9798941 ]\n [0.01474848 0.96703195 0.01821957]\n [0.90522012 0.04655899 0.04822089]\n [0.00832582 0.97527338 0.0164008 ]\n [0.02833206 0.95201897 0.01964897]\n [0.04645529 0.91615649 0.03738822]\n [0.0165827  0.02159107 0.96182623]\n [0.94997477 0.02812262 0.02190261]\n [0.04689168 0.90606668 0.04704165]\n [0.97091266 0.0149813  0.01410603]\n [0.00379039 0.0032137  0.9929959 ]]",
        "confidence_scores": "[0.94579593 0.97303645 0.98955914 0.96282754 0.97195326 0.94890332\n 0.97641796 0.96245177 0.97337089 0.96647308 0.97729922 0.96989358\n 0.94438516 0.96897253 0.96848392 0.98357463 0.96857805 0.96970841\n 0.96876544 0.96493105 0.97207337 0.90482302 0.97533608 0.93828625\n 0.97478002 0.98376066 0.94972461 0.97704145 0.94196376 0.96741865\n 0.98412189 0.96486691 0.94897882 0.96121496 0.99252827 0.9583415\n 0.97025707 0.97465467 0.98811374 0.98669634 0.9743211  0.96380295\n 0.96204541 0.96319338 0.9821881  0.97924647 0.98742154 0.96361066\n 0.96874914 0.97797887 0.98006651 0.97377925 0.96978828 0.94193706\n 0.94111048 0.9814912  0.98279527 0.96505371 0.96257538 0.9816492\n 0.95565028 0.95448016 0.96969517 0.89109641 0.98684995 0.94871897\n 0.97352421 0.96875707 0.94960675 0.98387476 0.9823666  0.9352358\n 0.96797925 0.98860656 0.96847631 0.97046928 0.9341563  0.93847485\n 0.96543038 0.98588975 0.96507077 0.92373321 0.93843916 0.9841201\n 0.96795128 0.90999784 0.94007278 0.98258513 0.97352438 0.97436472\n 0.96511793 0.97503121 0.96637205 0.96824738 0.98649117 0.98202554\n 0.96441193 0.9737848  0.9817999  0.98545508 0.98223108 0.96409996\n 0.95181824 0.9798941  0.96703195 0.90522012 0.97527338 0.95201897\n 0.91615649 0.96182623 0.94997477 0.90606668 0.97091266 0.9929959 ]"
      },
      "predictions": {
        "predictions": "[1 2 0 2 2 1 2 2 2 1 0 2 1 0 0 2 2 1 2 2 0 1 2 0 2 0 1 2 1 0 0 2 2 2 2 2 1\n 0 2 0 2 1 2 2 0 2 2 1 0 2 2 1 1 0 2 2 0 1 0 2 1 2 0 1 2 1 1 1 1 2 2 0 2 2\n 1 2 1 1 0 2 2 1 2 0 2 1 1 0 1 2 1 1 1 0 2 2 2 1 2 2 0 0 2 2 1 0 1 1 1 2 0\n 1 0 2]",
        "probabilities": "[[0.0264018  0.94579593 0.02780227]\n [0.0069763  0.01998724 0.97303645]\n [0.98955914 0.00483825 0.00560261]\n [0.01496618 0.02220628 0.96282754]\n [0.01104894 0.01699779 0.97195326]\n [0.02755149 0.94890332 0.02354518]\n [0.00810756 0.01547447 0.97641796]\n [0.01473321 0.02281502 0.96245177]\n [0.01206903 0.01456008 0.97337089]\n [0.01654932 0.96647308 0.0169776 ]\n [0.97729922 0.00896927 0.01373151]\n [0.01068388 0.01942254 0.96989358]\n [0.02777447 0.94438516 0.02784037]\n [0.96897253 0.01986719 0.01116028]\n [0.96848392 0.01759407 0.01392201]\n [0.01096895 0.00545642 0.98357463]\n [0.00887609 0.02254586 0.96857805]\n [0.01092519 0.96970841 0.0193664 ]\n [0.00935507 0.0218795  0.96876544]\n [0.01395792 0.02111103 0.96493105]\n [0.97207337 0.01748082 0.01044581]\n [0.04509557 0.90482302 0.05008141]\n [0.01021116 0.01445276 0.97533608]\n [0.93828625 0.04194053 0.01977322]\n [0.01191475 0.01330523 0.97478002]\n [0.98376066 0.01025349 0.00598586]\n [0.03029216 0.94972461 0.01998323]\n [0.00741341 0.01554514 0.97704145]\n [0.0311922  0.94196376 0.02684404]\n [0.96741865 0.02342665 0.0091547 ]\n [0.98412189 0.00942907 0.00644904]\n [0.01281634 0.02231675 0.96486691]\n [0.01909516 0.03192602 0.94897882]\n [0.01739415 0.02139089 0.96121496]\n [0.00366538 0.00380636 0.99252827]\n [0.013762   0.02789649 0.9583415 ]\n [0.01545582 0.97025707 0.01428712]\n [0.97465467 0.01072997 0.01461536]\n [0.00714254 0.00474372 0.98811374]\n [0.98669634 0.00662243 0.00668124]\n [0.01082891 0.01484999 0.9743211 ]\n [0.01828484 0.96380295 0.01791221]\n [0.00788993 0.03006467 0.96204541]\n [0.0150163  0.02179032 0.96319338]\n [0.9821881  0.01113392 0.00667798]\n [0.00697745 0.01377608 0.97924647]\n [0.00496333 0.00761513 0.98742154]\n [0.01062271 0.96361066 0.02576663]\n [0.96874914 0.01847604 0.01277482]\n [0.00834991 0.01367123 0.97797887]\n [0.00929469 0.0106388  0.98006651]\n [0.01246566 0.97377925 0.01375509]\n [0.01589766 0.96978828 0.01431406]\n [0.94193706 0.03068625 0.02737669]\n [0.02270414 0.03618538 0.94111048]\n [0.0071171  0.0113917  0.9814912 ]\n [0.98279527 0.01136454 0.0058402 ]\n [0.01954387 0.96505371 0.01540242]\n [0.96257538 0.02664502 0.0107796 ]\n [0.00423853 0.01411226 0.9816492 ]\n [0.0212949  0.95565028 0.02305482]\n [0.01384703 0.03167281 0.95448016]\n [0.96969517 0.01549871 0.01480612]\n [0.05033516 0.89109641 0.05856843]\n [0.00683734 0.00631271 0.98684995]\n [0.02654895 0.94871897 0.02473209]\n [0.01111838 0.97352421 0.0153574 ]\n [0.00973656 0.96875707 0.02150636]\n [0.01965889 0.94960675 0.03073436]\n [0.00754361 0.00858163 0.98387476]\n [0.00603995 0.01159345 0.9823666 ]\n [0.9352358  0.03557293 0.02919128]\n [0.01242682 0.01959393 0.96797925]\n [0.00461241 0.00678103 0.98860656]\n [0.01746561 0.96847631 0.01405808]\n [0.0134614  0.01606932 0.97046928]\n [0.02004937 0.9341563  0.04579433]\n [0.030177   0.93847485 0.03134815]\n [0.96543038 0.02080333 0.01376629]\n [0.00528666 0.00882359 0.98588975]\n [0.01506203 0.01986721 0.96507077]\n [0.0448446  0.92373321 0.03142219]\n [0.02126212 0.04029872 0.93843916]\n [0.9841201  0.01047107 0.00540883]\n [0.01015812 0.02189061 0.96795128]\n [0.04860066 0.90999784 0.0414015 ]\n [0.03079294 0.94007278 0.02913428]\n [0.98258513 0.00722992 0.01018495]\n [0.01529251 0.97352438 0.01118311]\n [0.00955755 0.01607774 0.97436472]\n [0.01970145 0.96511793 0.01518062]\n [0.01273363 0.97503121 0.01223516]\n [0.01778002 0.96637205 0.01584793]\n [0.96824738 0.01338332 0.01836931]\n [0.00578214 0.00772669 0.98649117]\n [0.00799943 0.00997503 0.98202554]\n [0.01740505 0.01818302 0.96441193]\n [0.01434765 0.9737848  0.01186755]\n [0.00941233 0.00878776 0.9817999 ]\n [0.0056219  0.00892302 0.98545508]\n [0.98223108 0.00685524 0.01091367]\n [0.96409996 0.01999051 0.01590953]\n [0.0215179  0.02666385 0.95181824]\n [0.00836042 0.01174548 0.9798941 ]\n [0.01474848 0.96703195 0.01821957]\n [0.90522012 0.04655899 0.04822089]\n [0.00832582 0.97527338 0.0164008 ]\n [0.02833206 0.95201897 0.01964897]\n [0.04645529 0.91615649 0.03738822]\n [0.0165827  0.02159107 0.96182623]\n [0.94997477 0.02812262 0.02190261]\n [0.04689168 0.90606668 0.04704165]\n [0.97091266 0.0149813  0.01410603]\n [0.00379039 0.0032137  0.9929959 ]]",
        "confidence_scores": "[0.94579593 0.97303645 0.98955914 0.96282754 0.97195326 0.94890332\n 0.97641796 0.96245177 0.97337089 0.96647308 0.97729922 0.96989358\n 0.94438516 0.96897253 0.96848392 0.98357463 0.96857805 0.96970841\n 0.96876544 0.96493105 0.97207337 0.90482302 0.97533608 0.93828625\n 0.97478002 0.98376066 0.94972461 0.97704145 0.94196376 0.96741865\n 0.98412189 0.96486691 0.94897882 0.96121496 0.99252827 0.9583415\n 0.97025707 0.97465467 0.98811374 0.98669634 0.9743211  0.96380295\n 0.96204541 0.96319338 0.9821881  0.97924647 0.98742154 0.96361066\n 0.96874914 0.97797887 0.98006651 0.97377925 0.96978828 0.94193706\n 0.94111048 0.9814912  0.98279527 0.96505371 0.96257538 0.9816492\n 0.95565028 0.95448016 0.96969517 0.89109641 0.98684995 0.94871897\n 0.97352421 0.96875707 0.94960675 0.98387476 0.9823666  0.9352358\n 0.96797925 0.98860656 0.96847631 0.97046928 0.9341563  0.93847485\n 0.96543038 0.98588975 0.96507077 0.92373321 0.93843916 0.9841201\n 0.96795128 0.90999784 0.94007278 0.98258513 0.97352438 0.97436472\n 0.96511793 0.97503121 0.96637205 0.96824738 0.98649117 0.98202554\n 0.96441193 0.9737848  0.9817999  0.98545508 0.98223108 0.96409996\n 0.95181824 0.9798941  0.96703195 0.90522012 0.97527338 0.95201897\n 0.91615649 0.96182623 0.94997477 0.90606668 0.97091266 0.9929959 ]",
        "high_confidence_mask": "[ True  True  True  True  True  True  True  True  True  True  True  True\n  True  True  True  True  True  True  True  True  True  True  True  True\n  True  True  True  True  True  True  True  True  True  True  True  True\n  True  True  True  True  True  True  True  True  True  True  True  True\n  True  True  True  True  True  True  True  True  True  True  True  True\n  True  True  True  True  True  True  True  True  True  True  True  True\n  True  True  True  True  True  True  True  True  True  True  True  True\n  True  True  True  True  True  True  True  True  True  True  True  True\n  True  True  True  True  True  True  True  True  True  True  True  True\n  True  True  True  True  True  True]",
        "low_confidence_mask": "[False False False False False False False False False False False False\n False False False False False False False False False False False False\n False False False False False False False False False False False False\n False False False False False False False False False False False False\n False False False False False False False False False False False False\n False False False False False False False False False False False False\n False False False False False False False False False False False False\n False False False False False False False False False False False False\n False False False False False False False False False False False False\n False False False False False False]",
        "high_confidence_count": "114",
        "low_confidence_count": "0",
        "confidence_threshold": 0.6
      },
      "feature_importance": {
        "lightgbm": "                 feature  importance\n100             away_win         123\n99                  draw         123\n98              home_win         121\n101      goal_difference          37\n232        match_balance          29\n..                   ...         ...\n236   team_strength_diff           0\n237   home_team_win_rate           0\n238   away_team_win_rate           0\n239  home_team_avg_goals           0\n240  away_team_avg_goals           0\n\n[241 rows x 2 columns]",
        "random_forest": "                      feature  importance\n100                  away_win    0.108435\n120            result_encoded    0.099564\n101           goal_difference    0.089809\n104            home_advantage    0.077312\n99                       draw    0.074642\n..                        ...         ...\n210   home_clean_sheet_streak    0.000000\n212     home_current_position    0.000000\n211   away_clean_sheet_streak    0.000000\n214      home_position_change    0.000000\n225  transfer_learning_factor    0.000000\n\n[241 rows x 2 columns]",
        "ensemble": "                      feature  importance\n100                  away_win   72.664427\n99                       draw   72.656194\n98                   home_win   71.467136\n101           goal_difference   21.872338\n232             match_balance   17.142691\n..                        ...         ...\n210   home_clean_sheet_streak    0.000000\n212     home_current_position    0.000000\n211   away_clean_sheet_streak    0.000000\n214      home_position_change    0.000000\n225  transfer_learning_factor    0.000000\n\n[241 rows x 2 columns]"
      },
      "uncertainty_estimates": {
        "mean_prediction": "[[0.26239102 0.22380208 0.5138069 ]\n [0.33832575 0.24323953 0.41843472]\n [0.28091445 0.33642533 0.38266022]\n [0.22545258 0.31227514 0.46227228]\n [0.12856988 0.27971397 0.59171615]\n [0.30119535 0.2417001  0.45710455]\n [0.26284096 0.29732936 0.43982968]\n [0.24367614 0.31955601 0.43676785]\n [0.34084594 0.22578155 0.43337251]\n [0.22574069 0.33109469 0.44316462]\n [0.28263993 0.20520831 0.51215176]\n [0.26332364 0.18589463 0.55078173]\n [0.26489194 0.29672803 0.43838003]\n [0.28625945 0.35200324 0.36173731]\n [0.28281928 0.24017044 0.47701029]\n [0.20855339 0.40772058 0.38372603]\n [0.37526271 0.28140951 0.34332777]\n [0.22393429 0.3730868  0.40297891]\n [0.10902768 0.29991009 0.59106223]\n [0.16911376 0.25755689 0.57332935]\n [0.22655834 0.33382622 0.43961544]\n [0.26296875 0.36832347 0.36870779]\n [0.32257076 0.38960276 0.28782648]\n [0.3022811  0.24053259 0.45718631]\n [0.21027124 0.42237079 0.36735796]\n [0.20497654 0.33531901 0.45970445]\n [0.31884424 0.3189222  0.36223356]\n [0.18805187 0.43125439 0.38069374]\n [0.28288922 0.31672599 0.40038479]\n [0.26376974 0.28088292 0.45534734]\n [0.36034423 0.33348777 0.30616799]\n [0.16646976 0.4133943  0.42013594]\n [0.20440038 0.29869475 0.49690488]\n [0.16578616 0.23966587 0.59454797]\n [0.24281871 0.2411314  0.51604989]\n [0.29981221 0.29970044 0.40048735]\n [0.29950541 0.24289238 0.45760222]\n [0.18853417 0.27784401 0.53362182]\n [0.30166637 0.31629331 0.38204032]\n [0.16531712 0.30151324 0.53316965]\n [0.20831245 0.41001288 0.38167467]\n [0.28277158 0.29707432 0.4201541 ]\n [0.3204629  0.29710931 0.38242779]\n [0.2256902  0.35434281 0.41996699]\n [0.22650358 0.31742444 0.45607199]\n [0.30066678 0.25923438 0.44009884]\n [0.20545291 0.3356953  0.45885179]\n [0.22305695 0.22253418 0.55440887]\n [0.26451622 0.37142601 0.36405777]\n [0.24503049 0.3359001  0.41906941]\n [0.22338423 0.3559894  0.42062637]\n [0.14864033 0.35554468 0.495815  ]\n [0.26283663 0.29767191 0.43949146]\n [0.37920089 0.28013489 0.34066422]\n [0.24442966 0.31391481 0.44165553]\n [0.2834659  0.41149085 0.30504325]\n [0.29974341 0.28213935 0.41811724]\n [0.2417961  0.33628689 0.42191701]\n [0.18663735 0.37564722 0.43771542]\n [0.18931542 0.38995994 0.42072464]\n [0.1880616  0.35523283 0.45670558]\n [0.22816855 0.33243139 0.43940006]\n [0.26466435 0.31429557 0.42104008]\n [0.19008216 0.39119083 0.41872701]\n [0.31909099 0.22270462 0.45820439]\n [0.26176955 0.35405568 0.38417477]\n [0.32222184 0.20209762 0.47568054]\n [0.22624844 0.37153269 0.40221887]\n [0.16873175 0.45003816 0.38123009]\n [0.22519182 0.26238295 0.51242523]\n [0.22233871 0.2408385  0.5368228 ]\n [0.4350703  0.22559671 0.33933298]\n [0.27992566 0.26082537 0.45924897]\n [0.27912432 0.20495484 0.51592084]\n [0.28001009 0.37349417 0.34649574]\n [0.24498615 0.18300889 0.57200496]\n [0.22379071 0.35653466 0.41967463]\n [0.24517637 0.23918218 0.51564145]\n [0.18944814 0.44759607 0.36295579]\n [0.16941153 0.36871035 0.46187812]\n [0.24518599 0.3532886  0.40152541]\n [0.20897455 0.35291088 0.43811457]\n [0.18943322 0.42861433 0.38195245]\n [0.14900649 0.31455057 0.53644294]\n [0.09321656 0.39017799 0.51660544]\n [0.28419323 0.35565281 0.36015396]\n [0.28038922 0.29994028 0.4196705 ]\n [0.22666587 0.31745641 0.45587773]\n [0.20279087 0.35539599 0.44181315]\n [0.26420802 0.2962804  0.43951158]\n [0.26402784 0.37518551 0.36078665]\n [0.18581349 0.29889116 0.51529535]\n [0.26404585 0.25745888 0.47849527]\n [0.26219152 0.37253804 0.36527044]\n [0.14943012 0.29672497 0.55384491]\n [0.14904604 0.27753569 0.57341827]\n [0.26676866 0.3147208  0.41851054]\n [0.30291701 0.27770941 0.41937358]\n [0.22571356 0.37320004 0.4010864 ]\n [0.26302963 0.3146887  0.42228167]\n [0.22417796 0.26188256 0.51393948]\n [0.37983164 0.22046019 0.39970817]\n [0.24517148 0.29861944 0.45620908]\n [0.24791341 0.31228515 0.43980144]\n [0.32091958 0.3165802  0.36250021]\n [0.3017446  0.20314394 0.49511146]\n [0.18900034 0.38930862 0.42169103]\n [0.34383151 0.29758836 0.35858013]\n [0.24223296 0.2795577  0.47820934]\n [0.28327777 0.23953286 0.47718937]\n [0.20739079 0.37276925 0.41983997]\n [0.24588024 0.37276858 0.38135118]\n [0.26486681 0.31827151 0.41686168]\n [0.11352029 0.37041357 0.51606614]]",
        "prediction_variance": "[[0.17612639 0.15098541 0.22746133]\n [0.20291867 0.16054551 0.22073166]\n [0.18073303 0.19693501 0.21434925]\n [0.15375608 0.18663025 0.22488225]\n [0.096413   0.17785416 0.21828551]\n [0.18942675 0.15777417 0.22330155]\n [0.17214045 0.18137434 0.22221695]\n [0.1672507  0.19387016 0.22511285]\n [0.20713275 0.15370719 0.22433503]\n [0.15410093 0.19466274 0.22532495]\n [0.18526089 0.14178579 0.22709004]\n [0.17690287 0.13018557 0.22525869]\n [0.17253041 0.17814109 0.21955665]\n [0.18496492 0.20085326 0.20912223]\n [0.18253124 0.15667981 0.22545393]\n [0.14791486 0.21485449 0.2154154 ]\n [0.21330434 0.17882124 0.2050246 ]\n [0.15411783 0.20627407 0.21772207]\n [0.08125349 0.18585285 0.21788227]\n [0.12476518 0.16844728 0.22129107]\n [0.15716908 0.19680307 0.22470705]\n [0.17139094 0.20294935 0.20939057]\n [0.19820345 0.21021448 0.18457382]\n [0.19030287 0.15675346 0.22524511]\n [0.14440778 0.21119164 0.20915885]\n [0.14441279 0.19793013 0.22706089]\n [0.19532229 0.19093327 0.20903256]\n [0.13334211 0.21726991 0.2138226 ]\n [0.18365818 0.19056592 0.21849794]\n [0.17456931 0.17580978 0.22360159]\n [0.20770124 0.1954578  0.19192735]\n [0.11779811 0.21245221 0.21844555]\n [0.14356001 0.18502963 0.22660169]\n [0.12114861 0.15859081 0.2188399 ]\n [0.16590769 0.15983357 0.22727522]\n [0.19212329 0.18808503 0.22139295]\n [0.19079024 0.16094978 0.22718188]\n [0.13420586 0.17462955 0.22370006]\n [0.19024976 0.19048473 0.21358769]\n [0.11874824 0.18492182 0.22300227]\n [0.14566008 0.21348538 0.21398128]\n [0.1837849  0.18416921 0.22145233]\n [0.19808508 0.18535139 0.21741407]\n [0.15539273 0.20129812 0.22046769]\n [0.15655145 0.19078324 0.2252314 ]\n [0.190541   0.16954096 0.22617481]\n [0.14282154 0.19561907 0.22422746]\n [0.1540238  0.149004   0.22328821]\n [0.17475355 0.20576755 0.21079105]\n [0.16729081 0.19910299 0.22324523]\n [0.15322351 0.20368629 0.22100093]\n [0.11097795 0.20574941 0.22880588]\n [0.1736857  0.18326182 0.22383361]\n [0.21355679 0.17683571 0.20354226]\n [0.16527972 0.18835724 0.22426878]\n [0.17938079 0.21171934 0.18798763]\n [0.18989829 0.17819757 0.22204415]\n [0.16189307 0.1968207  0.22132288]\n [0.13451513 0.21051449 0.22564617]\n [0.13509827 0.21053739 0.22178214]\n [0.13403005 0.20226573 0.22414164]\n [0.1559959  0.19183429 0.22143038]\n [0.17302347 0.18808634 0.22080637]\n [0.13629769 0.21144482 0.22061555]\n [0.19895685 0.15277171 0.22866312]\n [0.17179305 0.20166351 0.21444688]\n [0.19911191 0.13789579 0.22745829]\n [0.1537788  0.20285689 0.21479259]\n [0.12039655 0.21748423 0.21147954]\n [0.15858885 0.17255494 0.22811437]\n [0.15548379 0.15998907 0.22731209]\n [0.22555056 0.15387547 0.20455741]\n [0.1795613  0.16694235 0.22421585]\n [0.18313164 0.14245118 0.22943843]\n [0.17908443 0.20611963 0.20360228]\n [0.1651897  0.12345101 0.21878275]\n [0.15459946 0.20266323 0.21962696]\n [0.17054836 0.16269416 0.23004161]\n [0.13340954 0.21756551 0.2081583 ]\n [0.12318044 0.20552988 0.2261734 ]\n [0.1638584  0.2000033  0.21650511]\n [0.14644562 0.20070817 0.22220811]\n [0.13517124 0.21669886 0.21390467]\n [0.10643852 0.18505372 0.22258274]\n [0.06508496 0.20766272 0.2235199 ]\n [0.18549549 0.20434811 0.20901699]\n [0.18206503 0.18491274 0.22244607]\n [0.15897711 0.19321252 0.22594513]\n [0.13865013 0.19862694 0.2208692 ]\n [0.17422125 0.18087989 0.22286988]\n [0.17074692 0.2031053  0.20420217]\n [0.13253475 0.18350357 0.22578038]\n [0.17666225 0.16752693 0.22807141]\n [0.17073397 0.20571002 0.20930136]\n [0.11150398 0.18455116 0.22466025]\n [0.10941362 0.17513104 0.220046  ]\n [0.17952856 0.19264205 0.22351811]\n [0.19479862 0.17955095 0.22449473]\n [0.15620181 0.20896888 0.21931228]\n [0.17560585 0.19118544 0.22394049]\n [0.15818248 0.17169184 0.22804242]\n [0.21630189 0.1481879  0.2188686 ]\n [0.16799836 0.18699053 0.22584364]\n [0.16872858 0.18929513 0.22555074]\n [0.19621532 0.18890927 0.20826554]\n [0.19166545 0.13745887 0.22593072]\n [0.13588984 0.21190553 0.22330513]\n [0.20666124 0.18464471 0.20813269]\n [0.16080145 0.1727108  0.22331955]\n [0.18310297 0.15773302 0.22708439]\n [0.14327169 0.20460392 0.21827364]\n [0.16470999 0.20488826 0.21188202]\n [0.17477392 0.18969729 0.21859591]\n [0.08217641 0.20358422 0.22385058]]",
        "confidence_intervals": {
          "lower": "[[0.00379434 0.00401727 0.0058402 ]\n [0.00584015 0.00667481 0.00746957]\n [0.00697656 0.00530311 0.00723524]\n [0.00397535 0.00355795 0.01055108]\n [0.00389123 0.00420402 0.01052091]\n [0.00458585 0.00537638 0.00746705]\n [0.00571596 0.00884596 0.00597718]\n [0.00605083 0.00483825 0.00560261]\n [0.00503608 0.00483825 0.00545243]\n [0.00528666 0.00490408 0.00723524]\n [0.0069763  0.00667481 0.00587297]\n [0.00379434 0.00466333 0.0058402 ]\n [0.00700887 0.00702622 0.0060287 ]\n [0.00686887 0.00564909 0.00545243]\n [0.00571596 0.00651908 0.00746705]\n [0.0040543  0.00443997 0.0060287 ]\n [0.00520557 0.00509674 0.01052091]\n [0.00423853 0.00490408 0.00746705]\n [0.00389123 0.00466333 0.01144909]\n [0.00563556 0.00490408 0.00667871]\n [0.00389123 0.00417762 0.00667871]\n [0.0036935  0.00380636 0.0058453 ]\n [0.00389123 0.00403853 0.00545243]\n [0.00571596 0.00726297 0.00598586]\n [0.00412705 0.00401727 0.00640183]\n [0.00395741 0.00401727 0.00597718]\n [0.00528666 0.00879582 0.01044581]\n [0.00518169 0.00678103 0.00705781]\n [0.00432266 0.00679773 0.00614158]\n [0.00432266 0.00497734 0.00565607]\n [0.00569851 0.00527537 0.00565607]\n [0.00683734 0.00691043 0.0106117 ]\n [0.00566709 0.00490408 0.01091367]\n [0.00389123 0.00357922 0.01080977]\n [0.00476412 0.00631271 0.00609007]\n [0.0036935  0.00380636 0.00614158]\n [0.00440161 0.00722992 0.00609007]\n [0.00536209 0.00497734 0.0060287 ]\n [0.00605083 0.00685524 0.01018495]\n [0.00700798 0.00734169 0.01031875]\n [0.00496333 0.00665811 0.00760337]\n [0.00476412 0.00679773 0.00579306]\n [0.00578214 0.00530311 0.00587297]\n [0.00469137 0.00497734 0.00565607]\n [0.00723278 0.00706325 0.00650128]\n [0.00432266 0.00665811 0.00667871]\n [0.00520557 0.00497734 0.00687646]\n [0.0056219  0.00509674 0.0095203 ]\n [0.00539814 0.00564909 0.00609007]\n [0.00412705 0.00457666 0.00614158]\n [0.00545615 0.00476499 0.00667871]\n [0.00511151 0.00638239 0.00723777]\n [0.00483955 0.00678103 0.00728962]\n [0.00628804 0.00675911 0.00569439]\n [0.00379039 0.00334705 0.00568884]\n [0.00754526 0.00667481 0.00760337]\n [0.00578214 0.00517    0.00565607]\n [0.00565795 0.00638239 0.00938651]\n [0.00395741 0.00403853 0.00602943]\n [0.00514756 0.00684579 0.00614232]\n [0.00563556 0.00509674 0.00938651]\n [0.00440161 0.00764023 0.00609007]\n [0.00536209 0.00631271 0.00644904]\n [0.00563556 0.00483825 0.00568884]\n [0.00469137 0.00517    0.00565607]\n [0.00511302 0.00517    0.00565607]\n [0.0036935  0.00380636 0.00565607]\n [0.00536209 0.00788704 0.00760084]\n [0.00464385 0.00997503 0.00804977]\n [0.00389123 0.00403853 0.00568884]\n [0.00423853 0.00509674 0.00742341]\n [0.00697745 0.00651908 0.00545243]\n [0.00389123 0.00357922 0.00587297]\n [0.00389123 0.00371831 0.00609007]\n [0.00538498 0.00660575 0.01141982]\n [0.00539814 0.00516643 0.00752827]\n [0.00482327 0.00509674 0.00650128]\n [0.00423853 0.00631271 0.00668124]\n [0.00496333 0.00665811 0.00723777]\n [0.00528666 0.00490408 0.00752574]\n [0.00621937 0.00675911 0.01018495]\n [0.00520557 0.00641808 0.00609007]\n [0.00707554 0.00706325 0.00667871]\n [0.00503608 0.00660575 0.01116542]\n [0.00469137 0.00631271 0.01373682]\n [0.00700887 0.00529207 0.00609007]\n [0.00423853 0.00523969 0.00584457]\n [0.00464385 0.00685524 0.00597718]\n [0.00628231 0.00862801 0.01229477]\n [0.0056219  0.00476499 0.00609007]\n [0.00727994 0.00896927 0.00961109]\n [0.00461241 0.00679773 0.00723524]\n [0.00503608 0.00702622 0.00644904]\n [0.0036935  0.00380636 0.01031875]\n [0.00432266 0.00490408 0.00553866]\n [0.00511151 0.00764023 0.01018448]\n [0.00461241 0.00523969 0.00560261]\n [0.00420248 0.00401727 0.00565607]\n [0.00469137 0.00679773 0.00763101]\n [0.00389123 0.00401635 0.00644904]\n [0.00389123 0.00403853 0.00560261]\n [0.00511151 0.00476499 0.00560261]\n [0.00387846 0.00403853 0.00728962]\n [0.0036935  0.00380636 0.00568884]\n [0.00379039 0.00334705 0.00545243]\n [0.00511302 0.00476499 0.00540883]\n [0.00539814 0.00474372 0.00667798]\n [0.00493361 0.00483825 0.00560261]\n [0.00483955 0.00665811 0.01044581]\n [0.00625089 0.00483825 0.00579306]\n [0.00625063 0.00724368 0.00598586]\n [0.00482327 0.00638239 0.00668124]\n [0.00589537 0.00638239 0.0058402 ]\n [0.00511151 0.0078326  0.01044581]]",
          "upper": "[[0.98279527 0.96836892 0.991535  ]\n [0.98250547 0.97268363 0.98330282]\n [0.98249579 0.97278911 0.98715997]\n [0.98052639 0.9737219  0.9920083 ]\n [0.9761234  0.97268376 0.99174167]\n [0.98249579 0.97352434 0.98543564]\n [0.98412189 0.97503121 0.98476017]\n [0.98955914 0.97474952 0.98317532]\n [0.98955914 0.97475077 0.98707689]\n [0.98249579 0.96978828 0.98795799]\n [0.98603581 0.9698564  0.98002772]\n [0.98412149 0.97286331 0.99137925]\n [0.98266832 0.97206358 0.98113185]\n [0.98412189 0.97013362 0.98314141]\n [0.98249579 0.97013362 0.98503198]\n [0.98611668 0.97503121 0.99137925]\n [0.98052639 0.97487986 0.98795799]\n [0.98250547 0.97352421 0.98795799]\n [0.96848392 0.97268363 0.99137925]\n [0.98369507 0.96949436 0.98782939]\n [0.98274798 0.97245174 0.99164588]\n [0.98669634 0.97414471 0.99252827]\n [0.98412149 0.97377925 0.9905137 ]\n [0.98376066 0.97259039 0.98653611]\n [0.98140959 0.97278924 0.991535  ]\n [0.98382201 0.97487982 0.991535  ]\n [0.97670419 0.97527338 0.98588975]\n [0.98258679 0.97469218 0.98849567]\n [0.9834165  0.9737219  0.98799528]\n [0.98611668 0.96977031 0.98833993]\n [0.98382201 0.97352438 0.98761334]\n [0.9761234  0.97503121 0.9858412 ]\n [0.98223108 0.97352421 0.98761334]\n [0.9794105  0.97268376 0.9920083 ]\n [0.98404061 0.97352434 0.98821132]\n [0.98403923 0.97527338 0.99252827]\n [0.98354344 0.97352434 0.98697909]\n [0.98266832 0.97352438 0.98579195]\n [0.98258513 0.9686939  0.981766  ]\n [0.9813958  0.97298676 0.9854864 ]\n [0.98569165 0.97278911 0.98742154]\n [0.98412149 0.97352421 0.98496953]\n [0.98354344 0.97521889 0.98774866]\n [0.98611668 0.97487986 0.98833993]\n [0.98404061 0.96977031 0.98353543]\n [0.98377473 0.96978828 0.98849567]\n [0.98038284 0.96991957 0.98729293]\n [0.97670419 0.96943118 0.98782939]\n [0.98404061 0.97013362 0.98676923]\n [0.98349616 0.97474952 0.99116992]\n [0.98568198 0.97493845 0.98795799]\n [0.98458198 0.97377925 0.98729293]\n [0.98274798 0.97238844 0.9881306 ]\n [0.9841201  0.97278911 0.98476017]\n [0.98669634 0.96970841 0.99289069]\n [0.98266832 0.96977031 0.98090412]\n [0.98382201 0.9737262  0.98684995]\n [0.9813958  0.97299106 0.98684995]\n [0.98581859 0.97503121 0.99137925]\n [0.98403923 0.97288558 0.98721221]\n [0.98250547 0.97493845 0.98811374]\n [0.98354344 0.97238857 0.98721221]\n [0.98412149 0.97015159 0.98782939]\n [0.98833576 0.97474952 0.98663391]\n [0.98611668 0.97286331 0.98849567]\n [0.98382201 0.9737262  0.98782939]\n [0.98382201 0.97378355 0.99252827]\n [0.9810881  0.97474952 0.98663391]\n [0.97937719 0.9749372  0.98202554]\n [0.98825448 0.9737848  0.9910346 ]\n [0.98038306 0.96875707 0.98860656]\n [0.98412149 0.97372186 0.98006651]\n [0.98354344 0.97258609 0.99174167]\n [0.98412189 0.96978828 0.99174167]\n [0.97729922 0.97503121 0.98729293]\n [0.9834165  0.95413221 0.98795799]\n [0.98611709 0.97521889 0.98782939]\n [0.98581859 0.97403923 0.98821132]\n [0.98223108 0.97469218 0.98795799]\n [0.98049308 0.9691681  0.98782939]\n [0.98258513 0.97288128 0.9858412 ]\n [0.98404061 0.97015159 0.98729293]\n [0.98340683 0.97268376 0.98330282]\n [0.97137568 0.96991957 0.98729293]\n [0.96729321 0.9749372  0.98742154]\n [0.98404061 0.97469218 0.98098614]\n [0.98611668 0.9737219  0.98628418]\n [0.98382201 0.97493845 0.98223909]\n [0.96513103 0.97352434 0.98353543]\n [0.98404061 0.96949436 0.98774866]\n [0.97729922 0.97521889 0.97279274]\n [0.98222141 0.97378355 0.9881306 ]\n [0.98412149 0.97378355 0.98707689]\n [0.98250547 0.97288128 0.99252827]\n [0.9841201  0.97395703 0.98849567]\n [0.97586224 0.97298676 0.98721221]\n [0.98891501 0.97352434 0.98860656]\n [0.98412189 0.97015159 0.991535  ]\n [0.98222141 0.9737219  0.98795799]\n [0.9838234  0.97474952 0.9920083 ]\n [0.98955914 0.97474952 0.9909368 ]\n [0.98891501 0.97013362 0.98795799]\n [0.98377612 0.97352438 0.99164588]\n [0.98825448 0.97278911 0.99252827]\n [0.98955914 0.9737848  0.99289069]\n [0.9841201  0.96836892 0.98849567]\n [0.98249579 0.9737219  0.98811374]\n [0.98955914 0.97474952 0.98720257]\n [0.97407388 0.97266579 0.98503198]\n [0.98891501 0.97278924 0.98709244]\n [0.98376066 0.97474952 0.98380723]\n [0.98611709 0.97298676 0.98567978]\n [0.98382201 0.97475077 0.98653611]\n [0.97207337 0.96955626 0.98653611]]",
          "level": 0.95
        },
        "bootstrap_predictions": "[[[0.01954387 0.96505371 0.01540242]\n  [0.96848392 0.01759407 0.01392201]\n  [0.90522012 0.04655899 0.04822089]\n  ...\n  [0.01828484 0.96380295 0.01791221]\n  [0.0056219  0.00892302 0.98545508]\n  [0.0215179  0.02666385 0.95181824]]\n\n [[0.01281634 0.02231675 0.96486691]\n  [0.01506203 0.01986721 0.96507077]\n  [0.01740505 0.01818302 0.96441193]\n  ...\n  [0.0448446  0.92373321 0.03142219]\n  [0.01068388 0.01942254 0.96989358]\n  [0.96897253 0.01986719 0.01116028]]\n\n [[0.97207337 0.01748082 0.01044581]\n  [0.00799943 0.00997503 0.98202554]\n  [0.96409996 0.01999051 0.01590953]\n  ...\n  [0.01434765 0.9737848  0.01186755]\n  [0.00955755 0.01607774 0.97436472]\n  [0.01778002 0.96637205 0.01584793]]\n\n ...\n\n [[0.98279527 0.01136454 0.0058402 ]\n  [0.98258513 0.00722992 0.01018495]\n  [0.01273363 0.97503121 0.01223516]\n  ...\n  [0.96897253 0.01986719 0.01116028]\n  [0.98669634 0.00662243 0.00668124]\n  [0.01654932 0.96647308 0.0169776 ]]\n\n [[0.9821881  0.01113392 0.00667798]\n  [0.00788993 0.03006467 0.96204541]\n  [0.01954387 0.96505371 0.01540242]\n  ...\n  [0.02777447 0.94438516 0.02784037]\n  [0.04509557 0.90482302 0.05008141]\n  [0.01506203 0.01986721 0.96507077]]\n\n [[0.0165827  0.02159107 0.96182623]\n  [0.98223108 0.00685524 0.01091367]\n  [0.00834991 0.01367123 0.97797887]\n  ...\n  [0.01545582 0.97025707 0.01428712]\n  [0.00366538 0.00380636 0.99252827]\n  [0.97207337 0.01748082 0.01044581]]]"
      },
      "success": true
    },
    "model_validation": {
      "validation_results": {
        "time_series_cv": {
          "scores": [
            1.0,
            1.0,
            1.0
          ],
          "train_scores": [
            1.0,
            1.0,
            1.0
          ],
          "overfitting_gaps": [
            0.0,
            0.0,
            0.0
          ],
          "fold_details": [
            {
              "fold": 1,
              "train_size": 144,
              "val_size": 142,
              "train_accuracy": 1.0,
              "val_accuracy": 1.0,
              "train_logloss": 0.026095349356865208,
              "val_logloss": 0.09675237329443341,
              "overfitting_gap": 0.0,
              "success": true
            },
            {
              "fold": 2,
              "train_size": 286,
              "val_size": 142,
              "train_accuracy": 1.0,
              "val_accuracy": 1.0,
              "train_logloss": 0.019209385001301062,
              "val_logloss": 0.05992041716626637,
              "overfitting_gap": 0.0,
              "success": true
            },
            {
              "fold": 3,
              "train_size": 428,
              "val_size": 142,
              "train_accuracy": 1.0,
              "val_accuracy": 1.0,
              "train_logloss": 0.01102508301367757,
              "val_logloss": 0.03732364064752772,
              "overfitting_gap": 0.0,
              "success": true
            }
          ],
          "successful_folds": 3,
          "failed_folds": 0,
          "mean_score": 1.0,
          "std_score": 0.0,
          "mean_overfitting": 0.0,
          "max_overfitting": 0.0,
          "coefficient_of_variation": 0.0
        },
        "walk_forward": {
          "scores": [
            1.0,
            1.0,
            1.0
          ],
          "train_sizes": [
            142,
            284,
            426
          ],
          "val_sizes": [
            142,
            142,
            142
          ],
          "fold_details": [
            {
              "fold": 1,
              "train_size": 142,
              "val_size": 142,
              "val_accuracy": 1.0,
              "val_logloss": 0.10535510766614918,
              "success": true
            },
            {
              "fold": 2,
              "train_size": 284,
              "val_size": 142,
              "val_accuracy": 1.0,
              "val_logloss": 0.057696710399280704,
              "success": true
            },
            {
              "fold": 3,
              "train_size": 426,
              "val_size": 142,
              "val_accuracy": 1.0,
              "val_logloss": 0.03816782912462303,
              "success": true
            }
          ],
          "successful_folds": 3,
          "failed_folds": 0,
          "mean_score": 1.0,
          "std_score": 0.0,
          "coefficient_of_variation": 0.0
        },
        "bootstrap": {
          "scores": [
            1.0,
            1.0,
            1.0,
            1.0,
            1.0,
            1.0,
            1.0,
            1.0,
            1.0,
            1.0,
            1.0,
            1.0,
            1.0,
            1.0,
            1.0,
            1.0,
            1.0,
            1.0,
            1.0,
            1.0,
            1.0,
            1.0,
            1.0,
            1.0,
            1.0,
            1.0,
            1.0,
            1.0,
            1.0,
            1.0,
            1.0,
            1.0,
            1.0,
            1.0,
            1.0,
            1.0,
            1.0,
            1.0,
            1.0,
            1.0,
            1.0,
            1.0,
            1.0,
            1.0,
            1.0,
            1.0,
            1.0,
            1.0,
            1.0,
            1.0
          ],
          "out_of_bag_scores": [
            1.0,
            1.0,
            1.0,
            1.0,
            1.0,
            1.0,
            1.0,
            1.0,
            1.0,
            1.0,
            1.0,
            1.0,
            1.0,
            1.0,
            1.0,
            1.0,
            1.0,
            1.0,
            1.0,
            1.0,
            1.0,
            1.0,
            1.0,
            1.0,
            1.0,
            1.0,
            1.0,
            1.0,
            1.0,
            1.0,
            1.0,
            1.0,
            1.0,
            1.0,
            1.0,
            1.0,
            1.0,
            1.0,
            1.0,
            1.0,
            1.0,
            1.0,
            1.0,
            1.0,
            1.0,
            1.0,
            1.0,
            1.0,
            1.0,
            1.0
          ],
          "bootstrap_details": [
            {
              "bootstrap": 1,
              "bootstrap_size": 456,
              "oob_size": 262,
              "bootstrap_accuracy": 1.0,
              "oob_accuracy": 1.0,
              "bootstrap_logloss": 0.008671698669312914,
              "oob_logloss": 0.04042422021921307,
              "success": true
            },
            {
              "bootstrap": 2,
              "bootstrap_size": 456,
              "oob_size": 269,
              "bootstrap_accuracy": 1.0,
              "oob_accuracy": 1.0,
              "bootstrap_logloss": 0.007138718788576337,
              "oob_logloss": 0.03502220366961063,
              "success": true
            },
            {
              "bootstrap": 3,
              "bootstrap_size": 456,
              "oob_size": 264,
              "bootstrap_accuracy": 1.0,
              "oob_accuracy": 1.0,
              "bootstrap_logloss": 0.007263725579353285,
              "oob_logloss": 0.03394656306172368,
              "success": true
            },
            {
              "bootstrap": 4,
              "bootstrap_size": 456,
              "oob_size": 258,
              "bootstrap_accuracy": 1.0,
              "oob_accuracy": 1.0,
              "bootstrap_logloss": 0.007403177962374466,
              "oob_logloss": 0.0368743077215698,
              "success": true
            },
            {
              "bootstrap": 5,
              "bootstrap_size": 456,
              "oob_size": 257,
              "bootstrap_accuracy": 1.0,
              "oob_accuracy": 1.0,
              "bootstrap_logloss": 0.00846100294227689,
              "oob_logloss": 0.043124266762243686,
              "success": true
            },
            {
              "bootstrap": 6,
              "bootstrap_size": 456,
              "oob_size": 261,
              "bootstrap_accuracy": 1.0,
              "oob_accuracy": 1.0,
              "bootstrap_logloss": 0.007829401179372414,
              "oob_logloss": 0.04181742036712382,
              "success": true
            },
            {
              "bootstrap": 7,
              "bootstrap_size": 456,
              "oob_size": 261,
              "bootstrap_accuracy": 1.0,
              "oob_accuracy": 1.0,
              "bootstrap_logloss": 0.008124050408561276,
              "oob_logloss": 0.03711439327489356,
              "success": true
            },
            {
              "bootstrap": 8,
              "bootstrap_size": 456,
              "oob_size": 250,
              "bootstrap_accuracy": 1.0,
              "oob_accuracy": 1.0,
              "bootstrap_logloss": 0.008486909161235484,
              "oob_logloss": 0.036460220530682506,
              "success": true
            },
            {
              "bootstrap": 9,
              "bootstrap_size": 456,
              "oob_size": 255,
              "bootstrap_accuracy": 1.0,
              "oob_accuracy": 1.0,
              "bootstrap_logloss": 0.009411064721795587,
              "oob_logloss": 0.04432300847292654,
              "success": true
            },
            {
              "bootstrap": 10,
              "bootstrap_size": 456,
              "oob_size": 259,
              "bootstrap_accuracy": 1.0,
              "oob_accuracy": 1.0,
              "bootstrap_logloss": 0.008555064765494511,
              "oob_logloss": 0.03953029434600677,
              "success": true
            },
            {
              "bootstrap": 11,
              "bootstrap_size": 456,
              "oob_size": 252,
              "bootstrap_accuracy": 1.0,
              "oob_accuracy": 1.0,
              "bootstrap_logloss": 0.008089202422976489,
              "oob_logloss": 0.03700233172787841,
              "success": true
            },
            {
              "bootstrap": 12,
              "bootstrap_size": 456,
              "oob_size": 267,
              "bootstrap_accuracy": 1.0,
              "oob_accuracy": 1.0,
              "bootstrap_logloss": 0.008780327674477569,
              "oob_logloss": 0.04248351037085125,
              "success": true
            },
            {
              "bootstrap": 13,
              "bootstrap_size": 456,
              "oob_size": 257,
              "bootstrap_accuracy": 1.0,
              "oob_accuracy": 1.0,
              "bootstrap_logloss": 0.0071452204956352575,
              "oob_logloss": 0.041236837323802185,
              "success": true
            },
            {
              "bootstrap": 14,
              "bootstrap_size": 456,
              "oob_size": 250,
              "bootstrap_accuracy": 1.0,
              "oob_accuracy": 1.0,
              "bootstrap_logloss": 0.0072797910322601085,
              "oob_logloss": 0.03833052200195446,
              "success": true
            },
            {
              "bootstrap": 15,
              "bootstrap_size": 456,
              "oob_size": 255,
              "bootstrap_accuracy": 1.0,
              "oob_accuracy": 1.0,
              "bootstrap_logloss": 0.009675738249116179,
              "oob_logloss": 0.03611376022592214,
              "success": true
            },
            {
              "bootstrap": 16,
              "bootstrap_size": 456,
              "oob_size": 258,
              "bootstrap_accuracy": 1.0,
              "oob_accuracy": 1.0,
              "bootstrap_logloss": 0.009874813921574209,
              "oob_logloss": 0.04409413562386913,
              "success": true
            },
            {
              "bootstrap": 17,
              "bootstrap_size": 456,
              "oob_size": 270,
              "bootstrap_accuracy": 1.0,
              "oob_accuracy": 1.0,
              "bootstrap_logloss": 0.007079354856348595,
              "oob_logloss": 0.029862823179560644,
              "success": true
            },
            {
              "bootstrap": 18,
              "bootstrap_size": 456,
              "oob_size": 262,
              "bootstrap_accuracy": 1.0,
              "oob_accuracy": 1.0,
              "bootstrap_logloss": 0.006627081793269864,
              "oob_logloss": 0.026032858977832706,
              "success": true
            },
            {
              "bootstrap": 19,
              "bootstrap_size": 456,
              "oob_size": 258,
              "bootstrap_accuracy": 1.0,
              "oob_accuracy": 1.0,
              "bootstrap_logloss": 0.008813455912941746,
              "oob_logloss": 0.037052610139035706,
              "success": true
            },
            {
              "bootstrap": 20,
              "bootstrap_size": 456,
              "oob_size": 268,
              "bootstrap_accuracy": 1.0,
              "oob_accuracy": 1.0,
              "bootstrap_logloss": 0.00886029460206048,
              "oob_logloss": 0.03482454711775662,
              "success": true
            },
            {
              "bootstrap": 21,
              "bootstrap_size": 456,
              "oob_size": 254,
              "bootstrap_accuracy": 1.0,
              "oob_accuracy": 1.0,
              "bootstrap_logloss": 0.00835759685923243,
              "oob_logloss": 0.03941873139551504,
              "success": true
            },
            {
              "bootstrap": 22,
              "bootstrap_size": 456,
              "oob_size": 254,
              "bootstrap_accuracy": 1.0,
              "oob_accuracy": 1.0,
              "bootstrap_logloss": 0.008880562739282864,
              "oob_logloss": 0.0458667766473659,
              "success": true
            },
            {
              "bootstrap": 23,
              "bootstrap_size": 456,
              "oob_size": 247,
              "bootstrap_accuracy": 1.0,
              "oob_accuracy": 1.0,
              "bootstrap_logloss": 0.008526289371792941,
              "oob_logloss": 0.033442604756615445,
              "success": true
            },
            {
              "bootstrap": 24,
              "bootstrap_size": 456,
              "oob_size": 254,
              "bootstrap_accuracy": 1.0,
              "oob_accuracy": 1.0,
              "bootstrap_logloss": 0.008343134110277556,
              "oob_logloss": 0.03912277182710101,
              "success": true
            },
            {
              "bootstrap": 25,
              "bootstrap_size": 456,
              "oob_size": 255,
              "bootstrap_accuracy": 1.0,
              "oob_accuracy": 1.0,
              "bootstrap_logloss": 0.01058290160064445,
              "oob_logloss": 0.053536435421965865,
              "success": true
            },
            {
              "bootstrap": 26,
              "bootstrap_size": 456,
              "oob_size": 266,
              "bootstrap_accuracy": 1.0,
              "oob_accuracy": 1.0,
              "bootstrap_logloss": 0.007223013453196037,
              "oob_logloss": 0.029076611769610523,
              "success": true
            },
            {
              "bootstrap": 27,
              "bootstrap_size": 456,
              "oob_size": 254,
              "bootstrap_accuracy": 1.0,
              "oob_accuracy": 1.0,
              "bootstrap_logloss": 0.00902511180663014,
              "oob_logloss": 0.03923638156107071,
              "success": true
            },
            {
              "bootstrap": 28,
              "bootstrap_size": 456,
              "oob_size": 256,
              "bootstrap_accuracy": 1.0,
              "oob_accuracy": 1.0,
              "bootstrap_logloss": 0.007993445336426923,
              "oob_logloss": 0.04280713486159281,
              "success": true
            },
            {
              "bootstrap": 29,
              "bootstrap_size": 456,
              "oob_size": 250,
              "bootstrap_accuracy": 1.0,
              "oob_accuracy": 1.0,
              "bootstrap_logloss": 0.00788880297799848,
              "oob_logloss": 0.03310415000097538,
              "success": true
            },
            {
              "bootstrap": 30,
              "bootstrap_size": 456,
              "oob_size": 258,
              "bootstrap_accuracy": 1.0,
              "oob_accuracy": 1.0,
              "bootstrap_logloss": 0.00781658151317873,
              "oob_logloss": 0.04238171778785546,
              "success": true
            },
            {
              "bootstrap": 31,
              "bootstrap_size": 456,
              "oob_size": 267,
              "bootstrap_accuracy": 1.0,
              "oob_accuracy": 1.0,
              "bootstrap_logloss": 0.00789542818007044,
              "oob_logloss": 0.03767126218479709,
              "success": true
            },
            {
              "bootstrap": 32,
              "bootstrap_size": 456,
              "oob_size": 263,
              "bootstrap_accuracy": 1.0,
              "oob_accuracy": 1.0,
              "bootstrap_logloss": 0.009104630938634162,
              "oob_logloss": 0.04617246476620297,
              "success": true
            },
            {
              "bootstrap": 33,
              "bootstrap_size": 456,
              "oob_size": 258,
              "bootstrap_accuracy": 1.0,
              "oob_accuracy": 1.0,
              "bootstrap_logloss": 0.008720985844723101,
              "oob_logloss": 0.03589493420402042,
              "success": true
            },
            {
              "bootstrap": 34,
              "bootstrap_size": 456,
              "oob_size": 246,
              "bootstrap_accuracy": 1.0,
              "oob_accuracy": 1.0,
              "bootstrap_logloss": 0.008718922555480036,
              "oob_logloss": 0.03598668872685558,
              "success": true
            },
            {
              "bootstrap": 35,
              "bootstrap_size": 456,
              "oob_size": 246,
              "bootstrap_accuracy": 1.0,
              "oob_accuracy": 1.0,
              "bootstrap_logloss": 0.00980503855083397,
              "oob_logloss": 0.04281311550406163,
              "success": true
            },
            {
              "bootstrap": 36,
              "bootstrap_size": 456,
              "oob_size": 247,
              "bootstrap_accuracy": 1.0,
              "oob_accuracy": 1.0,
              "bootstrap_logloss": 0.007649753701916742,
              "oob_logloss": 0.02721663169270128,
              "success": true
            },
            {
              "bootstrap": 37,
              "bootstrap_size": 456,
              "oob_size": 254,
              "bootstrap_accuracy": 1.0,
              "oob_accuracy": 1.0,
              "bootstrap_logloss": 0.008179082691529725,
              "oob_logloss": 0.03432300842572824,
              "success": true
            },
            {
              "bootstrap": 38,
              "bootstrap_size": 456,
              "oob_size": 264,
              "bootstrap_accuracy": 1.0,
              "oob_accuracy": 1.0,
              "bootstrap_logloss": 0.007797646184557483,
              "oob_logloss": 0.03621068518452596,
              "success": true
            },
            {
              "bootstrap": 39,
              "bootstrap_size": 456,
              "oob_size": 256,
              "bootstrap_accuracy": 1.0,
              "oob_accuracy": 1.0,
              "bootstrap_logloss": 0.008038461341756576,
              "oob_logloss": 0.032732593417834396,
              "success": true
            },
            {
              "bootstrap": 40,
              "bootstrap_size": 456,
              "oob_size": 253,
              "bootstrap_accuracy": 1.0,
              "oob_accuracy": 1.0,
              "bootstrap_logloss": 0.007792905604928176,
              "oob_logloss": 0.030813950874896297,
              "success": true
            },
            {
              "bootstrap": 41,
              "bootstrap_size": 456,
              "oob_size": 242,
              "bootstrap_accuracy": 1.0,
              "oob_accuracy": 1.0,
              "bootstrap_logloss": 0.009067227554916468,
              "oob_logloss": 0.03658469727703237,
              "success": true
            },
            {
              "bootstrap": 42,
              "bootstrap_size": 456,
              "oob_size": 266,
              "bootstrap_accuracy": 1.0,
              "oob_accuracy": 1.0,
              "bootstrap_logloss": 0.009527610019729908,
              "oob_logloss": 0.044870467135983985,
              "success": true
            },
            {
              "bootstrap": 43,
              "bootstrap_size": 456,
              "oob_size": 249,
              "bootstrap_accuracy": 1.0,
              "oob_accuracy": 1.0,
              "bootstrap_logloss": 0.008346682480416694,
              "oob_logloss": 0.039266647543142616,
              "success": true
            },
            {
              "bootstrap": 44,
              "bootstrap_size": 456,
              "oob_size": 265,
              "bootstrap_accuracy": 1.0,
              "oob_accuracy": 1.0,
              "bootstrap_logloss": 0.007399443463327715,
              "oob_logloss": 0.03276469457669375,
              "success": true
            },
            {
              "bootstrap": 45,
              "bootstrap_size": 456,
              "oob_size": 247,
              "bootstrap_accuracy": 1.0,
              "oob_accuracy": 1.0,
              "bootstrap_logloss": 0.006166065839373863,
              "oob_logloss": 0.032061915969938695,
              "success": true
            },
            {
              "bootstrap": 46,
              "bootstrap_size": 456,
              "oob_size": 258,
              "bootstrap_accuracy": 1.0,
              "oob_accuracy": 1.0,
              "bootstrap_logloss": 0.008661761569987042,
              "oob_logloss": 0.03838790955305712,
              "success": true
            },
            {
              "bootstrap": 47,
              "bootstrap_size": 456,
              "oob_size": 247,
              "bootstrap_accuracy": 1.0,
              "oob_accuracy": 1.0,
              "bootstrap_logloss": 0.006835045210074675,
              "oob_logloss": 0.03758795968014183,
              "success": true
            },
            {
              "bootstrap": 48,
              "bootstrap_size": 456,
              "oob_size": 258,
              "bootstrap_accuracy": 1.0,
              "oob_accuracy": 1.0,
              "bootstrap_logloss": 0.007865821658413008,
              "oob_logloss": 0.035537940696879426,
              "success": true
            },
            {
              "bootstrap": 49,
              "bootstrap_size": 456,
              "oob_size": 250,
              "bootstrap_accuracy": 1.0,
              "oob_accuracy": 1.0,
              "bootstrap_logloss": 0.00956019508096262,
              "oob_logloss": 0.04465684939081085,
              "success": true
            },
            {
              "bootstrap": 50,
              "bootstrap_size": 456,
              "oob_size": 245,
              "bootstrap_accuracy": 1.0,
              "oob_accuracy": 1.0,
              "bootstrap_logloss": 0.00789803644852038,
              "oob_logloss": 0.03168209717618347,
              "success": true
            }
          ],
          "successful_bootstraps": 50,
          "failed_bootstraps": 0,
          "mean_score": 1.0,
          "std_score": 0.0,
          "mean_oob_score": 1.0,
          "std_oob_score": 0.0,
          "optimism": 0.0
        },
        "calibration": {
          "original": {
            "brier_score": 0.003485964912280703,
            "reliability": 0.0,
            "sharpness": 0.20180526315789477,
            "calibration_error": 0.032368421052631595
          },
          "calibrated": {
            "brier_score": 3.399279256131833e-05,
            "reliability": 0.0,
            "sharpness": 0.22161506418083565,
            "calibration_error": 0.0009277334583605861
          },
          "improvement": {
            "brier_score": 0.0034519721197193848,
            "calibration_error": 0.03144068759427101
          }
        },
        "overfitting": {
          "detected": false,
          "severity": "none",
          "metrics": {
            "mean_overfitting": 0.0,
            "max_overfitting": 0.0
          },
          "recommendations": []
        },
        "stability": {
          "stable": true,
          "metrics": {
            "coefficient_of_variation": 0.0,
            "std_score": 0.0,
            "walk_forward_cv": 0.0,
            "optimism": 0.0
          },
          "issues": []
        },
        "overall": {
          "validation_passed": true,
          "overall_score": 1.0,
          "issues": [],
          "recommendations": [
            "Model validation passed successfully"
          ]
        }
      },
      "report": "================================================================================\nNON-MAJOR LEAGUE MODEL VALIDATION REPORT\n================================================================================\n\nOVERALL ASSESSMENT:\n  Validation Status: PASSED\n  Overall Score: 1.0000\n\n  Recommendations:\n    - Model validation passed successfully\n\nDETAILED VALIDATION RESULTS:\n----------------------------------------\n\nTime Series Cross-Validation:\n  Successful folds: 3\n  Failed folds: 0\n  Mean accuracy: 1.0000\n  Std accuracy: 0.0000\n  Coefficient of variation: 0.0000\n  Mean overfitting: 0.0000\n  Max overfitting: 0.0000\n\nWalk-Forward Validation:\n  Successful folds: 3\n  Failed folds: 0\n  Mean accuracy: 1.0000\n  Std accuracy: 0.0000\n  Coefficient of variation: 0.0000\n\nBootstrap Validation:\n  Successful bootstraps: 50\n  Failed bootstraps: 0\n  Mean accuracy: 1.0000\n  Mean OOB accuracy: 1.0000\n  Optimism: 0.0000\n\nModel Calibration:\n  Original Brier score: 0.0035\n  Original calibration error: 0.0324\n  Calibrated Brier score: 0.0000\n  Calibrated calibration error: 0.0009\n  Brier score improvement: +0.0035\n  Calibration error improvement: +0.0314\n\nOverfitting Detection:\n  Overfitting detected: NO\n  Severity: none\n\nStability Analysis:\n  Model stable: YES\n\n================================================================================",
      "success": true
    }
  },
  "timestamp": "2025-10-16T00:13:51.237604"
}