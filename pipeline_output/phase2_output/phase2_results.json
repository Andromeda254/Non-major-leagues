{
  "architecture_results": {
    "base_models": {
      "xgboost": "XGBClassifier(base_score=None, booster=None, callbacks=None,\n              colsample_bylevel=None, colsample_bynode=None,\n              colsample_bytree=0.8, device=None, early_stopping_rounds=None,\n              enable_categorical=False, eval_metric='mlogloss',\n              feature_types=None, feature_weights=None, gamma=None,\n              grow_policy=None, importance_type=None,\n              interaction_constraints=None, learning_rate=0.05, max_bin=None,\n              max_cat_threshold=None, max_cat_to_onehot=None,\n              max_delta_step=None, max_depth=6, max_leaves=None,\n              min_child_weight=3, missing=nan, monotone_constraints=None,\n              multi_strategy=None, n_estimators=200, n_jobs=None,\n              num_parallel_tree=None, ...)",
      "lightgbm": "LGBMClassifier(bagging_fraction=0.8, bagging_freq=5, feature_fraction=0.8,\n               lambda_l1=0.1, lambda_l2=1.0, learning_rate=0.05, max_depth=6,\n               metric='multi_logloss', min_data_in_leaf=20, n_estimators=200,\n               num_class=3, num_leaves=50, objective='multiclass',\n               random_state=42, verbose=-1)",
      "random_forest": "RandomForestClassifier(class_weight='balanced', max_depth=10,\n                       min_samples_leaf=2, min_samples_split=5,\n                       random_state=42)",
      "logistic_regression": "LogisticRegression(class_weight='balanced', max_iter=1000, random_state=42)"
    },
    "training_results": {
      "xgboost": {
        "model": "XGBClassifier(base_score=None, booster=None, callbacks=None,\n              colsample_bylevel=None, colsample_bynode=None,\n              colsample_bytree=0.8, device=None, early_stopping_rounds=None,\n              enable_categorical=False, eval_metric='mlogloss',\n              feature_types=None, feature_weights=None, gamma=None,\n              grow_policy=None, importance_type=None,\n              interaction_constraints=None, learning_rate=0.05, max_bin=None,\n              max_cat_threshold=None, max_cat_to_onehot=None,\n              max_delta_step=None, max_depth=6, max_leaves=None,\n              min_child_weight=3, missing=nan, monotone_constraints=None,\n              multi_strategy=None, n_estimators=200, n_jobs=None,\n              num_parallel_tree=None, ...)",
        "train_accuracy": 1.0,
        "val_accuracy": 1.0,
        "train_logloss": 0.010888727160769005,
        "val_logloss": 0.010889639223756489,
        "success": true
      },
      "lightgbm": {
        "model": "LGBMClassifier(bagging_fraction=0.8, bagging_freq=5, feature_fraction=0.8,\n               lambda_l1=0.1, lambda_l2=1.0, learning_rate=0.05, max_depth=6,\n               metric='multi_logloss', min_data_in_leaf=20, n_estimators=200,\n               num_class=3, num_leaves=50, objective='multiclass',\n               random_state=42, verbose=-1)",
        "train_accuracy": 1.0,
        "val_accuracy": 1.0,
        "train_logloss": 0.0012882157434589115,
        "val_logloss": 0.001284234129843217,
        "success": true
      },
      "random_forest": {
        "model": "RandomForestClassifier(class_weight='balanced', max_depth=10,\n                       min_samples_leaf=2, min_samples_split=5,\n                       random_state=42)",
        "train_accuracy": 1.0,
        "val_accuracy": 1.0,
        "train_logloss": 0.015337317409059753,
        "val_logloss": 0.029023413348350846,
        "success": true
      },
      "logistic_regression": {
        "model": "LogisticRegression(class_weight='balanced', max_iter=1000, random_state=42)",
        "train_accuracy": 1.0,
        "val_accuracy": 1.0,
        "train_logloss": 0.003133323409808377,
        "val_logloss": 0.00409310790726011,
        "success": true
      }
    },
    "ensemble_model": "VotingClassifier(estimators=[('xgboost',\n                              XGBClassifier(base_score=None, booster=None,\n                                            callbacks=None,\n                                            colsample_bylevel=None,\n                                            colsample_bynode=None,\n                                            colsample_bytree=0.8, device=None,\n                                            early_stopping_rounds=None,\n                                            enable_categorical=False,\n                                            eval_metric='mlogloss',\n                                            feature_types=None,\n                                            feature_weights=None, gamma=None,\n                                            grow_policy=None,\n                                            importance_type=None,\n                                            interactio...\n                                             min_data_in_leaf=20,\n                                             n_estimators=200, num_class=3,\n                                             num_leaves=50,\n                                             objective='multiclass',\n                                             random_state=42, verbose=-1)),\n                             ('random_forest',\n                              RandomForestClassifier(class_weight='balanced',\n                                                     max_depth=10,\n                                                     min_samples_leaf=2,\n                                                     min_samples_split=5,\n                                                     random_state=42)),\n                             ('logistic_regression',\n                              LogisticRegression(class_weight='balanced',\n                                                 max_iter=1000,\n                                                 random_state=42))],\n                 voting='soft')",
    "ensemble_results": {
      "model": "VotingClassifier(estimators=[('xgboost',\n                              XGBClassifier(base_score=None, booster=None,\n                                            callbacks=None,\n                                            colsample_bylevel=None,\n                                            colsample_bynode=None,\n                                            colsample_bytree=0.8, device=None,\n                                            early_stopping_rounds=None,\n                                            enable_categorical=False,\n                                            eval_metric='mlogloss',\n                                            feature_types=None,\n                                            feature_weights=None, gamma=None,\n                                            grow_policy=None,\n                                            importance_type=None,\n                                            interactio...\n                                             min_data_in_leaf=20,\n                                             n_estimators=200, num_class=3,\n                                             num_leaves=50,\n                                             objective='multiclass',\n                                             random_state=42, verbose=-1)),\n                             ('random_forest',\n                              RandomForestClassifier(class_weight='balanced',\n                                                     max_depth=10,\n                                                     min_samples_leaf=2,\n                                                     min_samples_split=5,\n                                                     random_state=42)),\n                             ('logistic_regression',\n                              LogisticRegression(class_weight='balanced',\n                                                 max_iter=1000,\n                                                 random_state=42))],\n                 voting='soft')",
      "train_accuracy": 1.0,
      "val_accuracy": 1.0,
      "train_logloss": 0.007579608973137412,
      "val_logloss": 0.011132555756122146,
      "success": true
    },
    "calibrated_models": {
      "xgboost": "CalibratedClassifierCV(cv=3,\n                       estimator=XGBClassifier(base_score=None, booster=None,\n                                               callbacks=None,\n                                               colsample_bylevel=None,\n                                               colsample_bynode=None,\n                                               colsample_bytree=0.8,\n                                               device=None,\n                                               early_stopping_rounds=None,\n                                               enable_categorical=False,\n                                               eval_metric='mlogloss',\n                                               feature_types=None,\n                                               feature_weights=None, gamma=None,\n                                               grow_policy=None,\n                                               importance_type=None,\n                                               interaction_constraints=None,\n                                               learning_rate=0.05, max_bin=None,\n                                               max_cat_threshold=None,\n                                               max_cat_to_onehot=None,\n                                               max_delta_step=None, max_depth=6,\n                                               max_leaves=None,\n                                               min_child_weight=3, missing=nan,\n                                               monotone_constraints=None,\n                                               multi_strategy=None,\n                                               n_estimators=200, n_jobs=None,\n                                               num_parallel_tree=None, ...),\n                       method='isotonic')",
      "lightgbm": "CalibratedClassifierCV(cv=3,\n                       estimator=LGBMClassifier(bagging_fraction=0.8,\n                                                bagging_freq=5,\n                                                feature_fraction=0.8,\n                                                lambda_l1=0.1, lambda_l2=1.0,\n                                                learning_rate=0.05, max_depth=6,\n                                                metric='multi_logloss',\n                                                min_data_in_leaf=20,\n                                                n_estimators=200, num_class=3,\n                                                num_leaves=50,\n                                                objective='multiclass',\n                                                random_state=42, verbose=-1),\n                       method='isotonic')",
      "random_forest": "CalibratedClassifierCV(cv=3,\n                       estimator=RandomForestClassifier(class_weight='balanced',\n                                                        max_depth=10,\n                                                        min_samples_leaf=2,\n                                                        min_samples_split=5,\n                                                        random_state=42),\n                       method='isotonic')",
      "logistic_regression": "CalibratedClassifierCV(cv=3,\n                       estimator=LogisticRegression(class_weight='balanced',\n                                                    max_iter=1000,\n                                                    random_state=42),\n                       method='isotonic')",
      "ensemble": "CalibratedClassifierCV(cv=3,\n                       estimator=VotingClassifier(estimators=[('xgboost',\n                                                               XGBClassifier(base_score=None,\n                                                                             booster=None,\n                                                                             callbacks=None,\n                                                                             colsample_bylevel=None,\n                                                                             colsample_bynode=None,\n                                                                             colsample_bytree=0.8,\n                                                                             device=None,\n                                                                             early_stopping_rounds=None,\n                                                                             enable_categorical=False,\n                                                                             eval_metric='mlogloss',\n                                                                             feature_types=None,\n                                                                             feature_weights=None,\n                                                                             gamma=None,\n                                                                             grow_polic...\n                                                                              n_estimators=200,\n                                                                              num_class=3,\n                                                                              num_leaves=50,\n                                                                              objective='multiclass',\n                                                                              random_state=42,\n                                                                              verbose=-1)),\n                                                              ('random_forest',\n                                                               RandomForestClassifier(class_weight='balanced',\n                                                                                      max_depth=10,\n                                                                                      min_samples_leaf=2,\n                                                                                      min_samples_split=5,\n                                                                                      random_state=42)),\n                                                              ('logistic_regression',\n                                                               LogisticRegression(class_weight='balanced',\n                                                                                  max_iter=1000,\n                                                                                  random_state=42))],\n                                                  voting='soft'),\n                       method='isotonic')"
    },
    "evaluation_results": {
      "xgboost": {
        "accuracy": 1.0,
        "log_loss": 0.010854325623035364,
        "predictions": "[0 2 2 0 2 2 2 1 2 1 1 1 2 2 0 2 0 2 2 0 1 2 2 0 2 2 0 2 0 2 2 1 2 1 2 2 1\n 2 2 2 0 0 1 2 0 2 0 1 0 2 2 2 1 2 1 2 1 0 2 0 0 0 0 2 2 2 2 1 2 2 0 0 1 0\n 1 2 1 2 0 0 2 2 1 0 0 2 2 0 1 0 0 0 2 2 1 2 2 2 0 1 2 0 2 2 2 0 2 2 1 1 0\n 2 0 2 1 2 2 2 2 0 2 0 0 0 0 2 2 1 2 2 0 0 0 0 2 1 2 2 2 0 1 1 0 1 0 0 0 1\n 0 0 0 2 1 1]",
        "probabilities": "[[0.9891468  0.00551647 0.00533676]\n [0.00514989 0.00537398 0.9894761 ]\n [0.00529043 0.00543968 0.9892699 ]\n [0.9891398  0.00552345 0.00533673]\n [0.00515026 0.00530303 0.9895467 ]\n [0.00514997 0.00535859 0.98949146]\n [0.00526134 0.00543304 0.9893056 ]\n [0.00563348 0.98873377 0.00563272]\n [0.00526124 0.00545167 0.989287  ]\n [0.00558653 0.9887966  0.00561686]\n [0.00563348 0.98873377 0.00563272]\n [0.00560248 0.98876464 0.00563289]\n [0.00514989 0.00537398 0.9894761 ]\n [0.00515007 0.00534028 0.9895097 ]\n [0.98926765 0.00542446 0.00530787]\n [0.00515018 0.00531825 0.9895315 ]\n [0.9892453  0.00544692 0.00530775]\n [0.00529055 0.00541733 0.98929214]\n [0.00514989 0.00537398 0.9894761 ]\n [0.9892559  0.00543625 0.00530781]\n [0.00660472 0.9866099  0.00678545]\n [0.00515011 0.00533165 0.9895182 ]\n [0.00526104 0.00548996 0.989249  ]\n [0.9891122  0.00546496 0.00542282]\n [0.00526134 0.00543304 0.9893056 ]\n [0.00515003 0.00534697 0.989503  ]\n [0.9891517  0.00551146 0.00533679]\n [0.00515015 0.00532492 0.98952496]\n [0.98922056 0.00547187 0.00530762]\n [0.00514989 0.00537398 0.9894761 ]\n [0.00526104 0.00548996 0.989249  ]\n [0.0055738  0.98869985 0.00572632]\n [0.00515026 0.00530303 0.9895467 ]\n [0.00558585 0.9886755  0.0057387 ]\n [0.00515018 0.00531825 0.9895315 ]\n [0.00515018 0.00531825 0.9895315 ]\n [0.00550131 0.9889675  0.00553117]\n [0.00515018 0.00531825 0.9895315 ]\n [0.00515026 0.00530303 0.9895467 ]\n [0.0052613  0.00543984 0.9892988 ]\n [0.9892267  0.0054656  0.00530765]\n [0.98922414 0.00543871 0.00533718]\n [0.00553109 0.98881793 0.00565099]\n [0.00526134 0.00543304 0.9893056 ]\n [0.98914635 0.00543069 0.00542301]\n [0.00514997 0.00535859 0.98949146]\n [0.98922414 0.00543871 0.00533718]\n [0.00553582 0.98889834 0.00556587]\n [0.9891468  0.00551647 0.00533676]\n [0.00515018 0.00531825 0.9895315 ]\n [0.00529043 0.00543968 0.9892699 ]\n [0.00515011 0.00533165 0.9895182 ]\n [0.0055366  0.98695403 0.00750937]\n [0.00514989 0.00537398 0.9894761 ]\n [0.00560248 0.98876464 0.00563289]\n [0.00514997 0.00535859 0.98949146]\n [0.00563279 0.98861235 0.0057549 ]\n [0.9891089  0.00543807 0.00545299]\n [0.00515003 0.00534697 0.989503  ]\n [0.98916745 0.00549569 0.00533688]\n [0.98922414 0.00543871 0.00533718]\n [0.9892831  0.00540893 0.00530796]\n [0.98905224 0.00549505 0.00545267]\n [0.00515026 0.00530303 0.9895467 ]\n [0.00517884 0.00529137 0.9895298 ]\n [0.00517855 0.00534681 0.98947465]\n [0.0051504  0.00527637 0.9895732 ]\n [0.00560465 0.9886692  0.00572614]\n [0.00526112 0.00547424 0.98926467]\n [0.00514997 0.00535859 0.98949146]\n [0.98917425 0.00548881 0.00533691]\n [0.9891517  0.00551146 0.00533679]\n [0.00553582 0.98889834 0.00556587]\n [0.9892831  0.00540893 0.00530796]\n [0.00555861 0.9888526  0.00558879]\n [0.00526142 0.00541748 0.9893211 ]\n [0.00554405 0.98876023 0.00569576]\n [0.00517863 0.0053315  0.98948985]\n [0.9891398  0.00552345 0.00533673]\n [0.9892111  0.00548129 0.00530757]\n [0.00514997 0.00535859 0.98949146]\n [0.00515018 0.00531825 0.9895315 ]\n [0.00550065 0.9888482  0.00565116]\n [0.98916745 0.00549569 0.00533688]\n [0.9892151  0.00542417 0.00536079]\n [0.00514989 0.00537398 0.9894761 ]\n [0.00529017 0.0054898  0.9892201 ]\n [0.9891122  0.00546496 0.00542282]\n [0.0055738  0.98869985 0.00572632]\n [0.98924035 0.00545186 0.00530773]\n [0.9892267  0.0054656  0.00530765]\n [0.9892831  0.00540893 0.00530796]\n [0.00526104 0.00548996 0.989249  ]\n [0.00515032 0.00529152 0.98955816]\n [0.00557448 0.9888208  0.00560474]\n [0.00514997 0.00535859 0.98949146]\n [0.0051787  0.0053181  0.98950315]\n [0.00515011 0.00533165 0.9895182 ]\n [0.9892831  0.00540893 0.00530796]\n [0.00560248 0.98876464 0.00563289]\n [0.00526112 0.00547424 0.98926467]\n [0.98915523 0.00550795 0.00533681]\n [0.00514989 0.00537398 0.9894761 ]\n [0.0051787  0.0053181  0.98950315]\n [0.0051787  0.0053181  0.98950315]\n [0.98926765 0.00542446 0.00530787]\n [0.00529029 0.00546716 0.9892426 ]\n [0.00515018 0.00531825 0.9895315 ]\n [0.00558653 0.9887966  0.00561686]\n [0.00555793 0.988732   0.00571002]\n [0.98919934 0.0054932  0.00530751]\n [0.00526112 0.00547424 0.98926467]\n [0.98926765 0.00542446 0.00530787]\n [0.00515003 0.00534697 0.989503  ]\n [0.00551702 0.988936   0.00554697]\n [0.00515011 0.00533165 0.9895182 ]\n [0.0051504  0.00527637 0.9895732 ]\n [0.00517878 0.00530288 0.98951834]\n [0.00526112 0.00547424 0.98926467]\n [0.9891812  0.00548188 0.00533695]\n [0.00515011 0.00533165 0.9895182 ]\n [0.9892831  0.00540893 0.00530796]\n [0.9891812  0.00548188 0.00533695]\n [0.9891517  0.00551146 0.00533679]\n [0.9892018  0.00546123 0.00533706]\n [0.00526157 0.00539025 0.9893482 ]\n [0.00515003 0.00534697 0.989503  ]\n [0.00550131 0.9889675  0.00553117]\n [0.00515018 0.00531825 0.9895315 ]\n [0.00526112 0.00547424 0.98926467]\n [0.9891685  0.0054083  0.00542313]\n [0.9892831  0.00540893 0.00530796]\n [0.98913085 0.00544629 0.00542292]\n [0.98912406 0.0055393  0.00533664]\n [0.00515018 0.00531825 0.9895315 ]\n [0.00558906 0.9887666  0.00564432]\n [0.00514997 0.00535859 0.98949146]\n [0.00515032 0.00529152 0.98955816]\n [0.0051504  0.00527637 0.9895732 ]\n [0.9892336  0.00545876 0.00530769]\n [0.00561745 0.9887659  0.00561668]\n [0.00550065 0.9888482  0.00565116]\n [0.98922414 0.00543871 0.00533718]\n [0.00557951 0.98881066 0.0056098 ]\n [0.9892111  0.00548129 0.00530757]\n [0.9892267  0.0054656  0.00530765]\n [0.9891967  0.00546618 0.00533703]\n [0.0055738  0.98869985 0.00572632]\n [0.9892831  0.00540893 0.00530796]\n [0.9892831  0.00540893 0.00530796]\n [0.98905224 0.00549505 0.00545267]\n [0.00517884 0.00529137 0.9895298 ]\n [0.00553176 0.98893726 0.005531  ]\n [0.00553176 0.98893726 0.005531  ]]"
      },
      "lightgbm": {
        "accuracy": 1.0,
        "log_loss": 0.0012796861394519484,
        "predictions": "[0 2 2 0 2 2 2 1 2 1 1 1 2 2 0 2 0 2 2 0 1 2 2 0 2 2 0 2 0 2 2 1 2 1 2 2 1\n 2 2 2 0 0 1 2 0 2 0 1 0 2 2 2 1 2 1 2 1 0 2 0 0 0 0 2 2 2 2 1 2 2 0 0 1 0\n 1 2 1 2 0 0 2 2 1 0 0 2 2 0 1 0 0 0 2 2 1 2 2 2 0 1 2 0 2 2 2 0 2 2 1 1 0\n 2 0 2 1 2 2 2 2 0 2 0 0 0 0 2 2 1 2 2 0 0 0 0 2 1 2 2 2 0 1 1 0 1 0 0 0 1\n 0 0 0 2 1 1]",
        "probabilities": "[[9.98695110e-01 6.48569085e-04 6.56320948e-04]\n [6.18282640e-04 6.23597953e-04 9.98758119e-01]\n [6.18283350e-04 6.22450468e-04 9.98759266e-01]\n [9.98693915e-01 6.49764689e-04 6.56320163e-04]\n [6.18283350e-04 6.22450468e-04 9.98759266e-01]\n [6.18283350e-04 6.22450468e-04 9.98759266e-01]\n [6.18283350e-04 6.22450468e-04 9.98759266e-01]\n [6.52200159e-04 9.98681881e-01 6.65919282e-04]\n [6.18283350e-04 6.22450468e-04 9.98759266e-01]\n [6.52200159e-04 9.98681881e-01 6.65919282e-04]\n [6.52200159e-04 9.98681881e-01 6.65919282e-04]\n [6.52200159e-04 9.98681881e-01 6.65919282e-04]\n [6.18283350e-04 6.22450468e-04 9.98759266e-01]\n [6.18282640e-04 6.23597953e-04 9.98758119e-01]\n [9.98693915e-01 6.49764689e-04 6.56320163e-04]\n [6.18283350e-04 6.22450468e-04 9.98759266e-01]\n [9.98695110e-01 6.48569085e-04 6.56320948e-04]\n [6.18283350e-04 6.22450468e-04 9.98759266e-01]\n [6.18283350e-04 6.22450468e-04 9.98759266e-01]\n [9.98693915e-01 6.49764689e-04 6.56320163e-04]\n [6.78434794e-04 9.98655663e-01 6.65901801e-04]\n [6.18282640e-04 6.23597953e-04 9.98758119e-01]\n [6.18283350e-04 6.22450468e-04 9.98759266e-01]\n [9.98695110e-01 6.48569085e-04 6.56320948e-04]\n [6.18283350e-04 6.22450468e-04 9.98759266e-01]\n [6.18283350e-04 6.22450468e-04 9.98759266e-01]\n [9.98693915e-01 6.49764689e-04 6.56320163e-04]\n [6.18282640e-04 6.23597953e-04 9.98758119e-01]\n [9.98693915e-01 6.49764689e-04 6.56320163e-04]\n [6.18282640e-04 6.23597953e-04 9.98758119e-01]\n [6.18283350e-04 6.22450468e-04 9.98759266e-01]\n [6.52200159e-04 9.98681881e-01 6.65919282e-04]\n [6.18283350e-04 6.22450468e-04 9.98759266e-01]\n [6.52200159e-04 9.98681881e-01 6.65919282e-04]\n [6.18283350e-04 6.22450468e-04 9.98759266e-01]\n [6.18283350e-04 6.22450468e-04 9.98759266e-01]\n [6.52200159e-04 9.98681881e-01 6.65919282e-04]\n [6.18283350e-04 6.22450468e-04 9.98759266e-01]\n [6.18283350e-04 6.22450468e-04 9.98759266e-01]\n [6.18282640e-04 6.23597953e-04 9.98758119e-01]\n [9.98695110e-01 6.48569085e-04 6.56320948e-04]\n [9.98695110e-01 6.48569085e-04 6.56320948e-04]\n [6.52200159e-04 9.98681881e-01 6.65919282e-04]\n [6.18282640e-04 6.23597953e-04 9.98758119e-01]\n [9.98695110e-01 6.48569085e-04 6.56320948e-04]\n [6.18282640e-04 6.23597953e-04 9.98758119e-01]\n [9.98693915e-01 6.49764689e-04 6.56320163e-04]\n [6.52200159e-04 9.98681881e-01 6.65919282e-04]\n [9.98693915e-01 6.49764689e-04 6.56320163e-04]\n [6.18282640e-04 6.23597953e-04 9.98758119e-01]\n [6.18283350e-04 6.22450468e-04 9.98759266e-01]\n [6.18282640e-04 6.23597953e-04 9.98758119e-01]\n [6.52200159e-04 9.98681881e-01 6.65919282e-04]\n [6.18282640e-04 6.23597953e-04 9.98758119e-01]\n [6.52200159e-04 9.98681881e-01 6.65919282e-04]\n [6.18283350e-04 6.22450468e-04 9.98759266e-01]\n [6.52200159e-04 9.98681881e-01 6.65919282e-04]\n [9.98695110e-01 6.48569085e-04 6.56320948e-04]\n [6.18283350e-04 6.22450468e-04 9.98759266e-01]\n [9.98695110e-01 6.48569085e-04 6.56320948e-04]\n [9.98695110e-01 6.48569085e-04 6.56320948e-04]\n [9.98695110e-01 6.48569085e-04 6.56320948e-04]\n [9.98695110e-01 6.48569085e-04 6.56320948e-04]\n [6.18282640e-04 6.23597953e-04 9.98758119e-01]\n [6.18282640e-04 6.23597953e-04 9.98758119e-01]\n [6.18282640e-04 6.23597953e-04 9.98758119e-01]\n [6.18283350e-04 6.22450468e-04 9.98759266e-01]\n [6.52200159e-04 9.98681881e-01 6.65919282e-04]\n [6.18283350e-04 6.22450468e-04 9.98759266e-01]\n [6.18283350e-04 6.22450468e-04 9.98759266e-01]\n [9.98693915e-01 6.49764689e-04 6.56320163e-04]\n [9.98695110e-01 6.48569085e-04 6.56320948e-04]\n [6.52200159e-04 9.98681881e-01 6.65919282e-04]\n [9.98693915e-01 6.49764689e-04 6.56320163e-04]\n [6.52200159e-04 9.98681881e-01 6.65919282e-04]\n [6.18283350e-04 6.22450468e-04 9.98759266e-01]\n [6.52200159e-04 9.98681881e-01 6.65919282e-04]\n [6.18282640e-04 6.23597953e-04 9.98758119e-01]\n [9.98693915e-01 6.49764689e-04 6.56320163e-04]\n [9.98695110e-01 6.48569085e-04 6.56320948e-04]\n [6.18283350e-04 6.22450468e-04 9.98759266e-01]\n [6.18282640e-04 6.23597953e-04 9.98758119e-01]\n [6.52200159e-04 9.98681881e-01 6.65919282e-04]\n [9.98695110e-01 6.48569085e-04 6.56320948e-04]\n [9.98693915e-01 6.49764689e-04 6.56320163e-04]\n [6.18282640e-04 6.23597953e-04 9.98758119e-01]\n [6.18283350e-04 6.22450468e-04 9.98759266e-01]\n [9.98695110e-01 6.48569085e-04 6.56320948e-04]\n [6.52200159e-04 9.98681881e-01 6.65919282e-04]\n [9.98693915e-01 6.49764689e-04 6.56320163e-04]\n [9.98695110e-01 6.48569085e-04 6.56320948e-04]\n [9.98693915e-01 6.49764689e-04 6.56320163e-04]\n [6.18283350e-04 6.22450468e-04 9.98759266e-01]\n [6.18282640e-04 6.23597953e-04 9.98758119e-01]\n [6.52200159e-04 9.98681881e-01 6.65919282e-04]\n [6.18282640e-04 6.23597953e-04 9.98758119e-01]\n [6.18282640e-04 6.23597953e-04 9.98758119e-01]\n [6.18282640e-04 6.23597953e-04 9.98758119e-01]\n [9.98695110e-01 6.48569085e-04 6.56320948e-04]\n [6.52200159e-04 9.98681881e-01 6.65919282e-04]\n [6.18283350e-04 6.22450468e-04 9.98759266e-01]\n [9.98693915e-01 6.49764689e-04 6.56320163e-04]\n [6.18282640e-04 6.23597953e-04 9.98758119e-01]\n [6.18282640e-04 6.23597953e-04 9.98758119e-01]\n [6.18282640e-04 6.23597953e-04 9.98758119e-01]\n [9.98695110e-01 6.48569085e-04 6.56320948e-04]\n [6.18282640e-04 6.23597953e-04 9.98758119e-01]\n [6.18282640e-04 6.23597953e-04 9.98758119e-01]\n [6.52200159e-04 9.98681881e-01 6.65919282e-04]\n [6.52200159e-04 9.98681881e-01 6.65919282e-04]\n [9.98695110e-01 6.48569085e-04 6.56320948e-04]\n [6.18283350e-04 6.22450468e-04 9.98759266e-01]\n [9.98693915e-01 6.49764689e-04 6.56320163e-04]\n [6.18282640e-04 6.23597953e-04 9.98758119e-01]\n [6.52200159e-04 9.98681881e-01 6.65919282e-04]\n [6.18273576e-04 6.38248172e-04 9.98743478e-01]\n [6.18283350e-04 6.22450468e-04 9.98759266e-01]\n [6.18282640e-04 6.23597953e-04 9.98758119e-01]\n [6.18282640e-04 6.23597953e-04 9.98758119e-01]\n [9.98693915e-01 6.49764689e-04 6.56320163e-04]\n [6.18282640e-04 6.23597953e-04 9.98758119e-01]\n [9.98695110e-01 6.48569085e-04 6.56320948e-04]\n [9.98695110e-01 6.48569085e-04 6.56320948e-04]\n [9.98693915e-01 6.49764689e-04 6.56320163e-04]\n [9.98695110e-01 6.48569085e-04 6.56320948e-04]\n [6.18282640e-04 6.23597953e-04 9.98758119e-01]\n [6.18282640e-04 6.23597953e-04 9.98758119e-01]\n [6.52200159e-04 9.98681881e-01 6.65919282e-04]\n [6.18283350e-04 6.22450468e-04 9.98759266e-01]\n [6.18283350e-04 6.22450468e-04 9.98759266e-01]\n [9.98695110e-01 6.48569085e-04 6.56320948e-04]\n [9.98693915e-01 6.49764689e-04 6.56320163e-04]\n [9.98695110e-01 6.48569085e-04 6.56320948e-04]\n [9.98695110e-01 6.48569085e-04 6.56320948e-04]\n [6.18282640e-04 6.23597953e-04 9.98758119e-01]\n [6.52200159e-04 9.98681881e-01 6.65919282e-04]\n [6.18282640e-04 6.23597953e-04 9.98758119e-01]\n [6.18282640e-04 6.23597953e-04 9.98758119e-01]\n [6.18273576e-04 6.38248172e-04 9.98743478e-01]\n [9.98693915e-01 6.49764689e-04 6.56320163e-04]\n [6.52200159e-04 9.98681881e-01 6.65919282e-04]\n [6.52200159e-04 9.98681881e-01 6.65919282e-04]\n [9.98695110e-01 6.48569085e-04 6.56320948e-04]\n [6.52200159e-04 9.98681881e-01 6.65919282e-04]\n [9.98693915e-01 6.49764689e-04 6.56320163e-04]\n [9.98695110e-01 6.48569085e-04 6.56320948e-04]\n [9.98695110e-01 6.48569085e-04 6.56320948e-04]\n [6.52200159e-04 9.98681881e-01 6.65919282e-04]\n [9.98693915e-01 6.49764689e-04 6.56320163e-04]\n [9.98693915e-01 6.49764689e-04 6.56320163e-04]\n [9.98695110e-01 6.48569085e-04 6.56320948e-04]\n [6.18283350e-04 6.22450468e-04 9.98759266e-01]\n [6.52200159e-04 9.98681881e-01 6.65919282e-04]\n [6.52200159e-04 9.98681881e-01 6.65919282e-04]]"
      },
      "random_forest": {
        "accuracy": 1.0,
        "log_loss": 0.028180794713455214,
        "predictions": "[0 2 2 0 2 2 2 1 2 1 1 1 2 2 0 2 0 2 2 0 1 2 2 0 2 2 0 2 0 2 2 1 2 1 2 2 1\n 2 2 2 0 0 1 2 0 2 0 1 0 2 2 2 1 2 1 2 1 0 2 0 0 0 0 2 2 2 2 1 2 2 0 0 1 0\n 1 2 1 2 0 0 2 2 1 0 0 2 2 0 1 0 0 0 2 2 1 2 2 2 0 1 2 0 2 2 2 0 2 2 1 1 0\n 2 0 2 1 2 2 2 2 0 2 0 0 0 0 2 2 1 2 2 0 0 0 0 2 1 2 2 2 0 1 1 0 1 0 0 0 1\n 0 0 0 2 1 1]",
        "probabilities": "[[0.88220821 0.0335545  0.08423729]\n [0.00253731 0.         0.99746269]\n [0.04659256 0.01448053 0.93892691]\n [0.9112649  0.02769951 0.0610356 ]\n [0.03487495 0.01528571 0.94983933]\n [0.01       0.02       0.97      ]\n [0.01       0.02       0.97      ]\n [0.01475524 0.94953047 0.03571429]\n [0.         0.         1.        ]\n [0.         0.99       0.01      ]\n [0.01       0.95428571 0.03571429]\n [0.02496806 0.95503194 0.02      ]\n [0.00311927 0.01688073 0.98      ]\n [0.00576271 0.         0.99423729]\n [0.98311927 0.00688073 0.01      ]\n [0.01576271 0.         0.98423729]\n [0.99       0.         0.01      ]\n [0.0180315  0.         0.9819685 ]\n [0.00475524 0.01       0.98524476]\n [0.98       0.02       0.        ]\n [0.05439907 0.89299567 0.05260526]\n [0.         0.         1.        ]\n [0.00475524 0.02371229 0.97153247]\n [0.97576271 0.02       0.00423729]\n [0.         0.         1.        ]\n [0.         0.01       0.99      ]\n [0.96972776 0.02027224 0.01      ]\n [0.01       0.01       0.98      ]\n [0.98       0.01       0.01      ]\n [0.01       0.         0.99      ]\n [0.03475524 0.00524476 0.96      ]\n [0.0164455  0.9835545  0.        ]\n [0.         0.0275     0.9725    ]\n [0.         0.976      0.024     ]\n [0.         0.         1.        ]\n [0.01911224 0.         0.98088776]\n [0.00153499 0.99846501 0.        ]\n [0.         0.01       0.99      ]\n [0.0080315  0.01628571 0.97568279]\n [0.00774522 0.02255977 0.96969501]\n [1.         0.         0.        ]\n [0.96       0.02       0.02      ]\n [0.01       0.99       0.        ]\n [0.         0.         1.        ]\n [0.9252732  0.06524476 0.00948204]\n [0.         0.         1.        ]\n [0.98       0.01       0.01      ]\n [0.01475524 0.98124476 0.004     ]\n [0.86808732 0.05791268 0.074     ]\n [0.01       0.01       0.98      ]\n [0.01082985 0.01448053 0.97468962]\n [0.00731183 0.         0.99268817]\n [0.         0.99       0.01      ]\n [0.         0.         1.        ]\n [0.         0.9475     0.0525    ]\n [0.00576271 0.01       0.98423729]\n [0.0064455  0.9835545  0.01      ]\n [0.99       0.01       0.        ]\n [0.02307454 0.006      0.97092546]\n [0.98787451 0.00524476 0.00688073]\n [0.9864455  0.0135545  0.        ]\n [1.         0.         0.        ]\n [0.99       0.01       0.        ]\n [0.         0.         1.        ]\n [0.00576271 0.006      0.98823729]\n [0.         0.01       0.99      ]\n [0.02729256 0.02524476 0.94746269]\n [0.01376731 0.95623269 0.03      ]\n [0.         0.03       0.97      ]\n [0.02       0.09778571 0.88221429]\n [0.9364455  0.0335545  0.03      ]\n [0.993309   0.00547445 0.00121655]\n [0.03259386 0.96740614 0.        ]\n [0.99671053 0.         0.00328947]\n [0.01972234 0.90370158 0.07657609]\n [0.         0.         1.        ]\n [0.01146577 0.97124476 0.01728947]\n [0.02247324 0.01       0.96752676]\n [0.99       0.006      0.004     ]\n [0.94       0.0375     0.0225    ]\n [0.02406274 0.006      0.96993726]\n [0.         0.         1.        ]\n [0.05314405 0.84853777 0.09831818]\n [0.9980315  0.         0.0019685 ]\n [0.95576271 0.01       0.03423729]\n [0.00184783 0.         0.99815217]\n [0.02576271 0.01       0.96423729]\n [0.99       0.         0.01      ]\n [0.04120074 0.94879926 0.01      ]\n [0.99       0.01       0.        ]\n [0.99475524 0.00524476 0.        ]\n [0.96576271 0.01       0.02423729]\n [0.04278674 0.04521229 0.91200097]\n [0.         0.         1.        ]\n [0.         1.         0.        ]\n [0.         0.         1.        ]\n [0.         0.0175     0.9825    ]\n [0.         0.02       0.98      ]\n [0.97949727 0.00524476 0.01525798]\n [0.02639707 0.88788865 0.08571429]\n [0.00279835 0.01848053 0.97872111]\n [0.84808732 0.05791268 0.094     ]\n [0.         0.01       0.99      ]\n [0.01       0.01       0.98      ]\n [0.         0.00882353 0.99117647]\n [0.9980315  0.         0.0019685 ]\n [0.01       0.01292308 0.97707692]\n [0.         0.0335     0.9665    ]\n [0.01465425 0.96067908 0.02466667]\n [0.0064455  0.9935545  0.        ]\n [0.99       0.01       0.        ]\n [0.00475524 0.00428571 0.99095904]\n [0.9880315  0.01       0.0019685 ]\n [0.         0.         1.        ]\n [0.00475524 0.98524476 0.01      ]\n [0.02852828 0.         0.97147172]\n [0.         0.         1.        ]\n [0.00279835 0.00617284 0.99102881]\n [0.00871795 0.01       0.98128205]\n [0.97       0.03       0.        ]\n [0.00576271 0.         0.99423729]\n [0.98949727 0.         0.01050273]\n [0.92685732 0.03567999 0.03746269]\n [0.97972776 0.02027224 0.        ]\n [0.9029609  0.04980467 0.04723443]\n [0.         0.017      0.983     ]\n [0.         0.         1.        ]\n [0.05646958 0.86581614 0.07771429]\n [0.00671053 0.01230769 0.98098178]\n [0.01       0.02       0.97      ]\n [0.97573476 0.01705198 0.00721326]\n [0.99       0.01       0.        ]\n [0.98981033 0.         0.01018967]\n [0.9864455  0.0135545  0.        ]\n [0.         0.         1.        ]\n [0.         0.98285714 0.01714286]\n [0.00871795 0.         0.99128205]\n [0.01       0.         0.99      ]\n [0.01207912 0.01       0.97792088]\n [0.99671053 0.         0.00328947]\n [0.03       0.96018182 0.00981818]\n [0.00376731 0.99623269 0.        ]\n [0.99534332 0.00268817 0.0019685 ]\n [0.         0.995      0.005     ]\n [0.97880286 0.01524476 0.00595238]\n [0.99       0.01       0.        ]\n [0.98731183 0.00868817 0.004     ]\n [0.         0.992      0.008     ]\n [0.99       0.01       0.        ]\n [1.         0.         0.        ]\n [0.9137619  0.05316886 0.03306923]\n [0.02307454 0.00428571 0.97263975]\n [0.01       0.98       0.01      ]\n [0.05462366 0.8775117  0.06786464]]"
      },
      "logistic_regression": {
        "accuracy": 1.0,
        "log_loss": 0.0035368560315809676,
        "predictions": "[0 2 2 0 2 2 2 1 2 1 1 1 2 2 0 2 0 2 2 0 1 2 2 0 2 2 0 2 0 2 2 1 2 1 2 2 1\n 2 2 2 0 0 1 2 0 2 0 1 0 2 2 2 1 2 1 2 1 0 2 0 0 0 0 2 2 2 2 1 2 2 0 0 1 0\n 1 2 1 2 0 0 2 2 1 0 0 2 2 0 1 0 0 0 2 2 1 2 2 2 0 1 2 0 2 2 2 0 2 2 1 1 0\n 2 0 2 1 2 2 2 2 0 2 0 0 0 0 2 2 1 2 2 0 0 0 0 2 1 2 2 2 0 1 1 0 1 0 0 0 1\n 0 0 0 2 1 1]",
        "probabilities": "[[9.93425530e-01 4.26490773e-03 2.30956230e-03]\n [3.60447087e-06 5.54465110e-05 9.99940949e-01]\n [1.91374580e-03 5.95701300e-03 9.92129241e-01]\n [9.98824724e-01 9.33867546e-04 2.41408319e-04]\n [6.83378180e-05 3.31372319e-04 9.99600290e-01]\n [8.72877517e-04 4.08967620e-03 9.95037446e-01]\n [7.75292834e-04 2.62450107e-03 9.96600206e-01]\n [2.49402649e-03 9.93777388e-01 3.72858562e-03]\n [7.36841698e-05 4.22738784e-04 9.99503577e-01]\n [2.99403875e-03 9.94957570e-01 2.04839129e-03]\n [3.39100305e-03 9.93658229e-01 2.95076752e-03]\n [2.88010572e-03 9.93756558e-01 3.36333669e-03]\n [6.91599867e-04 3.69986307e-03 9.95608537e-01]\n [8.05143240e-06 2.26112044e-04 9.99765837e-01]\n [9.97125355e-01 2.30940287e-03 5.65242337e-04]\n [9.78678708e-05 3.55675530e-04 9.99546457e-01]\n [9.96818110e-01 2.64532477e-03 5.36564875e-04]\n [4.33117247e-03 6.87959277e-03 9.88789235e-01]\n [5.97565970e-04 4.65756040e-03 9.94744874e-01]\n [9.89200564e-01 9.50336011e-03 1.29607582e-03]\n [4.66279321e-03 9.88407809e-01 6.92939765e-03]\n [4.09761020e-07 1.28146260e-05 9.99986776e-01]\n [6.30838965e-04 3.14106012e-03 9.96228101e-01]\n [9.96938580e-01 2.60271829e-03 4.58701361e-04]\n [2.36504963e-04 9.55887881e-04 9.98807607e-01]\n [1.56377614e-03 5.47043671e-03 9.92965787e-01]\n [9.99353772e-01 5.69201861e-04 7.70264281e-05]\n [4.26841674e-06 3.41489364e-05 9.99961583e-01]\n [9.92478558e-01 6.72088565e-03 8.00556736e-04]\n [4.29461088e-06 1.39627843e-04 9.99856078e-01]\n [1.05834000e-03 3.47025725e-03 9.95471403e-01]\n [1.74698943e-03 9.95663393e-01 2.58961744e-03]\n [1.25483979e-03 2.08549328e-03 9.96659667e-01]\n [1.06241852e-03 9.96244735e-01 2.69284608e-03]\n [1.35444237e-04 1.91169752e-04 9.99673386e-01]\n [3.54642741e-05 1.55216154e-04 9.99809320e-01]\n [1.14489857e-03 9.97632061e-01 1.22304043e-03]\n [4.57388608e-05 7.67327592e-05 9.99877528e-01]\n [1.97926812e-03 7.60662073e-03 9.90414111e-01]\n [1.36505051e-03 4.52163495e-03 9.94113315e-01]\n [9.99990892e-01 8.89289114e-06 2.14838125e-07]\n [9.99996925e-01 2.92860292e-06 1.46729585e-07]\n [7.56097323e-05 9.99783391e-01 1.40999193e-04]\n [6.27455637e-05 2.46449761e-04 9.99690805e-01]\n [9.94568415e-01 4.77962673e-03 6.51958521e-04]\n [3.68586305e-05 4.06592943e-03 9.95897212e-01]\n [9.94165238e-01 5.75110565e-03 8.36561196e-05]\n [1.62337464e-03 9.96950717e-01 1.42590793e-03]\n [9.93911368e-01 4.56881252e-03 1.51981929e-03]\n [5.21320143e-07 1.09267840e-05 9.99988552e-01]\n [3.69081220e-03 4.82725656e-03 9.91481931e-01]\n [9.76866720e-06 1.70134063e-04 9.99820097e-01]\n [8.33389384e-04 9.97943247e-01 1.22336394e-03]\n [3.52409320e-08 1.22315011e-06 9.99998742e-01]\n [1.79826948e-03 9.96507654e-01 1.69407625e-03]\n [4.66575765e-05 3.65687018e-04 9.99587655e-01]\n [4.22226331e-03 9.92247664e-01 3.53007225e-03]\n [9.96447086e-01 2.28441281e-03 1.26850119e-03]\n [1.94286130e-03 3.62842834e-03 9.94428710e-01]\n [9.99974004e-01 2.23364452e-05 3.65988251e-06]\n [9.96447731e-01 2.83232813e-03 7.19940686e-04]\n [9.96754112e-01 2.74132989e-03 5.04558234e-04]\n [9.99110255e-01 8.07361791e-04 8.23827321e-05]\n [2.52993931e-06 3.46698461e-05 9.99962800e-01]\n [9.29152318e-04 3.39969035e-03 9.95671157e-01]\n [2.98042440e-03 1.57415447e-02 9.81278031e-01]\n [1.59108534e-03 4.32616486e-03 9.94082750e-01]\n [2.77363762e-03 9.92297436e-01 4.92892669e-03]\n [1.68797725e-03 7.69843750e-03 9.90613585e-01]\n [1.50026359e-03 7.25944613e-03 9.91240290e-01]\n [9.92088844e-01 7.54325158e-03 3.67903947e-04]\n [9.99663272e-01 3.09854159e-04 2.68740337e-05]\n [7.97445638e-04 9.97290481e-01 1.91207333e-03]\n [9.96101344e-01 3.14352010e-03 7.55135796e-04]\n [3.23855362e-03 9.93635221e-01 3.12622497e-03]\n [1.82428424e-03 5.50601405e-03 9.92669702e-01]\n [4.50275664e-03 9.92108447e-01 3.38879639e-03]\n [1.65517374e-03 3.56853847e-03 9.94776288e-01]\n [9.99060306e-01 8.36641457e-04 1.03052923e-04]\n [9.99932009e-01 6.75199493e-05 4.71055203e-07]\n [3.57150692e-05 3.40659398e-04 9.99623626e-01]\n [7.99254372e-05 6.21369238e-04 9.99298705e-01]\n [1.77720904e-03 9.94047198e-01 4.17559254e-03]\n [9.99990041e-01 9.86718454e-06 9.16271857e-08]\n [9.94247939e-01 4.90491979e-03 8.47140864e-04]\n [1.86657047e-06 1.08873002e-04 9.99889260e-01]\n [7.10473268e-04 5.39564993e-03 9.93893877e-01]\n [9.99410224e-01 5.62141091e-04 2.76345767e-05]\n [4.62340083e-04 9.91204565e-01 8.33309540e-03]\n [9.94036783e-01 5.04866764e-03 9.14549802e-04]\n [9.99994833e-01 5.03607190e-06 1.30916462e-07]\n [9.97718874e-01 1.83852227e-03 4.42603367e-04]\n [6.20935326e-04 3.24207111e-03 9.96136994e-01]\n [1.52056600e-04 2.46362133e-04 9.99601581e-01]\n [4.38546563e-03 9.92519152e-01 3.09538231e-03]\n [1.22170105e-03 4.92512709e-03 9.93853172e-01]\n [1.35054319e-04 5.62165341e-04 9.99302780e-01]\n [1.47758671e-03 3.85424721e-03 9.94668166e-01]\n [9.98073841e-01 1.54014545e-03 3.86013765e-04]\n [3.13355541e-03 9.93660220e-01 3.20622492e-03]\n [2.69390813e-03 5.94070132e-03 9.91365391e-01]\n [9.96154334e-01 2.84774548e-03 9.97920543e-04]\n [2.27351376e-06 1.01548345e-04 9.99896178e-01]\n [1.39585829e-07 2.73722391e-06 9.99997123e-01]\n [3.08163632e-08 1.46786096e-06 9.99998501e-01]\n [9.97146916e-01 2.45730440e-03 3.95779836e-04]\n [6.61237865e-04 1.80677470e-03 9.97531987e-01]\n [3.43177673e-08 6.38126486e-07 9.99999328e-01]\n [3.53439412e-03 9.93775341e-01 2.69026518e-03]\n [1.56193729e-03 9.96152964e-01 2.28509897e-03]\n [9.99613313e-01 3.64964751e-04 2.17218356e-05]\n [1.88367998e-03 6.01049333e-03 9.92105827e-01]\n [9.98862377e-01 9.33200700e-04 2.04422202e-04]\n [4.14275172e-05 4.77806930e-04 9.99480766e-01]\n [1.96117058e-03 9.95182401e-01 2.85642866e-03]\n [2.02351046e-04 1.06492992e-03 9.98732719e-01]\n [9.71973459e-05 1.98827354e-04 9.99703975e-01]\n [1.77913197e-03 2.86941682e-03 9.95351451e-01]\n [3.52676614e-04 1.09254367e-03 9.98554780e-01]\n [9.93192986e-01 5.89897548e-03 9.08038704e-04]\n [1.13599052e-04 4.75591515e-04 9.99410809e-01]\n [9.99957496e-01 3.97977510e-05 2.70585241e-06]\n [9.95531004e-01 3.06371836e-03 1.40527773e-03]\n [9.98865243e-01 1.00739182e-03 1.27364806e-04]\n [9.93678495e-01 4.30179192e-03 2.01971335e-03]\n [1.82971777e-03 4.23716029e-03 9.93933122e-01]\n [8.28319212e-06 5.97146569e-05 9.99932002e-01]\n [1.16949019e-03 9.96556590e-01 2.27392016e-03]\n [1.71076592e-04 5.62160303e-04 9.99266763e-01]\n [1.95915258e-03 5.20962692e-03 9.92831221e-01]\n [9.97401759e-01 2.04215527e-03 5.56085606e-04]\n [9.95645649e-01 3.54134491e-03 8.13006282e-04]\n [9.99979744e-01 1.98622066e-05 3.94027204e-07]\n [9.95078144e-01 4.04975886e-03 8.72097484e-04]\n [3.17324469e-04 2.18507977e-03 9.97497596e-01]\n [1.06990470e-03 9.97898686e-01 1.03140977e-03]\n [4.67311557e-04 1.73998726e-03 9.97792701e-01]\n [1.23186808e-04 3.32504000e-04 9.99544309e-01]\n [6.27946812e-04 1.39495093e-03 9.97977102e-01]\n [9.96578217e-01 2.23111259e-03 1.19067008e-03]\n [4.12652673e-03 9.93653950e-01 2.21952306e-03]\n [1.07821855e-03 9.97353852e-01 1.56792985e-03]\n [9.98733709e-01 9.70941572e-04 2.95349015e-04]\n [4.16114462e-03 9.93429146e-01 2.40970942e-03]\n [9.99413353e-01 5.55684058e-04 3.09630743e-05]\n [9.99992735e-01 7.12504747e-06 1.40442310e-07]\n [9.97468782e-01 2.00610728e-03 5.25110323e-04]\n [6.21248635e-04 9.98856962e-01 5.21789285e-04]\n [9.98995845e-01 9.29578802e-04 7.45766310e-05]\n [9.95176757e-01 3.93061467e-03 8.92628674e-04]\n [9.97575719e-01 1.64371552e-03 7.80565536e-04]\n [2.37200842e-03 6.84830646e-03 9.90779685e-01]\n [8.25352567e-04 9.97745017e-01 1.42963003e-03]\n [1.51249397e-03 9.97059906e-01 1.42760026e-03]]"
      },
      "ensemble": {
        "accuracy": 1.0,
        "log_loss": 0.010718459009873949,
        "predictions": "[0 2 2 0 2 2 2 1 2 1 1 1 2 2 0 2 0 2 2 0 1 2 2 0 2 2 0 2 0 2 2 1 2 1 2 2 1\n 2 2 2 0 0 1 2 0 2 0 1 0 2 2 2 1 2 1 2 1 0 2 0 0 0 0 2 2 2 2 1 2 2 0 0 1 0\n 1 2 1 2 0 0 2 2 1 0 0 2 2 0 1 0 0 0 2 2 1 2 2 2 0 1 2 0 2 2 2 0 2 2 1 1 0\n 2 0 2 1 2 2 2 2 0 2 0 0 0 0 2 2 1 2 2 0 0 0 0 2 1 2 2 2 0 1 1 0 1 0 0 0 1\n 0 0 0 2 1 1]",
        "probabilities": "[[0.96573164 0.01102815 0.0232402 ]\n [0.00186724 0.0012603  0.99687245]\n [0.01335862 0.00634851 0.98029288]\n [0.97433484 0.00874249 0.01692268]\n [0.00996787 0.0051422  0.98488991]\n [0.00395802 0.00727677 0.98876521]\n [0.00393366 0.00690191 0.98916445]\n [0.0060458  0.9823008  0.01165339]\n [0.00125047 0.00134362 0.9974059 ]\n [0.00248206 0.99272445 0.00479349]\n [0.0050813  0.98347104 0.01144766]\n [0.00869121 0.98367971 0.00762909]\n [0.00219249 0.00640512 0.99140239]\n [0.00268249 0.00131088 0.99600663]\n [0.99189284 0.00387241 0.00423475]\n [0.0051972  0.00132685 0.99347594]\n [0.99351541 0.00224701 0.00423756]\n [0.00682269 0.00296337 0.99021394]\n [0.002578   0.00491874 0.99250325]\n [0.98913335 0.00894919 0.00191747]\n [0.01690884 0.9661361  0.01695507]\n [0.00123214 0.00124528 0.99752257]\n [0.00258632 0.00796769 0.98944599]\n [0.99000325 0.00722932 0.00276742]\n [0.00129894 0.00149033 0.99721074]\n [0.00163076 0.00511757 0.99325166]\n [0.98908127 0.00679604 0.00412267]\n [0.00373314 0.00374214 0.99252473]\n [0.98994285 0.00575352 0.00430363]\n [0.00373309 0.00128135 0.99498556]\n [0.01019319 0.0034331  0.9863737 ]\n [0.00627126 0.99129926 0.00242948]\n [0.00155352 0.00864774 0.98979876]\n [0.00198715 0.98957044 0.0084424 ]\n [0.00126591 0.00128573 0.99744835]\n [0.00601898 0.00127674 0.99270427]\n [0.00238571 0.99554527 0.00206903]\n [0.00124349 0.00375712 0.99499938]\n [0.0037425  0.00722445 0.98903307]\n [0.00350961 0.00800948 0.98848092]\n [0.9968186  0.00158796 0.00159343]\n [0.98683092 0.00657561 0.00659348]\n [0.00423038 0.99397543 0.00179419]\n [0.00124774 0.00129955 0.9974527 ]\n [0.97678833 0.01908474 0.00412692]\n [0.00124123 0.00226292 0.99649584]\n [0.99037003 0.00550555 0.00412443]\n [0.00581805 0.99105442 0.00312754]\n [0.96233288 0.01719372 0.0204734 ]\n [0.00373218 0.00374066 0.99252714]\n [0.00486221 0.00606607 0.98907173]\n [0.00306243 0.00128461 0.99565294]\n [0.00192873 0.99300948 0.00506179]\n [0.00123202 0.00124675 0.99752122]\n [0.00218305 0.98247575 0.01534119]\n [0.00268436 0.00383786 0.99347777]\n [0.00438849 0.9904598  0.0051517 ]\n [0.99345046 0.00463893 0.00191061]\n [0.00749414 0.00366267 0.9888432 ]\n [0.99377301 0.00290245 0.00332451]\n [0.99255203 0.00566448 0.0017835 ]\n [0.99602722 0.00225316 0.00171962]\n [0.99409552 0.00428043 0.00162405]\n [0.00123268 0.0012466  0.9975207 ]\n [0.00291279 0.0035969  0.9934903 ]\n [0.00198492 0.00768534 0.99032972]\n [0.00846074 0.00863971 0.98289955]\n [0.00585837 0.98362732 0.01051431]\n [0.00165398 0.01067664 0.98766938]\n [0.00661484 0.02752127 0.9658639 ]\n [0.97894891 0.01186577 0.00918532]\n [0.995071   0.00302472 0.00190427]\n [0.01006356 0.98769515 0.00224129]\n [0.99504166 0.00235371 0.00260463]\n [0.00746336 0.97082899 0.02170765]\n [0.00169591 0.00262228 0.99568183]\n [0.0057096  0.98735581 0.0069346 ]\n [0.00727189 0.0046477  0.98808042]\n [0.99407751 0.00329331 0.00262919]\n [0.98181093 0.01097052 0.00721854]\n [0.00725663 0.00283161 0.98991176]\n [0.00125203 0.00139327 0.99735468]\n [0.01549486 0.95711197 0.02739317]\n [0.99633331 0.00158111 0.00208557]\n [0.98433435 0.0053011  0.01036455]\n [0.00169444 0.00127366 0.9970319 ]\n [0.00785807 0.00510889 0.98703305]\n [0.99418049 0.00171918 0.00410033]\n [0.01213132 0.98152234 0.00634635]\n [0.9928324  0.00533546 0.00183212]\n [0.9955084  0.00289819 0.00159341]\n [0.9876921  0.00453445 0.00777345]\n [0.01209171 0.01336794 0.97454034]\n [0.00127007 0.00129927 0.99743066]\n [0.00281957 0.99463571 0.00254472]\n [0.00153741 0.00248331 0.99597927]\n [0.00126582 0.00575347 0.9929807 ]\n [0.00160143 0.00720564 0.99119292]\n [0.99123147 0.00326406 0.00550448]\n [0.00910421 0.96689647 0.02399931]\n [0.00261284 0.00736528 0.99002188]\n [0.95789362 0.01676346 0.02534293]\n [0.00123258 0.00377183 0.99499558]\n [0.00373209 0.00373862 0.99252928]\n [0.00123206 0.00344418 0.99532374]\n [0.99563329 0.00218216 0.00218455]\n [0.00389736 0.0049204  0.99118223]\n [0.00123206 0.00961309 0.98915483]\n [0.00627633 0.98509619 0.00862749]\n [0.003725   0.99392165 0.00235335]\n [0.99421865 0.00418257 0.00159878]\n [0.00289172 0.00382608 0.9932822 ]\n [0.99356216 0.00430113 0.00213671]\n [0.00124239 0.00136153 0.99739607]\n [0.00339484 0.99162779 0.00497738]\n [0.00863997 0.001879   0.98948103]\n [0.00125635 0.00128883 0.99745483]\n [0.0023764  0.00350404 0.99411957]\n [0.00350745 0.00402749 0.99246505]\n [0.98763145 0.0105481  0.00182046]\n [0.00270887 0.00137446 0.99591668]\n [0.99418539 0.00158477 0.00422984]\n [0.97742029 0.01125922 0.01132048]\n [0.99145913 0.00690559 0.00163526]\n [0.97098778 0.01510526 0.01390696]\n [0.0016895  0.00654289 0.99176762]\n [0.0012341  0.00125701 0.99750887]\n [0.01611364 0.96213811 0.02174825]\n [0.00296021 0.00446883 0.99257098]\n [0.00422956 0.00756238 0.98820806]\n [0.99010583 0.00634835 0.00354581]\n [0.9932431  0.00496021 0.00179669]\n [0.99427545 0.00158361 0.00414094]\n [0.99220331 0.00598527 0.00181141]\n [0.00131136 0.00178975 0.9968989 ]\n [0.00199498 0.99168622 0.0063188 ]\n [0.00352833 0.00168144 0.99479023]\n [0.00376287 0.00131673 0.99492041]\n [0.00463413 0.00445022 0.99091565]\n [0.99513751 0.00214911 0.00271338]\n [0.01025311 0.98495707 0.00478981]\n [0.00292286 0.99492622 0.00215092]\n [0.99534798 0.00248255 0.00216948]\n [0.00275768 0.9936137  0.00362862]\n [0.99137494 0.00553585 0.00308922]\n [0.99431906 0.00408752 0.00159341]\n [0.99302836 0.00424692 0.00272473]\n [0.00187852 0.99422016 0.00390132]\n [0.99408065 0.00430727 0.00161209]\n [0.99561589 0.00255747 0.00182663]\n [0.97465237 0.01528173 0.01006591]\n [0.00760147 0.00403049 0.98836805]\n [0.00442201 0.99094615 0.00463183]\n [0.01574978 0.9651639  0.01908633]]"
      },
      "xgboost_calibrated": {
        "accuracy": 1.0,
        "log_loss": 8.377435439819955e-06,
        "predictions": "[0 2 2 0 2 2 2 1 2 1 1 1 2 2 0 2 0 2 2 0 1 2 2 0 2 2 0 2 0 2 2 1 2 1 2 2 1\n 2 2 2 0 0 1 2 0 2 0 1 0 2 2 2 1 2 1 2 1 0 2 0 0 0 0 2 2 2 2 1 2 2 0 0 1 0\n 1 2 1 2 0 0 2 2 1 0 0 2 2 0 1 0 0 0 2 2 1 2 2 2 0 1 2 0 2 2 2 0 2 2 1 1 0\n 2 0 2 1 2 2 2 2 0 2 0 0 0 0 2 2 1 2 2 0 0 0 0 2 1 2 2 2 0 1 1 0 1 0 0 0 1\n 0 0 0 2 1 1]",
        "probabilities": "[[1.00000000e+00 0.00000000e+00 0.00000000e+00]\n [0.00000000e+00 0.00000000e+00 1.00000000e+00]\n [0.00000000e+00 0.00000000e+00 1.00000000e+00]\n [1.00000000e+00 0.00000000e+00 0.00000000e+00]\n [0.00000000e+00 0.00000000e+00 1.00000000e+00]\n [0.00000000e+00 0.00000000e+00 1.00000000e+00]\n [0.00000000e+00 0.00000000e+00 1.00000000e+00]\n [0.00000000e+00 1.00000000e+00 0.00000000e+00]\n [0.00000000e+00 0.00000000e+00 1.00000000e+00]\n [0.00000000e+00 1.00000000e+00 0.00000000e+00]\n [0.00000000e+00 1.00000000e+00 0.00000000e+00]\n [0.00000000e+00 1.00000000e+00 0.00000000e+00]\n [0.00000000e+00 0.00000000e+00 1.00000000e+00]\n [0.00000000e+00 0.00000000e+00 1.00000000e+00]\n [1.00000000e+00 0.00000000e+00 0.00000000e+00]\n [0.00000000e+00 0.00000000e+00 1.00000000e+00]\n [1.00000000e+00 0.00000000e+00 0.00000000e+00]\n [0.00000000e+00 0.00000000e+00 1.00000000e+00]\n [0.00000000e+00 0.00000000e+00 1.00000000e+00]\n [1.00000000e+00 0.00000000e+00 0.00000000e+00]\n [5.31221760e-04 9.99109984e-01 3.58794702e-04]\n [0.00000000e+00 0.00000000e+00 1.00000000e+00]\n [0.00000000e+00 0.00000000e+00 1.00000000e+00]\n [1.00000000e+00 0.00000000e+00 0.00000000e+00]\n [0.00000000e+00 0.00000000e+00 1.00000000e+00]\n [0.00000000e+00 0.00000000e+00 1.00000000e+00]\n [1.00000000e+00 0.00000000e+00 0.00000000e+00]\n [0.00000000e+00 0.00000000e+00 1.00000000e+00]\n [1.00000000e+00 0.00000000e+00 0.00000000e+00]\n [0.00000000e+00 0.00000000e+00 1.00000000e+00]\n [0.00000000e+00 0.00000000e+00 1.00000000e+00]\n [0.00000000e+00 1.00000000e+00 0.00000000e+00]\n [0.00000000e+00 0.00000000e+00 1.00000000e+00]\n [0.00000000e+00 1.00000000e+00 0.00000000e+00]\n [0.00000000e+00 0.00000000e+00 1.00000000e+00]\n [0.00000000e+00 0.00000000e+00 1.00000000e+00]\n [0.00000000e+00 1.00000000e+00 0.00000000e+00]\n [0.00000000e+00 0.00000000e+00 1.00000000e+00]\n [0.00000000e+00 0.00000000e+00 1.00000000e+00]\n [0.00000000e+00 0.00000000e+00 1.00000000e+00]\n [1.00000000e+00 0.00000000e+00 0.00000000e+00]\n [1.00000000e+00 0.00000000e+00 0.00000000e+00]\n [0.00000000e+00 1.00000000e+00 0.00000000e+00]\n [0.00000000e+00 0.00000000e+00 1.00000000e+00]\n [1.00000000e+00 0.00000000e+00 0.00000000e+00]\n [0.00000000e+00 0.00000000e+00 1.00000000e+00]\n [1.00000000e+00 0.00000000e+00 0.00000000e+00]\n [0.00000000e+00 1.00000000e+00 0.00000000e+00]\n [1.00000000e+00 0.00000000e+00 0.00000000e+00]\n [0.00000000e+00 0.00000000e+00 1.00000000e+00]\n [0.00000000e+00 0.00000000e+00 1.00000000e+00]\n [0.00000000e+00 0.00000000e+00 1.00000000e+00]\n [0.00000000e+00 9.99601112e-01 3.98888493e-04]\n [0.00000000e+00 0.00000000e+00 1.00000000e+00]\n [0.00000000e+00 1.00000000e+00 0.00000000e+00]\n [0.00000000e+00 0.00000000e+00 1.00000000e+00]\n [0.00000000e+00 1.00000000e+00 0.00000000e+00]\n [1.00000000e+00 0.00000000e+00 0.00000000e+00]\n [0.00000000e+00 0.00000000e+00 1.00000000e+00]\n [1.00000000e+00 0.00000000e+00 0.00000000e+00]\n [1.00000000e+00 0.00000000e+00 0.00000000e+00]\n [1.00000000e+00 0.00000000e+00 0.00000000e+00]\n [1.00000000e+00 0.00000000e+00 0.00000000e+00]\n [0.00000000e+00 0.00000000e+00 1.00000000e+00]\n [0.00000000e+00 0.00000000e+00 1.00000000e+00]\n [0.00000000e+00 0.00000000e+00 1.00000000e+00]\n [0.00000000e+00 0.00000000e+00 1.00000000e+00]\n [0.00000000e+00 1.00000000e+00 0.00000000e+00]\n [0.00000000e+00 0.00000000e+00 1.00000000e+00]\n [0.00000000e+00 0.00000000e+00 1.00000000e+00]\n [1.00000000e+00 0.00000000e+00 0.00000000e+00]\n [1.00000000e+00 0.00000000e+00 0.00000000e+00]\n [0.00000000e+00 1.00000000e+00 0.00000000e+00]\n [1.00000000e+00 0.00000000e+00 0.00000000e+00]\n [0.00000000e+00 1.00000000e+00 0.00000000e+00]\n [0.00000000e+00 0.00000000e+00 1.00000000e+00]\n [0.00000000e+00 1.00000000e+00 0.00000000e+00]\n [0.00000000e+00 0.00000000e+00 1.00000000e+00]\n [1.00000000e+00 0.00000000e+00 0.00000000e+00]\n [1.00000000e+00 0.00000000e+00 0.00000000e+00]\n [0.00000000e+00 0.00000000e+00 1.00000000e+00]\n [0.00000000e+00 0.00000000e+00 1.00000000e+00]\n [0.00000000e+00 1.00000000e+00 0.00000000e+00]\n [1.00000000e+00 0.00000000e+00 0.00000000e+00]\n [1.00000000e+00 0.00000000e+00 0.00000000e+00]\n [0.00000000e+00 0.00000000e+00 1.00000000e+00]\n [0.00000000e+00 0.00000000e+00 1.00000000e+00]\n [1.00000000e+00 0.00000000e+00 0.00000000e+00]\n [0.00000000e+00 1.00000000e+00 0.00000000e+00]\n [1.00000000e+00 0.00000000e+00 0.00000000e+00]\n [1.00000000e+00 0.00000000e+00 0.00000000e+00]\n [1.00000000e+00 0.00000000e+00 0.00000000e+00]\n [0.00000000e+00 0.00000000e+00 1.00000000e+00]\n [0.00000000e+00 0.00000000e+00 1.00000000e+00]\n [0.00000000e+00 1.00000000e+00 0.00000000e+00]\n [0.00000000e+00 0.00000000e+00 1.00000000e+00]\n [0.00000000e+00 0.00000000e+00 1.00000000e+00]\n [0.00000000e+00 0.00000000e+00 1.00000000e+00]\n [1.00000000e+00 0.00000000e+00 0.00000000e+00]\n [0.00000000e+00 1.00000000e+00 0.00000000e+00]\n [0.00000000e+00 0.00000000e+00 1.00000000e+00]\n [1.00000000e+00 0.00000000e+00 0.00000000e+00]\n [0.00000000e+00 0.00000000e+00 1.00000000e+00]\n [0.00000000e+00 0.00000000e+00 1.00000000e+00]\n [0.00000000e+00 0.00000000e+00 1.00000000e+00]\n [1.00000000e+00 0.00000000e+00 0.00000000e+00]\n [0.00000000e+00 0.00000000e+00 1.00000000e+00]\n [0.00000000e+00 0.00000000e+00 1.00000000e+00]\n [0.00000000e+00 1.00000000e+00 0.00000000e+00]\n [0.00000000e+00 1.00000000e+00 0.00000000e+00]\n [1.00000000e+00 0.00000000e+00 0.00000000e+00]\n [0.00000000e+00 0.00000000e+00 1.00000000e+00]\n [1.00000000e+00 0.00000000e+00 0.00000000e+00]\n [0.00000000e+00 0.00000000e+00 1.00000000e+00]\n [0.00000000e+00 1.00000000e+00 0.00000000e+00]\n [0.00000000e+00 0.00000000e+00 1.00000000e+00]\n [0.00000000e+00 0.00000000e+00 1.00000000e+00]\n [0.00000000e+00 0.00000000e+00 1.00000000e+00]\n [0.00000000e+00 0.00000000e+00 1.00000000e+00]\n [1.00000000e+00 0.00000000e+00 0.00000000e+00]\n [0.00000000e+00 0.00000000e+00 1.00000000e+00]\n [1.00000000e+00 0.00000000e+00 0.00000000e+00]\n [1.00000000e+00 0.00000000e+00 0.00000000e+00]\n [1.00000000e+00 0.00000000e+00 0.00000000e+00]\n [1.00000000e+00 0.00000000e+00 0.00000000e+00]\n [0.00000000e+00 0.00000000e+00 1.00000000e+00]\n [0.00000000e+00 0.00000000e+00 1.00000000e+00]\n [0.00000000e+00 1.00000000e+00 0.00000000e+00]\n [0.00000000e+00 0.00000000e+00 1.00000000e+00]\n [0.00000000e+00 0.00000000e+00 1.00000000e+00]\n [1.00000000e+00 0.00000000e+00 0.00000000e+00]\n [1.00000000e+00 0.00000000e+00 0.00000000e+00]\n [1.00000000e+00 0.00000000e+00 0.00000000e+00]\n [9.99999256e-01 7.44225499e-07 0.00000000e+00]\n [0.00000000e+00 0.00000000e+00 1.00000000e+00]\n [0.00000000e+00 1.00000000e+00 0.00000000e+00]\n [0.00000000e+00 0.00000000e+00 1.00000000e+00]\n [0.00000000e+00 0.00000000e+00 1.00000000e+00]\n [0.00000000e+00 0.00000000e+00 1.00000000e+00]\n [1.00000000e+00 0.00000000e+00 0.00000000e+00]\n [0.00000000e+00 1.00000000e+00 0.00000000e+00]\n [0.00000000e+00 1.00000000e+00 0.00000000e+00]\n [1.00000000e+00 0.00000000e+00 0.00000000e+00]\n [0.00000000e+00 1.00000000e+00 0.00000000e+00]\n [1.00000000e+00 0.00000000e+00 0.00000000e+00]\n [1.00000000e+00 0.00000000e+00 0.00000000e+00]\n [1.00000000e+00 0.00000000e+00 0.00000000e+00]\n [0.00000000e+00 1.00000000e+00 0.00000000e+00]\n [1.00000000e+00 0.00000000e+00 0.00000000e+00]\n [1.00000000e+00 0.00000000e+00 0.00000000e+00]\n [1.00000000e+00 0.00000000e+00 0.00000000e+00]\n [0.00000000e+00 0.00000000e+00 1.00000000e+00]\n [0.00000000e+00 1.00000000e+00 0.00000000e+00]\n [0.00000000e+00 1.00000000e+00 0.00000000e+00]]"
      },
      "lightgbm_calibrated": {
        "accuracy": 1.0,
        "log_loss": 2.2204460492503136e-16,
        "predictions": "[0 2 2 0 2 2 2 1 2 1 1 1 2 2 0 2 0 2 2 0 1 2 2 0 2 2 0 2 0 2 2 1 2 1 2 2 1\n 2 2 2 0 0 1 2 0 2 0 1 0 2 2 2 1 2 1 2 1 0 2 0 0 0 0 2 2 2 2 1 2 2 0 0 1 0\n 1 2 1 2 0 0 2 2 1 0 0 2 2 0 1 0 0 0 2 2 1 2 2 2 0 1 2 0 2 2 2 0 2 2 1 1 0\n 2 0 2 1 2 2 2 2 0 2 0 0 0 0 2 2 1 2 2 0 0 0 0 2 1 2 2 2 0 1 1 0 1 0 0 0 1\n 0 0 0 2 1 1]",
        "probabilities": "[[1. 0. 0.]\n [0. 0. 1.]\n [0. 0. 1.]\n [1. 0. 0.]\n [0. 0. 1.]\n [0. 0. 1.]\n [0. 0. 1.]\n [0. 1. 0.]\n [0. 0. 1.]\n [0. 1. 0.]\n [0. 1. 0.]\n [0. 1. 0.]\n [0. 0. 1.]\n [0. 0. 1.]\n [1. 0. 0.]\n [0. 0. 1.]\n [1. 0. 0.]\n [0. 0. 1.]\n [0. 0. 1.]\n [1. 0. 0.]\n [0. 1. 0.]\n [0. 0. 1.]\n [0. 0. 1.]\n [1. 0. 0.]\n [0. 0. 1.]\n [0. 0. 1.]\n [1. 0. 0.]\n [0. 0. 1.]\n [1. 0. 0.]\n [0. 0. 1.]\n [0. 0. 1.]\n [0. 1. 0.]\n [0. 0. 1.]\n [0. 1. 0.]\n [0. 0. 1.]\n [0. 0. 1.]\n [0. 1. 0.]\n [0. 0. 1.]\n [0. 0. 1.]\n [0. 0. 1.]\n [1. 0. 0.]\n [1. 0. 0.]\n [0. 1. 0.]\n [0. 0. 1.]\n [1. 0. 0.]\n [0. 0. 1.]\n [1. 0. 0.]\n [0. 1. 0.]\n [1. 0. 0.]\n [0. 0. 1.]\n [0. 0. 1.]\n [0. 0. 1.]\n [0. 1. 0.]\n [0. 0. 1.]\n [0. 1. 0.]\n [0. 0. 1.]\n [0. 1. 0.]\n [1. 0. 0.]\n [0. 0. 1.]\n [1. 0. 0.]\n [1. 0. 0.]\n [1. 0. 0.]\n [1. 0. 0.]\n [0. 0. 1.]\n [0. 0. 1.]\n [0. 0. 1.]\n [0. 0. 1.]\n [0. 1. 0.]\n [0. 0. 1.]\n [0. 0. 1.]\n [1. 0. 0.]\n [1. 0. 0.]\n [0. 1. 0.]\n [1. 0. 0.]\n [0. 1. 0.]\n [0. 0. 1.]\n [0. 1. 0.]\n [0. 0. 1.]\n [1. 0. 0.]\n [1. 0. 0.]\n [0. 0. 1.]\n [0. 0. 1.]\n [0. 1. 0.]\n [1. 0. 0.]\n [1. 0. 0.]\n [0. 0. 1.]\n [0. 0. 1.]\n [1. 0. 0.]\n [0. 1. 0.]\n [1. 0. 0.]\n [1. 0. 0.]\n [1. 0. 0.]\n [0. 0. 1.]\n [0. 0. 1.]\n [0. 1. 0.]\n [0. 0. 1.]\n [0. 0. 1.]\n [0. 0. 1.]\n [1. 0. 0.]\n [0. 1. 0.]\n [0. 0. 1.]\n [1. 0. 0.]\n [0. 0. 1.]\n [0. 0. 1.]\n [0. 0. 1.]\n [1. 0. 0.]\n [0. 0. 1.]\n [0. 0. 1.]\n [0. 1. 0.]\n [0. 1. 0.]\n [1. 0. 0.]\n [0. 0. 1.]\n [1. 0. 0.]\n [0. 0. 1.]\n [0. 1. 0.]\n [0. 0. 1.]\n [0. 0. 1.]\n [0. 0. 1.]\n [0. 0. 1.]\n [1. 0. 0.]\n [0. 0. 1.]\n [1. 0. 0.]\n [1. 0. 0.]\n [1. 0. 0.]\n [1. 0. 0.]\n [0. 0. 1.]\n [0. 0. 1.]\n [0. 1. 0.]\n [0. 0. 1.]\n [0. 0. 1.]\n [1. 0. 0.]\n [1. 0. 0.]\n [1. 0. 0.]\n [1. 0. 0.]\n [0. 0. 1.]\n [0. 1. 0.]\n [0. 0. 1.]\n [0. 0. 1.]\n [0. 0. 1.]\n [1. 0. 0.]\n [0. 1. 0.]\n [0. 1. 0.]\n [1. 0. 0.]\n [0. 1. 0.]\n [1. 0. 0.]\n [1. 0. 0.]\n [1. 0. 0.]\n [0. 1. 0.]\n [1. 0. 0.]\n [1. 0. 0.]\n [1. 0. 0.]\n [0. 0. 1.]\n [0. 1. 0.]\n [0. 1. 0.]]"
      },
      "random_forest_calibrated": {
        "accuracy": 1.0,
        "log_loss": 0.0004747600931415896,
        "predictions": "[0 2 2 0 2 2 2 1 2 1 1 1 2 2 0 2 0 2 2 0 1 2 2 0 2 2 0 2 0 2 2 1 2 1 2 2 1\n 2 2 2 0 0 1 2 0 2 0 1 0 2 2 2 1 2 1 2 1 0 2 0 0 0 0 2 2 2 2 1 2 2 0 0 1 0\n 1 2 1 2 0 0 2 2 1 0 0 2 2 0 1 0 0 0 2 2 1 2 2 2 0 1 2 0 2 2 2 0 2 2 1 1 0\n 2 0 2 1 2 2 2 2 0 2 0 0 0 0 2 2 1 2 2 0 0 0 0 2 1 2 2 2 0 1 1 0 1 0 0 0 1\n 0 0 0 2 1 1]",
        "probabilities": "[[1.         0.         0.        ]\n [0.         0.         1.        ]\n [0.         0.         1.        ]\n [1.         0.         0.        ]\n [0.         0.         1.        ]\n [0.         0.         1.        ]\n [0.         0.         1.        ]\n [0.         1.         0.        ]\n [0.         0.         1.        ]\n [0.         1.         0.        ]\n [0.         1.         0.        ]\n [0.         1.         0.        ]\n [0.         0.         1.        ]\n [0.         0.         1.        ]\n [1.         0.         0.        ]\n [0.         0.         1.        ]\n [1.         0.         0.        ]\n [0.         0.         1.        ]\n [0.         0.         1.        ]\n [1.         0.         0.        ]\n [0.02137295 0.96772477 0.01090228]\n [0.         0.         1.        ]\n [0.         0.         1.        ]\n [1.         0.         0.        ]\n [0.         0.         1.        ]\n [0.         0.         1.        ]\n [1.         0.         0.        ]\n [0.         0.         1.        ]\n [1.         0.         0.        ]\n [0.         0.         1.        ]\n [0.         0.         1.        ]\n [0.         1.         0.        ]\n [0.         0.         1.        ]\n [0.         1.         0.        ]\n [0.         0.         1.        ]\n [0.         0.         1.        ]\n [0.         1.         0.        ]\n [0.         0.         1.        ]\n [0.         0.         1.        ]\n [0.         0.         1.        ]\n [1.         0.         0.        ]\n [1.         0.         0.        ]\n [0.         1.         0.        ]\n [0.         0.         1.        ]\n [1.         0.         0.        ]\n [0.         0.         1.        ]\n [1.         0.         0.        ]\n [0.         1.         0.        ]\n [1.         0.         0.        ]\n [0.         0.         1.        ]\n [0.         0.         1.        ]\n [0.         0.         1.        ]\n [0.         1.         0.        ]\n [0.         0.         1.        ]\n [0.         1.         0.        ]\n [0.         0.         1.        ]\n [0.         1.         0.        ]\n [1.         0.         0.        ]\n [0.         0.         1.        ]\n [1.         0.         0.        ]\n [1.         0.         0.        ]\n [1.         0.         0.        ]\n [1.         0.         0.        ]\n [0.         0.         1.        ]\n [0.         0.         1.        ]\n [0.         0.         1.        ]\n [0.         0.         1.        ]\n [0.         1.         0.        ]\n [0.         0.         1.        ]\n [0.         0.         1.        ]\n [1.         0.         0.        ]\n [1.         0.         0.        ]\n [0.         1.         0.        ]\n [1.         0.         0.        ]\n [0.         1.         0.        ]\n [0.         0.         1.        ]\n [0.         1.         0.        ]\n [0.         0.         1.        ]\n [1.         0.         0.        ]\n [1.         0.         0.        ]\n [0.         0.         1.        ]\n [0.         0.         1.        ]\n [0.01960255 0.97798046 0.00241699]\n [1.         0.         0.        ]\n [1.         0.         0.        ]\n [0.         0.         1.        ]\n [0.         0.         1.        ]\n [1.         0.         0.        ]\n [0.         1.         0.        ]\n [1.         0.         0.        ]\n [1.         0.         0.        ]\n [1.         0.         0.        ]\n [0.         0.         1.        ]\n [0.         0.         1.        ]\n [0.         1.         0.        ]\n [0.         0.         1.        ]\n [0.         0.         1.        ]\n [0.         0.         1.        ]\n [1.         0.         0.        ]\n [0.         1.         0.        ]\n [0.         0.         1.        ]\n [1.         0.         0.        ]\n [0.         0.         1.        ]\n [0.         0.         1.        ]\n [0.         0.         1.        ]\n [1.         0.         0.        ]\n [0.         0.         1.        ]\n [0.         0.         1.        ]\n [0.         1.         0.        ]\n [0.         1.         0.        ]\n [1.         0.         0.        ]\n [0.         0.         1.        ]\n [1.         0.         0.        ]\n [0.         0.         1.        ]\n [0.         1.         0.        ]\n [0.         0.         1.        ]\n [0.         0.         1.        ]\n [0.         0.         1.        ]\n [0.         0.         1.        ]\n [1.         0.         0.        ]\n [0.         0.         1.        ]\n [1.         0.         0.        ]\n [1.         0.         0.        ]\n [1.         0.         0.        ]\n [1.         0.         0.        ]\n [0.         0.         1.        ]\n [0.         0.         1.        ]\n [0.         1.         0.        ]\n [0.         0.         1.        ]\n [0.         0.         1.        ]\n [0.99194069 0.00805931 0.        ]\n [1.         0.         0.        ]\n [1.         0.         0.        ]\n [1.         0.         0.        ]\n [0.         0.         1.        ]\n [0.         1.         0.        ]\n [0.         0.         1.        ]\n [0.         0.         1.        ]\n [0.         0.         1.        ]\n [1.         0.         0.        ]\n [0.         1.         0.        ]\n [0.         1.         0.        ]\n [1.         0.         0.        ]\n [0.         1.         0.        ]\n [1.         0.         0.        ]\n [1.         0.         0.        ]\n [1.         0.         0.        ]\n [0.         1.         0.        ]\n [1.         0.         0.        ]\n [1.         0.         0.        ]\n [0.99010138 0.00989862 0.        ]\n [0.         0.         1.        ]\n [0.         1.         0.        ]\n [0.         1.         0.        ]]"
      },
      "logistic_regression_calibrated": {
        "accuracy": 1.0,
        "log_loss": 0.0013072101164344292,
        "predictions": "[0 2 2 0 2 2 2 1 2 1 1 1 2 2 0 2 0 2 2 0 1 2 2 0 2 2 0 2 0 2 2 1 2 1 2 2 1\n 2 2 2 0 0 1 2 0 2 0 1 0 2 2 2 1 2 1 2 1 0 2 0 0 0 0 2 2 2 2 1 2 2 0 0 1 0\n 1 2 1 2 0 0 2 2 1 0 0 2 2 0 1 0 0 0 2 2 1 2 2 2 0 1 2 0 2 2 2 0 2 2 1 1 0\n 2 0 2 1 2 2 2 2 0 2 0 0 0 0 2 2 1 2 2 0 0 0 0 2 1 2 2 2 0 1 1 0 1 0 0 0 1\n 0 0 0 2 1 1]",
        "probabilities": "[[1.00000000e+00 0.00000000e+00 0.00000000e+00]\n [0.00000000e+00 0.00000000e+00 1.00000000e+00]\n [0.00000000e+00 0.00000000e+00 1.00000000e+00]\n [1.00000000e+00 0.00000000e+00 0.00000000e+00]\n [0.00000000e+00 0.00000000e+00 1.00000000e+00]\n [0.00000000e+00 0.00000000e+00 1.00000000e+00]\n [0.00000000e+00 0.00000000e+00 1.00000000e+00]\n [0.00000000e+00 1.00000000e+00 0.00000000e+00]\n [0.00000000e+00 0.00000000e+00 1.00000000e+00]\n [0.00000000e+00 1.00000000e+00 0.00000000e+00]\n [0.00000000e+00 1.00000000e+00 0.00000000e+00]\n [0.00000000e+00 1.00000000e+00 0.00000000e+00]\n [0.00000000e+00 0.00000000e+00 1.00000000e+00]\n [0.00000000e+00 0.00000000e+00 1.00000000e+00]\n [1.00000000e+00 0.00000000e+00 0.00000000e+00]\n [0.00000000e+00 0.00000000e+00 1.00000000e+00]\n [1.00000000e+00 0.00000000e+00 0.00000000e+00]\n [0.00000000e+00 0.00000000e+00 1.00000000e+00]\n [0.00000000e+00 0.00000000e+00 1.00000000e+00]\n [1.00000000e+00 0.00000000e+00 0.00000000e+00]\n [0.00000000e+00 1.00000000e+00 0.00000000e+00]\n [0.00000000e+00 0.00000000e+00 1.00000000e+00]\n [0.00000000e+00 0.00000000e+00 1.00000000e+00]\n [1.00000000e+00 0.00000000e+00 0.00000000e+00]\n [0.00000000e+00 0.00000000e+00 1.00000000e+00]\n [0.00000000e+00 0.00000000e+00 1.00000000e+00]\n [1.00000000e+00 0.00000000e+00 0.00000000e+00]\n [0.00000000e+00 0.00000000e+00 1.00000000e+00]\n [9.89364733e-01 1.06352674e-02 0.00000000e+00]\n [0.00000000e+00 0.00000000e+00 1.00000000e+00]\n [0.00000000e+00 0.00000000e+00 1.00000000e+00]\n [0.00000000e+00 1.00000000e+00 0.00000000e+00]\n [0.00000000e+00 0.00000000e+00 1.00000000e+00]\n [0.00000000e+00 1.00000000e+00 0.00000000e+00]\n [0.00000000e+00 0.00000000e+00 1.00000000e+00]\n [0.00000000e+00 0.00000000e+00 1.00000000e+00]\n [0.00000000e+00 1.00000000e+00 0.00000000e+00]\n [0.00000000e+00 0.00000000e+00 1.00000000e+00]\n [0.00000000e+00 0.00000000e+00 1.00000000e+00]\n [0.00000000e+00 0.00000000e+00 1.00000000e+00]\n [1.00000000e+00 0.00000000e+00 0.00000000e+00]\n [1.00000000e+00 0.00000000e+00 0.00000000e+00]\n [0.00000000e+00 1.00000000e+00 0.00000000e+00]\n [0.00000000e+00 0.00000000e+00 1.00000000e+00]\n [1.00000000e+00 0.00000000e+00 0.00000000e+00]\n [0.00000000e+00 5.33712295e-02 9.46628770e-01]\n [1.00000000e+00 0.00000000e+00 0.00000000e+00]\n [0.00000000e+00 1.00000000e+00 0.00000000e+00]\n [1.00000000e+00 0.00000000e+00 0.00000000e+00]\n [0.00000000e+00 0.00000000e+00 1.00000000e+00]\n [0.00000000e+00 0.00000000e+00 1.00000000e+00]\n [0.00000000e+00 0.00000000e+00 1.00000000e+00]\n [0.00000000e+00 1.00000000e+00 0.00000000e+00]\n [0.00000000e+00 0.00000000e+00 1.00000000e+00]\n [0.00000000e+00 1.00000000e+00 0.00000000e+00]\n [0.00000000e+00 0.00000000e+00 1.00000000e+00]\n [0.00000000e+00 1.00000000e+00 0.00000000e+00]\n [1.00000000e+00 0.00000000e+00 0.00000000e+00]\n [0.00000000e+00 0.00000000e+00 1.00000000e+00]\n [1.00000000e+00 0.00000000e+00 0.00000000e+00]\n [1.00000000e+00 0.00000000e+00 0.00000000e+00]\n [1.00000000e+00 0.00000000e+00 0.00000000e+00]\n [1.00000000e+00 0.00000000e+00 0.00000000e+00]\n [0.00000000e+00 0.00000000e+00 1.00000000e+00]\n [0.00000000e+00 0.00000000e+00 1.00000000e+00]\n [0.00000000e+00 0.00000000e+00 1.00000000e+00]\n [0.00000000e+00 0.00000000e+00 1.00000000e+00]\n [0.00000000e+00 1.00000000e+00 0.00000000e+00]\n [0.00000000e+00 0.00000000e+00 1.00000000e+00]\n [0.00000000e+00 0.00000000e+00 1.00000000e+00]\n [9.86416876e-01 1.35831242e-02 0.00000000e+00]\n [1.00000000e+00 0.00000000e+00 0.00000000e+00]\n [0.00000000e+00 1.00000000e+00 0.00000000e+00]\n [1.00000000e+00 0.00000000e+00 0.00000000e+00]\n [0.00000000e+00 1.00000000e+00 0.00000000e+00]\n [0.00000000e+00 0.00000000e+00 1.00000000e+00]\n [0.00000000e+00 1.00000000e+00 0.00000000e+00]\n [0.00000000e+00 0.00000000e+00 1.00000000e+00]\n [1.00000000e+00 0.00000000e+00 0.00000000e+00]\n [1.00000000e+00 0.00000000e+00 0.00000000e+00]\n [0.00000000e+00 0.00000000e+00 1.00000000e+00]\n [0.00000000e+00 0.00000000e+00 1.00000000e+00]\n [0.00000000e+00 1.00000000e+00 0.00000000e+00]\n [1.00000000e+00 0.00000000e+00 0.00000000e+00]\n [1.00000000e+00 0.00000000e+00 0.00000000e+00]\n [0.00000000e+00 0.00000000e+00 1.00000000e+00]\n [0.00000000e+00 0.00000000e+00 1.00000000e+00]\n [1.00000000e+00 0.00000000e+00 0.00000000e+00]\n [0.00000000e+00 9.05256623e-01 9.47433769e-02]\n [1.00000000e+00 0.00000000e+00 0.00000000e+00]\n [1.00000000e+00 0.00000000e+00 0.00000000e+00]\n [1.00000000e+00 0.00000000e+00 0.00000000e+00]\n [0.00000000e+00 0.00000000e+00 1.00000000e+00]\n [0.00000000e+00 0.00000000e+00 1.00000000e+00]\n [0.00000000e+00 1.00000000e+00 0.00000000e+00]\n [0.00000000e+00 0.00000000e+00 1.00000000e+00]\n [0.00000000e+00 0.00000000e+00 1.00000000e+00]\n [0.00000000e+00 0.00000000e+00 1.00000000e+00]\n [1.00000000e+00 0.00000000e+00 0.00000000e+00]\n [0.00000000e+00 1.00000000e+00 0.00000000e+00]\n [0.00000000e+00 0.00000000e+00 1.00000000e+00]\n [1.00000000e+00 0.00000000e+00 0.00000000e+00]\n [0.00000000e+00 0.00000000e+00 1.00000000e+00]\n [0.00000000e+00 0.00000000e+00 1.00000000e+00]\n [0.00000000e+00 0.00000000e+00 1.00000000e+00]\n [1.00000000e+00 0.00000000e+00 0.00000000e+00]\n [0.00000000e+00 0.00000000e+00 1.00000000e+00]\n [0.00000000e+00 0.00000000e+00 1.00000000e+00]\n [0.00000000e+00 1.00000000e+00 0.00000000e+00]\n [0.00000000e+00 1.00000000e+00 0.00000000e+00]\n [1.00000000e+00 0.00000000e+00 0.00000000e+00]\n [0.00000000e+00 0.00000000e+00 1.00000000e+00]\n [1.00000000e+00 0.00000000e+00 0.00000000e+00]\n [0.00000000e+00 0.00000000e+00 1.00000000e+00]\n [0.00000000e+00 1.00000000e+00 0.00000000e+00]\n [0.00000000e+00 0.00000000e+00 1.00000000e+00]\n [0.00000000e+00 0.00000000e+00 1.00000000e+00]\n [0.00000000e+00 0.00000000e+00 1.00000000e+00]\n [0.00000000e+00 0.00000000e+00 1.00000000e+00]\n [9.78376352e-01 2.16236477e-02 0.00000000e+00]\n [0.00000000e+00 0.00000000e+00 1.00000000e+00]\n [1.00000000e+00 0.00000000e+00 0.00000000e+00]\n [1.00000000e+00 0.00000000e+00 0.00000000e+00]\n [1.00000000e+00 0.00000000e+00 0.00000000e+00]\n [1.00000000e+00 0.00000000e+00 0.00000000e+00]\n [0.00000000e+00 0.00000000e+00 1.00000000e+00]\n [0.00000000e+00 0.00000000e+00 1.00000000e+00]\n [0.00000000e+00 1.00000000e+00 0.00000000e+00]\n [0.00000000e+00 0.00000000e+00 1.00000000e+00]\n [0.00000000e+00 0.00000000e+00 1.00000000e+00]\n [1.00000000e+00 0.00000000e+00 0.00000000e+00]\n [1.00000000e+00 0.00000000e+00 0.00000000e+00]\n [1.00000000e+00 0.00000000e+00 0.00000000e+00]\n [1.00000000e+00 0.00000000e+00 0.00000000e+00]\n [0.00000000e+00 0.00000000e+00 1.00000000e+00]\n [0.00000000e+00 1.00000000e+00 0.00000000e+00]\n [0.00000000e+00 0.00000000e+00 1.00000000e+00]\n [0.00000000e+00 0.00000000e+00 1.00000000e+00]\n [0.00000000e+00 0.00000000e+00 1.00000000e+00]\n [1.00000000e+00 0.00000000e+00 0.00000000e+00]\n [6.95723856e-04 9.99304276e-01 0.00000000e+00]\n [0.00000000e+00 1.00000000e+00 0.00000000e+00]\n [1.00000000e+00 0.00000000e+00 0.00000000e+00]\n [0.00000000e+00 1.00000000e+00 0.00000000e+00]\n [1.00000000e+00 0.00000000e+00 0.00000000e+00]\n [1.00000000e+00 0.00000000e+00 0.00000000e+00]\n [1.00000000e+00 0.00000000e+00 0.00000000e+00]\n [0.00000000e+00 1.00000000e+00 0.00000000e+00]\n [1.00000000e+00 0.00000000e+00 0.00000000e+00]\n [1.00000000e+00 0.00000000e+00 0.00000000e+00]\n [1.00000000e+00 0.00000000e+00 0.00000000e+00]\n [0.00000000e+00 0.00000000e+00 1.00000000e+00]\n [0.00000000e+00 1.00000000e+00 0.00000000e+00]\n [0.00000000e+00 1.00000000e+00 0.00000000e+00]]"
      },
      "ensemble_calibrated": {
        "accuracy": 1.0,
        "log_loss": 8.528479789981625e-05,
        "predictions": "[0 2 2 0 2 2 2 1 2 1 1 1 2 2 0 2 0 2 2 0 1 2 2 0 2 2 0 2 0 2 2 1 2 1 2 2 1\n 2 2 2 0 0 1 2 0 2 0 1 0 2 2 2 1 2 1 2 1 0 2 0 0 0 0 2 2 2 2 1 2 2 0 0 1 0\n 1 2 1 2 0 0 2 2 1 0 0 2 2 0 1 0 0 0 2 2 1 2 2 2 0 1 2 0 2 2 2 0 2 2 1 1 0\n 2 0 2 1 2 2 2 2 0 2 0 0 0 0 2 2 1 2 2 0 0 0 0 2 1 2 2 2 0 1 1 0 1 0 0 0 1\n 0 0 0 2 1 1]",
        "probabilities": "[[1.00000000e+00 0.00000000e+00 0.00000000e+00]\n [0.00000000e+00 0.00000000e+00 1.00000000e+00]\n [0.00000000e+00 0.00000000e+00 1.00000000e+00]\n [1.00000000e+00 0.00000000e+00 0.00000000e+00]\n [0.00000000e+00 0.00000000e+00 1.00000000e+00]\n [0.00000000e+00 0.00000000e+00 1.00000000e+00]\n [0.00000000e+00 0.00000000e+00 1.00000000e+00]\n [0.00000000e+00 1.00000000e+00 0.00000000e+00]\n [0.00000000e+00 0.00000000e+00 1.00000000e+00]\n [0.00000000e+00 1.00000000e+00 0.00000000e+00]\n [0.00000000e+00 1.00000000e+00 0.00000000e+00]\n [0.00000000e+00 1.00000000e+00 0.00000000e+00]\n [0.00000000e+00 0.00000000e+00 1.00000000e+00]\n [0.00000000e+00 0.00000000e+00 1.00000000e+00]\n [1.00000000e+00 0.00000000e+00 0.00000000e+00]\n [0.00000000e+00 0.00000000e+00 1.00000000e+00]\n [1.00000000e+00 0.00000000e+00 0.00000000e+00]\n [0.00000000e+00 0.00000000e+00 1.00000000e+00]\n [0.00000000e+00 0.00000000e+00 1.00000000e+00]\n [1.00000000e+00 0.00000000e+00 0.00000000e+00]\n [3.76700824e-03 9.93021297e-01 3.21169517e-03]\n [0.00000000e+00 0.00000000e+00 1.00000000e+00]\n [0.00000000e+00 0.00000000e+00 1.00000000e+00]\n [1.00000000e+00 0.00000000e+00 0.00000000e+00]\n [0.00000000e+00 0.00000000e+00 1.00000000e+00]\n [0.00000000e+00 0.00000000e+00 1.00000000e+00]\n [1.00000000e+00 0.00000000e+00 0.00000000e+00]\n [0.00000000e+00 0.00000000e+00 1.00000000e+00]\n [1.00000000e+00 0.00000000e+00 0.00000000e+00]\n [0.00000000e+00 0.00000000e+00 1.00000000e+00]\n [0.00000000e+00 0.00000000e+00 1.00000000e+00]\n [0.00000000e+00 1.00000000e+00 0.00000000e+00]\n [0.00000000e+00 0.00000000e+00 1.00000000e+00]\n [0.00000000e+00 1.00000000e+00 0.00000000e+00]\n [0.00000000e+00 0.00000000e+00 1.00000000e+00]\n [0.00000000e+00 0.00000000e+00 1.00000000e+00]\n [0.00000000e+00 1.00000000e+00 0.00000000e+00]\n [0.00000000e+00 0.00000000e+00 1.00000000e+00]\n [0.00000000e+00 0.00000000e+00 1.00000000e+00]\n [0.00000000e+00 0.00000000e+00 1.00000000e+00]\n [1.00000000e+00 0.00000000e+00 0.00000000e+00]\n [1.00000000e+00 0.00000000e+00 0.00000000e+00]\n [0.00000000e+00 1.00000000e+00 0.00000000e+00]\n [0.00000000e+00 0.00000000e+00 1.00000000e+00]\n [1.00000000e+00 0.00000000e+00 0.00000000e+00]\n [0.00000000e+00 0.00000000e+00 1.00000000e+00]\n [1.00000000e+00 0.00000000e+00 0.00000000e+00]\n [0.00000000e+00 1.00000000e+00 0.00000000e+00]\n [1.00000000e+00 0.00000000e+00 0.00000000e+00]\n [0.00000000e+00 0.00000000e+00 1.00000000e+00]\n [0.00000000e+00 0.00000000e+00 1.00000000e+00]\n [0.00000000e+00 0.00000000e+00 1.00000000e+00]\n [0.00000000e+00 1.00000000e+00 0.00000000e+00]\n [0.00000000e+00 0.00000000e+00 1.00000000e+00]\n [0.00000000e+00 1.00000000e+00 0.00000000e+00]\n [0.00000000e+00 0.00000000e+00 1.00000000e+00]\n [0.00000000e+00 1.00000000e+00 0.00000000e+00]\n [1.00000000e+00 0.00000000e+00 0.00000000e+00]\n [0.00000000e+00 0.00000000e+00 1.00000000e+00]\n [1.00000000e+00 0.00000000e+00 0.00000000e+00]\n [1.00000000e+00 0.00000000e+00 0.00000000e+00]\n [1.00000000e+00 0.00000000e+00 0.00000000e+00]\n [1.00000000e+00 0.00000000e+00 0.00000000e+00]\n [0.00000000e+00 0.00000000e+00 1.00000000e+00]\n [0.00000000e+00 0.00000000e+00 1.00000000e+00]\n [0.00000000e+00 0.00000000e+00 1.00000000e+00]\n [0.00000000e+00 0.00000000e+00 1.00000000e+00]\n [0.00000000e+00 1.00000000e+00 0.00000000e+00]\n [0.00000000e+00 0.00000000e+00 1.00000000e+00]\n [0.00000000e+00 0.00000000e+00 1.00000000e+00]\n [1.00000000e+00 0.00000000e+00 0.00000000e+00]\n [1.00000000e+00 0.00000000e+00 0.00000000e+00]\n [0.00000000e+00 1.00000000e+00 0.00000000e+00]\n [1.00000000e+00 0.00000000e+00 0.00000000e+00]\n [0.00000000e+00 1.00000000e+00 0.00000000e+00]\n [0.00000000e+00 0.00000000e+00 1.00000000e+00]\n [0.00000000e+00 1.00000000e+00 0.00000000e+00]\n [0.00000000e+00 0.00000000e+00 1.00000000e+00]\n [1.00000000e+00 0.00000000e+00 0.00000000e+00]\n [1.00000000e+00 0.00000000e+00 0.00000000e+00]\n [0.00000000e+00 0.00000000e+00 1.00000000e+00]\n [0.00000000e+00 0.00000000e+00 1.00000000e+00]\n [1.98498624e-03 9.97227747e-01 7.87266351e-04]\n [1.00000000e+00 0.00000000e+00 0.00000000e+00]\n [1.00000000e+00 0.00000000e+00 0.00000000e+00]\n [0.00000000e+00 0.00000000e+00 1.00000000e+00]\n [0.00000000e+00 0.00000000e+00 1.00000000e+00]\n [1.00000000e+00 0.00000000e+00 0.00000000e+00]\n [0.00000000e+00 1.00000000e+00 0.00000000e+00]\n [1.00000000e+00 0.00000000e+00 0.00000000e+00]\n [1.00000000e+00 0.00000000e+00 0.00000000e+00]\n [1.00000000e+00 0.00000000e+00 0.00000000e+00]\n [0.00000000e+00 0.00000000e+00 1.00000000e+00]\n [0.00000000e+00 0.00000000e+00 1.00000000e+00]\n [0.00000000e+00 1.00000000e+00 0.00000000e+00]\n [0.00000000e+00 0.00000000e+00 1.00000000e+00]\n [0.00000000e+00 0.00000000e+00 1.00000000e+00]\n [0.00000000e+00 0.00000000e+00 1.00000000e+00]\n [1.00000000e+00 0.00000000e+00 0.00000000e+00]\n [0.00000000e+00 1.00000000e+00 0.00000000e+00]\n [0.00000000e+00 0.00000000e+00 1.00000000e+00]\n [1.00000000e+00 0.00000000e+00 0.00000000e+00]\n [0.00000000e+00 0.00000000e+00 1.00000000e+00]\n [0.00000000e+00 0.00000000e+00 1.00000000e+00]\n [0.00000000e+00 0.00000000e+00 1.00000000e+00]\n [1.00000000e+00 0.00000000e+00 0.00000000e+00]\n [0.00000000e+00 0.00000000e+00 1.00000000e+00]\n [0.00000000e+00 0.00000000e+00 1.00000000e+00]\n [0.00000000e+00 1.00000000e+00 0.00000000e+00]\n [0.00000000e+00 1.00000000e+00 0.00000000e+00]\n [1.00000000e+00 0.00000000e+00 0.00000000e+00]\n [0.00000000e+00 0.00000000e+00 1.00000000e+00]\n [1.00000000e+00 0.00000000e+00 0.00000000e+00]\n [0.00000000e+00 0.00000000e+00 1.00000000e+00]\n [0.00000000e+00 1.00000000e+00 0.00000000e+00]\n [0.00000000e+00 0.00000000e+00 1.00000000e+00]\n [0.00000000e+00 0.00000000e+00 1.00000000e+00]\n [0.00000000e+00 0.00000000e+00 1.00000000e+00]\n [0.00000000e+00 0.00000000e+00 1.00000000e+00]\n [1.00000000e+00 0.00000000e+00 0.00000000e+00]\n [0.00000000e+00 0.00000000e+00 1.00000000e+00]\n [1.00000000e+00 0.00000000e+00 0.00000000e+00]\n [1.00000000e+00 0.00000000e+00 0.00000000e+00]\n [1.00000000e+00 0.00000000e+00 0.00000000e+00]\n [1.00000000e+00 0.00000000e+00 0.00000000e+00]\n [0.00000000e+00 0.00000000e+00 1.00000000e+00]\n [0.00000000e+00 0.00000000e+00 1.00000000e+00]\n [0.00000000e+00 1.00000000e+00 0.00000000e+00]\n [0.00000000e+00 0.00000000e+00 1.00000000e+00]\n [0.00000000e+00 0.00000000e+00 1.00000000e+00]\n [9.98560846e-01 1.43915384e-03 0.00000000e+00]\n [1.00000000e+00 0.00000000e+00 0.00000000e+00]\n [1.00000000e+00 0.00000000e+00 0.00000000e+00]\n [1.00000000e+00 0.00000000e+00 0.00000000e+00]\n [0.00000000e+00 0.00000000e+00 1.00000000e+00]\n [0.00000000e+00 1.00000000e+00 0.00000000e+00]\n [0.00000000e+00 0.00000000e+00 1.00000000e+00]\n [0.00000000e+00 0.00000000e+00 1.00000000e+00]\n [0.00000000e+00 0.00000000e+00 1.00000000e+00]\n [1.00000000e+00 0.00000000e+00 0.00000000e+00]\n [0.00000000e+00 1.00000000e+00 0.00000000e+00]\n [0.00000000e+00 1.00000000e+00 0.00000000e+00]\n [1.00000000e+00 0.00000000e+00 0.00000000e+00]\n [0.00000000e+00 1.00000000e+00 0.00000000e+00]\n [1.00000000e+00 0.00000000e+00 0.00000000e+00]\n [1.00000000e+00 0.00000000e+00 0.00000000e+00]\n [1.00000000e+00 0.00000000e+00 0.00000000e+00]\n [0.00000000e+00 1.00000000e+00 0.00000000e+00]\n [1.00000000e+00 0.00000000e+00 0.00000000e+00]\n [1.00000000e+00 0.00000000e+00 0.00000000e+00]\n [9.98087434e-01 1.91256632e-03 0.00000000e+00]\n [0.00000000e+00 0.00000000e+00 1.00000000e+00]\n [0.00000000e+00 1.00000000e+00 0.00000000e+00]\n [0.00000000e+00 1.00000000e+00 0.00000000e+00]]"
      }
    },
    "feature_importance": {
      "xgboost": "                 feature  importance\n120       result_encoded    0.331848\n232        match_balance    0.137195\n101      goal_difference    0.125426\n98              home_win    0.120615\n99                  draw    0.108038\n..                   ...         ...\n235   away_team_strength    0.000000\n237   home_team_win_rate    0.000000\n238   away_team_win_rate    0.000000\n239  home_team_avg_goals    0.000000\n240  away_team_avg_goals    0.000000\n\n[241 rows x 2 columns]",
      "lightgbm": "                 feature  importance\n100             away_win         169\n98              home_win         164\n99                  draw         157\n101      goal_difference          51\n232        match_balance          37\n..                   ...         ...\n236   team_strength_diff           0\n237   home_team_win_rate           0\n238   away_team_win_rate           0\n239  home_team_avg_goals           0\n240  away_team_avg_goals           0\n\n[241 rows x 2 columns]",
      "random_forest": "                      feature  importance\n100                  away_win    0.105030\n101           goal_difference    0.093980\n104            home_advantage    0.091204\n99                       draw    0.089655\n232             match_balance    0.083351\n..                        ...         ...\n215      away_position_change    0.000000\n211   away_clean_sheet_streak    0.000000\n210   home_clean_sheet_streak    0.000000\n214      home_position_change    0.000000\n225  transfer_learning_factor    0.000000\n\n[241 rows x 2 columns]"
    },
    "success": true
  },
  "transfer_results": {
    "success": false,
    "error": "No source data files"
  },
  "tuning_results": {
    "optimization_results": {
      "xgboost": {
        "best_params": {
          "n_estimators": 182,
          "max_depth": 5,
          "learning_rate": 0.03822754945608633,
          "subsample": 0.9489588389616428,
          "colsample_bytree": 0.9948184317827552,
          "min_child_weight": 2,
          "reg_alpha": 0.7311119726517566,
          "reg_lambda": 1.1780281152900898
        },
        "best_value": 0.01023306955660576,
        "n_trials": 30,
        "study": "<optuna.study.study.Study object at 0x7ffb299ee120>",
        "model_type": "xgboost"
      },
      "lightgbm": {
        "best_params": {
          "n_estimators": 199,
          "max_depth": 4,
          "learning_rate": 0.09939936498022436,
          "num_leaves": 71,
          "feature_fraction": 0.7944598432202841,
          "bagging_fraction": 0.8275521263940526,
          "bagging_freq": 7,
          "lambda_l1": 0.10846079552796235,
          "lambda_l2": 1.3790065449478965,
          "min_data_in_leaf": 42
        },
        "best_value": 0.0014072020231248384,
        "n_trials": 30,
        "study": "<optuna.study.study.Study object at 0x7ffb29545e50>",
        "model_type": "lightgbm"
      },
      "random_forest": {
        "best_params": {
          "n_estimators": 152,
          "max_depth": 14,
          "min_samples_split": 16,
          "min_samples_leaf": 2,
          "max_features": null
        },
        "best_value": 2.2204460492503136e-16,
        "n_trials": 30,
        "study": "<optuna.study.study.Study object at 0x7ffb29546ad0>",
        "model_type": "random_forest"
      },
      "logistic_regression": {
        "best_params": {
          "C": 9.976267815765638,
          "max_iter": 893,
          "solver": "lbfgs"
        },
        "best_value": 0.0020167308279405416,
        "n_trials": 30,
        "study": "<optuna.study.study.Study object at 0x7ffb25c5cb00>",
        "model_type": "logistic_regression"
      }
    },
    "validation_results": {
      "models": {
        "xgboost": {
          "validation_passed": true,
          "issues": []
        },
        "lightgbm": {
          "validation_passed": true,
          "issues": []
        },
        "random_forest": {
          "validation_passed": true,
          "issues": []
        },
        "logistic_regression": {
          "validation_passed": true,
          "issues": []
        }
      },
      "overall": {
        "validation_passed": true,
        "issues": []
      }
    },
    "comparison_results": {
      "models": {
        "xgboost": {
          "score": 0.01023306955660576,
          "params": {
            "n_estimators": 182,
            "max_depth": 5,
            "learning_rate": 0.03822754945608633,
            "subsample": 0.9489588389616428,
            "colsample_bytree": 0.9948184317827552,
            "min_child_weight": 2,
            "reg_alpha": 0.7311119726517566,
            "reg_lambda": 1.1780281152900898
          }
        },
        "lightgbm": {
          "score": 0.0014072020231248384,
          "params": {
            "n_estimators": 199,
            "max_depth": 4,
            "learning_rate": 0.09939936498022436,
            "num_leaves": 71,
            "feature_fraction": 0.7944598432202841,
            "bagging_fraction": 0.8275521263940526,
            "bagging_freq": 7,
            "lambda_l1": 0.10846079552796235,
            "lambda_l2": 1.3790065449478965,
            "min_data_in_leaf": 42
          }
        },
        "random_forest": {
          "score": 2.2204460492503136e-16,
          "params": {
            "n_estimators": 152,
            "max_depth": 14,
            "min_samples_split": 16,
            "min_samples_leaf": 2,
            "max_features": null
          }
        },
        "logistic_regression": {
          "score": 0.0020167308279405416,
          "params": {
            "C": 9.976267815765638,
            "max_iter": 893,
            "solver": "lbfgs"
          }
        }
      },
      "best_model": "random_forest",
      "best_score": 2.2204460492503136e-16
    },
    "evaluation_results": {
      "xgboost": {
        "mean_score": 0.01023722162467019,
        "std_score": 0.0044520051711102165,
        "scores": [
          0.01634205762231578,
          0.00851850454688483,
          0.005851102704809961
        ],
        "model": "XGBClassifier(base_score=None, booster=None, callbacks=None,\n              colsample_bylevel=None, colsample_bynode=None,\n              colsample_bytree=0.9948184317827552, device=None,\n              early_stopping_rounds=None, enable_categorical=False,\n              eval_metric='mlogloss', feature_types=None, feature_weights=None,\n              gamma=None, grow_policy=None, importance_type=None,\n              interaction_constraints=None, learning_rate=0.03822754945608633,\n              max_bin=None, max_cat_threshold=None, max_cat_to_onehot=None,\n              max_delta_step=None, max_depth=5, max_leaves=None,\n              min_child_weight=2, missing=nan, monotone_constraints=None,\n              multi_strategy=None, n_estimators=182, n_jobs=None,\n              num_parallel_tree=None, ...)"
      },
      "lightgbm": {
        "mean_score": 0.0014065491739287466,
        "std_score": 0.0006281130893479764,
        "scores": [
          0.0022584238838878187,
          0.0011986053702632328,
          0.0007626182676351891
        ],
        "model": "LGBMClassifier(bagging_fraction=0.8275521263940526, bagging_freq=7,\n               feature_fraction=0.7944598432202841,\n               lambda_l1=0.10846079552796235, lambda_l2=1.3790065449478965,\n               learning_rate=0.09939936498022436, max_depth=4,\n               min_data_in_leaf=42, n_estimators=199, num_leaves=71,\n               verbose=-1)"
      },
      "random_forest": {
        "mean_score": 2.2204460492503136e-16,
        "std_score": 0.0,
        "scores": [
          2.2204460492503136e-16,
          2.2204460492503136e-16,
          2.2204460492503136e-16
        ],
        "model": "RandomForestClassifier(max_depth=14, max_features=None, min_samples_leaf=2,\n                       min_samples_split=16, n_estimators=152)"
      },
      "logistic_regression": {
        "mean_score": 0.0020167308279405416,
        "std_score": 0.0016610782600570402,
        "scores": [
          0.004347770853846778,
          0.001103125041001909,
          0.0005992965889729385
        ],
        "model": "LogisticRegression(C=9.976267815765638, max_iter=893)"
      }
    },
    "importance_results": {
      "xgboost": {
        "importance": {
          "n_estimators": 0.5638587936015486,
          "learning_rate": 0.2871614882383129,
          "min_child_weight": 0.05216372218236044,
          "subsample": 0.04494135723604435,
          "colsample_bytree": 0.04451737811666808,
          "reg_alpha": 0.005431661746462462,
          "max_depth": 0.0010895584317984436,
          "reg_lambda": 0.000836040446804723
        },
        "best_params": {
          "n_estimators": 182,
          "max_depth": 5,
          "learning_rate": 0.03822754945608633,
          "subsample": 0.9489588389616428,
          "colsample_bytree": 0.9948184317827552,
          "min_child_weight": 2,
          "reg_alpha": 0.7311119726517566,
          "reg_lambda": 1.1780281152900898
        },
        "best_value": 0.01023306955660576
      },
      "lightgbm": {
        "importance": {
          "bagging_fraction": 0.29380350455179305,
          "learning_rate": 0.281566517082108,
          "num_leaves": 0.16149205443981657,
          "lambda_l1": 0.10471881249562245,
          "n_estimators": 0.08536584745942874,
          "bagging_freq": 0.028081862798343262,
          "min_data_in_leaf": 0.022597311344752017,
          "max_depth": 0.01694319179097299,
          "lambda_l2": 0.005033182863298278,
          "feature_fraction": 0.00039771517386453554
        },
        "best_params": {
          "n_estimators": 199,
          "max_depth": 4,
          "learning_rate": 0.09939936498022436,
          "num_leaves": 71,
          "feature_fraction": 0.7944598432202841,
          "bagging_fraction": 0.8275521263940526,
          "bagging_freq": 7,
          "lambda_l1": 0.10846079552796235,
          "lambda_l2": 1.3790065449478965,
          "min_data_in_leaf": 42
        },
        "best_value": 0.0014072020231248384
      },
      "random_forest": {
        "importance": {
          "max_features": 0.7510081828851517,
          "min_samples_leaf": 0.21596912747711353,
          "min_samples_split": 0.0316442778039048,
          "max_depth": 0.0007454389782344947,
          "n_estimators": 0.0006329728555954796
        },
        "best_params": {
          "n_estimators": 152,
          "max_depth": 14,
          "min_samples_split": 16,
          "min_samples_leaf": 2,
          "max_features": null
        },
        "best_value": 2.2204460492503136e-16
      },
      "logistic_regression": {
        "importance": {
          "solver": 0.8599107078878894,
          "C": 0.12528584031404363,
          "max_iter": 0.014803451798066997
        },
        "best_params": {
          "C": 9.976267815765638,
          "max_iter": 893,
          "solver": "lbfgs"
        },
        "best_value": 0.0020167308279405416
      }
    },
    "success": true
  },
  "ensemble_results": {
    "base_models": {
      "xgboost": "XGBClassifier(base_score=None, booster=None, callbacks=None,\n              colsample_bylevel=None, colsample_bynode=None,\n              colsample_bytree=0.8, device=None, early_stopping_rounds=None,\n              enable_categorical=False, eval_metric='mlogloss',\n              feature_types=None, feature_weights=None, gamma=None,\n              grow_policy=None, importance_type=None,\n              interaction_constraints=None, learning_rate=0.03, max_bin=None,\n              max_cat_threshold=None, max_cat_to_onehot=None,\n              max_delta_step=None, max_depth=5, max_leaves=None,\n              min_child_weight=5, missing=nan, monotone_constraints=None,\n              multi_strategy=None, n_estimators=150, n_jobs=None,\n              num_parallel_tree=None, ...)",
      "lightgbm": "LGBMClassifier(bagging_fraction=0.8, bagging_freq=5, feature_fraction=0.8,\n               lambda_l1=0.2, lambda_l2=1.5, learning_rate=0.03, max_depth=5,\n               metric='multi_logloss', min_data_in_leaf=10, n_estimators=150,\n               num_class=3, num_leaves=30, objective='multiclass',\n               random_state=42, verbose=-1)",
      "random_forest": "RandomForestClassifier(max_depth=8, min_samples_leaf=5, min_samples_split=10,\n                       random_state=42)",
      "logistic_regression": "LogisticRegression(C=0.1, max_iter=1000, random_state=42)"
    },
    "training_results": {
      "xgboost": {
        "model": "XGBClassifier(base_score=None, booster=None, callbacks=None,\n              colsample_bylevel=None, colsample_bynode=None,\n              colsample_bytree=0.8, device=None, early_stopping_rounds=None,\n              enable_categorical=False, eval_metric='mlogloss',\n              feature_types=None, feature_weights=None, gamma=None,\n              grow_policy=None, importance_type=None,\n              interaction_constraints=None, learning_rate=0.03, max_bin=None,\n              max_cat_threshold=None, max_cat_to_onehot=None,\n              max_delta_step=None, max_depth=5, max_leaves=None,\n              min_child_weight=5, missing=nan, monotone_constraints=None,\n              multi_strategy=None, n_estimators=150, n_jobs=None,\n              num_parallel_tree=None, ...)",
        "train_accuracy": 1.0,
        "val_accuracy": 1.0,
        "train_logloss": 0.019125269932739202,
        "val_logloss": 0.01904938496786702,
        "overfitting": 0.0,
        "success": true
      },
      "lightgbm": {
        "model": "LGBMClassifier(bagging_fraction=0.8, bagging_freq=5, feature_fraction=0.8,\n               lambda_l1=0.2, lambda_l2=1.5, learning_rate=0.03, max_depth=5,\n               metric='multi_logloss', min_data_in_leaf=10, n_estimators=150,\n               num_class=3, num_leaves=30, objective='multiclass',\n               random_state=42, verbose=-1)",
        "train_accuracy": 1.0,
        "val_accuracy": 1.0,
        "train_logloss": 0.007724634666161266,
        "val_logloss": 0.007729591544659295,
        "overfitting": 0.0,
        "success": true
      },
      "random_forest": {
        "model": "RandomForestClassifier(max_depth=8, min_samples_leaf=5, min_samples_split=10,\n                       random_state=42)",
        "train_accuracy": 1.0,
        "val_accuracy": 1.0,
        "train_logloss": 0.0331503380843572,
        "val_logloss": 0.04629859119306572,
        "overfitting": 0.0,
        "success": true
      },
      "logistic_regression": {
        "model": "LogisticRegression(C=0.1, max_iter=1000, random_state=42)",
        "train_accuracy": 1.0,
        "val_accuracy": 1.0,
        "train_logloss": 0.021689907893286876,
        "val_logloss": 0.026479359246040435,
        "overfitting": 0.0,
        "success": true
      }
    },
    "conservative_ensemble": {
      "models": {
        "xgboost": "XGBClassifier(base_score=None, booster=None, callbacks=None,\n              colsample_bylevel=None, colsample_bynode=None,\n              colsample_bytree=0.8, device=None, early_stopping_rounds=None,\n              enable_categorical=False, eval_metric='mlogloss',\n              feature_types=None, feature_weights=None, gamma=None,\n              grow_policy=None, importance_type=None,\n              interaction_constraints=None, learning_rate=0.03, max_bin=None,\n              max_cat_threshold=None, max_cat_to_onehot=None,\n              max_delta_step=None, max_depth=5, max_leaves=None,\n              min_child_weight=5, missing=nan, monotone_constraints=None,\n              multi_strategy=None, n_estimators=150, n_jobs=None,\n              num_parallel_tree=None, ...)",
        "lightgbm": "LGBMClassifier(bagging_fraction=0.8, bagging_freq=5, feature_fraction=0.8,\n               lambda_l1=0.2, lambda_l2=1.5, learning_rate=0.03, max_depth=5,\n               metric='multi_logloss', min_data_in_leaf=10, n_estimators=150,\n               num_class=3, num_leaves=30, objective='multiclass',\n               random_state=42, verbose=-1)",
        "random_forest": "RandomForestClassifier(max_depth=8, min_samples_leaf=5, min_samples_split=10,\n                       random_state=42)",
        "logistic_regression": "LogisticRegression(C=0.1, max_iter=1000, random_state=42)"
      },
      "weights": {
        "xgboost": 0.4001754372983308,
        "lightgbm": 0.3521543981896826,
        "random_forest": 0.14800154084236178,
        "logistic_regression": 0.09966862366962478
      },
      "type": "conservative_weighted",
      "training_results": {
        "xgboost": {
          "model": "XGBClassifier(base_score=None, booster=None, callbacks=None,\n              colsample_bylevel=None, colsample_bynode=None,\n              colsample_bytree=0.8, device=None, early_stopping_rounds=None,\n              enable_categorical=False, eval_metric='mlogloss',\n              feature_types=None, feature_weights=None, gamma=None,\n              grow_policy=None, importance_type=None,\n              interaction_constraints=None, learning_rate=0.03, max_bin=None,\n              max_cat_threshold=None, max_cat_to_onehot=None,\n              max_delta_step=None, max_depth=5, max_leaves=None,\n              min_child_weight=5, missing=nan, monotone_constraints=None,\n              multi_strategy=None, n_estimators=150, n_jobs=None,\n              num_parallel_tree=None, ...)",
          "train_accuracy": 1.0,
          "val_accuracy": 1.0,
          "train_logloss": 0.019125269932739202,
          "val_logloss": 0.01904938496786702,
          "overfitting": 0.0,
          "success": true
        },
        "lightgbm": {
          "model": "LGBMClassifier(bagging_fraction=0.8, bagging_freq=5, feature_fraction=0.8,\n               lambda_l1=0.2, lambda_l2=1.5, learning_rate=0.03, max_depth=5,\n               metric='multi_logloss', min_data_in_leaf=10, n_estimators=150,\n               num_class=3, num_leaves=30, objective='multiclass',\n               random_state=42, verbose=-1)",
          "train_accuracy": 1.0,
          "val_accuracy": 1.0,
          "train_logloss": 0.007724634666161266,
          "val_logloss": 0.007729591544659295,
          "overfitting": 0.0,
          "success": true
        },
        "random_forest": {
          "model": "RandomForestClassifier(max_depth=8, min_samples_leaf=5, min_samples_split=10,\n                       random_state=42)",
          "train_accuracy": 1.0,
          "val_accuracy": 1.0,
          "train_logloss": 0.0331503380843572,
          "val_logloss": 0.04629859119306572,
          "overfitting": 0.0,
          "success": true
        },
        "logistic_regression": {
          "model": "LogisticRegression(C=0.1, max_iter=1000, random_state=42)",
          "train_accuracy": 1.0,
          "val_accuracy": 1.0,
          "train_logloss": 0.021689907893286876,
          "val_logloss": 0.026479359246040435,
          "overfitting": 0.0,
          "success": true
        }
      }
    },
    "ensemble_results": {
      "val_accuracy": 1.0,
      "val_logloss": 0.019627778287645403,
      "success": true,
      "type": "conservative_weighted"
    },
    "calibrated_models": {
      "xgboost": "CalibratedClassifierCV(cv=3,\n                       estimator=XGBClassifier(base_score=None, booster=None,\n                                               callbacks=None,\n                                               colsample_bylevel=None,\n                                               colsample_bynode=None,\n                                               colsample_bytree=0.8,\n                                               device=None,\n                                               early_stopping_rounds=None,\n                                               enable_categorical=False,\n                                               eval_metric='mlogloss',\n                                               feature_types=None,\n                                               feature_weights=None, gamma=None,\n                                               grow_policy=None,\n                                               importance_type=None,\n                                               interaction_constraints=None,\n                                               learning_rate=0.03, max_bin=None,\n                                               max_cat_threshold=None,\n                                               max_cat_to_onehot=None,\n                                               max_delta_step=None, max_depth=5,\n                                               max_leaves=None,\n                                               min_child_weight=5, missing=nan,\n                                               monotone_constraints=None,\n                                               multi_strategy=None,\n                                               n_estimators=150, n_jobs=None,\n                                               num_parallel_tree=None, ...),\n                       method='isotonic')",
      "lightgbm": "CalibratedClassifierCV(cv=3,\n                       estimator=LGBMClassifier(bagging_fraction=0.8,\n                                                bagging_freq=5,\n                                                feature_fraction=0.8,\n                                                lambda_l1=0.2, lambda_l2=1.5,\n                                                learning_rate=0.03, max_depth=5,\n                                                metric='multi_logloss',\n                                                min_data_in_leaf=10,\n                                                n_estimators=150, num_class=3,\n                                                num_leaves=30,\n                                                objective='multiclass',\n                                                random_state=42, verbose=-1),\n                       method='isotonic')",
      "random_forest": "CalibratedClassifierCV(cv=3,\n                       estimator=RandomForestClassifier(max_depth=8,\n                                                        min_samples_leaf=5,\n                                                        min_samples_split=10,\n                                                        random_state=42),\n                       method='isotonic')",
      "logistic_regression": "CalibratedClassifierCV(cv=3,\n                       estimator=LogisticRegression(C=0.1, max_iter=1000,\n                                                    random_state=42),\n                       method='isotonic')"
    },
    "cv_results": {
      "mean_accuracy": 1.0,
      "std_accuracy": 0.0,
      "mean_log_loss": 0.04674941628646468,
      "std_log_loss": 0.022106056440222625,
      "n_folds": 3,
      "scores": {
        "accuracy": [
          1.0,
          1.0,
          1.0
        ],
        "log_loss": [
          0.07722474631808462,
          0.03754973865101127,
          0.02547376389029816
        ],
        "folds": [
          0,
          1,
          2
        ]
      }
    },
    "evaluation_results": {
      "accuracy": 1.0,
      "log_loss": 0.024329742892413127,
      "mean_confidence": 0.976011849321168,
      "classification_report": {
        "Away Win": {
          "precision": 1.0,
          "recall": 1.0,
          "f1-score": 1.0,
          "support": 51.0
        },
        "Draw": {
          "precision": 1.0,
          "recall": 1.0,
          "f1-score": 1.0,
          "support": 32.0
        },
        "Home Win": {
          "precision": 1.0,
          "recall": 1.0,
          "f1-score": 1.0,
          "support": 71.0
        },
        "accuracy": 1.0,
        "macro avg": {
          "precision": 1.0,
          "recall": 1.0,
          "f1-score": 1.0,
          "support": 154.0
        },
        "weighted avg": {
          "precision": 1.0,
          "recall": 1.0,
          "f1-score": 1.0,
          "support": 154.0
        }
      },
      "confusion_matrix": "[[51  0  0]\n [ 0 32  0]\n [ 0  0 71]]",
      "predictions": "[0 2 2 0 2 2 2 1 2 1 1 1 2 2 0 2 0 2 2 0 1 2 2 0 2 2 0 2 0 2 2 1 2 1 2 2 1\n 2 2 2 0 0 1 2 0 2 0 1 0 2 2 2 1 2 1 2 1 0 2 0 0 0 0 2 2 2 2 1 2 2 0 0 1 0\n 1 2 1 2 0 0 2 2 1 0 0 2 2 0 1 0 0 0 2 2 1 2 2 2 0 1 2 0 2 2 2 0 2 2 1 1 0\n 2 0 2 1 2 2 2 2 0 2 0 0 0 0 2 2 1 2 2 0 0 0 0 2 1 2 2 2 0 1 1 0 1 0 0 0 1\n 0 0 0 2 1 1]",
      "probabilities": "[[0.96038815 0.02030811 0.01930375]\n [0.00651443 0.00780865 0.98567692]\n [0.02315563 0.01945212 0.95739224]\n [0.96369966 0.02034735 0.015953  ]\n [0.00870771 0.00820246 0.98308984]\n [0.00939555 0.02051443 0.97009003]\n [0.00771039 0.01126958 0.98102005]\n [0.0126045  0.97076129 0.01663419]\n [0.00823913 0.00773049 0.98403037]\n [0.01574732 0.97125017 0.01300251]\n [0.01212544 0.9733366  0.01453795]\n [0.01644963 0.96016549 0.02338488]\n [0.00997736 0.01629136 0.97373128]\n [0.00796719 0.00650931 0.98552352]\n [0.97769259 0.0139895  0.00831789]\n [0.01294718 0.00641703 0.9806358 ]\n [0.98172879 0.01005027 0.00822094]\n [0.01185168 0.01452432 0.97362402]\n [0.0083861  0.00969872 0.98191517]\n [0.97040406 0.01897771 0.01061825]\n [0.03497017 0.92098063 0.04404921]\n [0.00694937 0.00754776 0.9855029 ]\n [0.00928627 0.01072965 0.97998409]\n [0.98036427 0.00935076 0.01028498]\n [0.00951121 0.00699885 0.98348993]\n [0.00840167 0.01415136 0.97744697]\n [0.97602008 0.01503181 0.00894812]\n [0.00696015 0.00888772 0.98415216]\n [0.97380613 0.01789113 0.00830276]\n [0.00610877 0.00773835 0.98615287]\n [0.01258772 0.01291087 0.97450141]\n [0.01156608 0.97766681 0.01076711]\n [0.00986588 0.01584128 0.97429283]\n [0.014688   0.97011365 0.01519834]\n [0.00995248 0.00735742 0.98269012]\n [0.00804918 0.00686584 0.98508497]\n [0.01095337 0.98038281 0.00866382]\n [0.01244955 0.00803103 0.97951944]\n [0.01185651 0.01519586 0.97294762]\n [0.00959082 0.01104204 0.97936715]\n [0.98316752 0.00985778 0.00697471]\n [0.98007033 0.01123426 0.0086954 ]\n [0.0082803  0.98280345 0.00891624]\n [0.00706341 0.0091028  0.98383378]\n [0.96541983 0.02074721 0.01383297]\n [0.0089791  0.011646   0.97937489]\n [0.97841847 0.01412405 0.0074575 ]\n [0.01249449 0.97745908 0.01004643]\n [0.95145463 0.02708093 0.02146445]\n [0.00820507 0.00705414 0.98474081]\n [0.01325963 0.01819936 0.968541  ]\n [0.00800616 0.00762024 0.9843736 ]\n [0.00877927 0.97879906 0.01242167]\n [0.00609769 0.0072648  0.9866375 ]\n [0.016117   0.96814223 0.01574076]\n [0.00896779 0.00858411 0.98244812]\n [0.01477202 0.9743761  0.0108519 ]\n [0.97891069 0.01037594 0.01071336]\n [0.01397901 0.01195694 0.97406406]\n [0.97863314 0.00981238 0.01155449]\n [0.97753442 0.01235244 0.01011314]\n [0.98039019 0.01090957 0.00870026]\n [0.98144126 0.00842132 0.01013744]\n [0.00681729 0.00649771 0.98668499]\n [0.00826021 0.00796837 0.98377143]\n [0.01322862 0.01515829 0.97161308]\n [0.00985395 0.02078005 0.96936599]\n [0.01791242 0.96691866 0.01516889]\n [0.0082731  0.01652537 0.97520153]\n [0.0117465  0.0245898  0.9636637 ]\n [0.95463114 0.02701928 0.01834958]\n [0.98402263 0.00867234 0.00730505]\n [0.01037871 0.97746187 0.0121594 ]\n [0.97583876 0.01555465 0.00860655]\n [0.01278456 0.96957423 0.01764121]\n [0.00784253 0.01190919 0.9802483 ]\n [0.01730386 0.96994624 0.0127499 ]\n [0.01401061 0.01001356 0.97597584]\n [0.98069501 0.01080642 0.00849857]\n [0.98341238 0.0085593  0.00802833]\n [0.00864839 0.0073152  0.98403643]\n [0.00754652 0.00673281 0.98572066]\n [0.02242355 0.94030638 0.03727004]\n [0.98356789 0.00883616 0.00759597]\n [0.96791421 0.01789938 0.01418642]\n [0.00728376 0.0087965  0.98391976]\n [0.01446334 0.01433335 0.97120331]\n [0.98209657 0.01046083 0.00744263]\n [0.01048578 0.97420783 0.01530636]\n [0.97587903 0.01572969 0.00839126]\n [0.9844267  0.00830385 0.00726948]\n [0.97566867 0.01399002 0.01034127]\n [0.0145969  0.0112832  0.97411991]\n [0.00857077 0.00785226 0.98357698]\n [0.01479496 0.97251366 0.01269135]\n [0.00771437 0.00993916 0.9823465 ]\n [0.00989786 0.00810951 0.98199261]\n [0.00815207 0.00909871 0.98274924]\n [0.9822199  0.01016214 0.00761794]\n [0.02204996 0.95634987 0.02160015]\n [0.00889976 0.01587016 0.97523009]\n [0.94979683 0.02723708 0.02296609]\n [0.00671002 0.00794476 0.98534522]\n [0.01352367 0.00847951 0.9779968 ]\n [0.00629172 0.00642318 0.98728511]\n [0.97922242 0.01165381 0.00912378]\n [0.00951757 0.01330139 0.97718103]\n [0.00741454 0.00751335 0.98507213]\n [0.02017441 0.9592599  0.0205657 ]\n [0.01092969 0.97738671 0.0116836 ]\n [0.98332132 0.00964151 0.00703719]\n [0.00997994 0.01319133 0.97682873]\n [0.98002999 0.00864444 0.01132554]\n [0.00780108 0.00717055 0.98502839]\n [0.01085758 0.9746706  0.01447182]\n [0.00874629 0.00979057 0.98146313]\n [0.01285513 0.00726655 0.97987832]\n [0.0118856  0.01054366 0.97757073]\n [0.00949903 0.01138503 0.97911596]\n [0.97159258 0.01881201 0.00959537]\n [0.00715247 0.00712234 0.98572518]\n [0.98402835 0.00700618 0.00896545]\n [0.96823777 0.01628213 0.01548009]\n [0.97900498 0.01196332 0.00903171]\n [0.95382363 0.02179573 0.02438067]\n [0.00866541 0.0112449  0.98008968]\n [0.00598271 0.00627553 0.98774179]\n [0.01471936 0.96417061 0.02111004]\n [0.01174276 0.00765265 0.98060458]\n [0.00967075 0.01260319 0.97772606]\n [0.96406713 0.02065221 0.01528068]\n [0.9703315  0.01982959 0.00983887]\n [0.98103217 0.00961981 0.009348  ]\n [0.97929549 0.01234289 0.00836163]\n [0.00700412 0.00754193 0.98545397]\n [0.00891109 0.97778829 0.01330061]\n [0.00719896 0.00892148 0.98387958]\n [0.00775826 0.00684735 0.9853944 ]\n [0.0110584  0.01387348 0.9750681 ]\n [0.97592259 0.01430001 0.00977738]\n [0.02108602 0.96579307 0.01312089]\n [0.01229476 0.97411131 0.01359392]\n [0.98135594 0.0089953  0.00964875]\n [0.01208749 0.97853949 0.00937301]\n [0.98140606 0.00964701 0.00894696]\n [0.9842184  0.00825437 0.00752725]\n [0.98065267 0.01131047 0.00803683]\n [0.01184278 0.97823164 0.00992555]\n [0.97857083 0.01088089 0.01054829]\n [0.97635415 0.01510382 0.00854202]\n [0.95182706 0.02236372 0.0258092 ]\n [0.02166972 0.0161407  0.9621896 ]\n [0.00948246 0.97677943 0.01373809]\n [0.01675317 0.96763443 0.0156124 ]]",
      "confidence_scores": "[0.96038815 0.98567692 0.95739224 0.96369966 0.98308984 0.97009003\n 0.98102005 0.97076129 0.98403037 0.97125017 0.9733366  0.96016549\n 0.97373128 0.98552352 0.97769259 0.9806358  0.98172879 0.97362402\n 0.98191517 0.97040406 0.92098063 0.9855029  0.97998409 0.98036427\n 0.98348993 0.97744697 0.97602008 0.98415216 0.97380613 0.98615287\n 0.97450141 0.97766681 0.97429283 0.97011365 0.98269012 0.98508497\n 0.98038281 0.97951944 0.97294762 0.97936715 0.98316752 0.98007033\n 0.98280345 0.98383378 0.96541983 0.97937489 0.97841847 0.97745908\n 0.95145463 0.98474081 0.968541   0.9843736  0.97879906 0.9866375\n 0.96814223 0.98244812 0.9743761  0.97891069 0.97406406 0.97863314\n 0.97753442 0.98039019 0.98144126 0.98668499 0.98377143 0.97161308\n 0.96936599 0.96691866 0.97520153 0.9636637  0.95463114 0.98402263\n 0.97746187 0.97583876 0.96957423 0.9802483  0.96994624 0.97597584\n 0.98069501 0.98341238 0.98403643 0.98572066 0.94030638 0.98356789\n 0.96791421 0.98391976 0.97120331 0.98209657 0.97420783 0.97587903\n 0.9844267  0.97566867 0.97411991 0.98357698 0.97251366 0.9823465\n 0.98199261 0.98274924 0.9822199  0.95634987 0.97523009 0.94979683\n 0.98534522 0.9779968  0.98728511 0.97922242 0.97718103 0.98507213\n 0.9592599  0.97738671 0.98332132 0.97682873 0.98002999 0.98502839\n 0.9746706  0.98146313 0.97987832 0.97757073 0.97911596 0.97159258\n 0.98572518 0.98402835 0.96823777 0.97900498 0.95382363 0.98008968\n 0.98774179 0.96417061 0.98060458 0.97772606 0.96406713 0.9703315\n 0.98103217 0.97929549 0.98545397 0.97778829 0.98387958 0.9853944\n 0.9750681  0.97592259 0.96579307 0.97411131 0.98135594 0.97853949\n 0.98140606 0.9842184  0.98065267 0.97823164 0.97857083 0.97635415\n 0.95182706 0.9621896  0.97677943 0.96763443]"
    },
    "predictions": {
      "predictions": "[0 2 2 0 2 2 2 1 2 1 1 1 2 2 0 2 0 2 2 0 1 2 2 0 2 2 0 2 0 2 2 1 2 1 2 2 1\n 2 2 2 0 0 1 2 0 2 0 1 0 2 2 2 1 2 1 2 1 0 2 0 0 0 0 2 2 2 2 1 2 2 0 0 1 0\n 1 2 1 2 0 0 2 2 1 0 0 2 2 0 1 0 0 0 2 2 1 2 2 2 0 1 2 0 2 2 2 0 2 2 1 1 0\n 2 0 2 1 2 2 2 2 0 2 0 0 0 0 2 2 1 2 2 0 0 0 0 2 1 2 2 2 0 1 1 0 1 0 0 0 1\n 0 0 0 2 1 1]",
      "probabilities": "[[0.96038815 0.02030811 0.01930375]\n [0.00651443 0.00780865 0.98567692]\n [0.02315563 0.01945212 0.95739224]\n [0.96369966 0.02034735 0.015953  ]\n [0.00870771 0.00820246 0.98308984]\n [0.00939555 0.02051443 0.97009003]\n [0.00771039 0.01126958 0.98102005]\n [0.0126045  0.97076129 0.01663419]\n [0.00823913 0.00773049 0.98403037]\n [0.01574732 0.97125017 0.01300251]\n [0.01212544 0.9733366  0.01453795]\n [0.01644963 0.96016549 0.02338488]\n [0.00997736 0.01629136 0.97373128]\n [0.00796719 0.00650931 0.98552352]\n [0.97769259 0.0139895  0.00831789]\n [0.01294718 0.00641703 0.9806358 ]\n [0.98172879 0.01005027 0.00822094]\n [0.01185168 0.01452432 0.97362402]\n [0.0083861  0.00969872 0.98191517]\n [0.97040406 0.01897771 0.01061825]\n [0.03497017 0.92098063 0.04404921]\n [0.00694937 0.00754776 0.9855029 ]\n [0.00928627 0.01072965 0.97998409]\n [0.98036427 0.00935076 0.01028498]\n [0.00951121 0.00699885 0.98348993]\n [0.00840167 0.01415136 0.97744697]\n [0.97602008 0.01503181 0.00894812]\n [0.00696015 0.00888772 0.98415216]\n [0.97380613 0.01789113 0.00830276]\n [0.00610877 0.00773835 0.98615287]\n [0.01258772 0.01291087 0.97450141]\n [0.01156608 0.97766681 0.01076711]\n [0.00986588 0.01584128 0.97429283]\n [0.014688   0.97011365 0.01519834]\n [0.00995248 0.00735742 0.98269012]\n [0.00804918 0.00686584 0.98508497]\n [0.01095337 0.98038281 0.00866382]\n [0.01244955 0.00803103 0.97951944]\n [0.01185651 0.01519586 0.97294762]\n [0.00959082 0.01104204 0.97936715]\n [0.98316752 0.00985778 0.00697471]\n [0.98007033 0.01123426 0.0086954 ]\n [0.0082803  0.98280345 0.00891624]\n [0.00706341 0.0091028  0.98383378]\n [0.96541983 0.02074721 0.01383297]\n [0.0089791  0.011646   0.97937489]\n [0.97841847 0.01412405 0.0074575 ]\n [0.01249449 0.97745908 0.01004643]\n [0.95145463 0.02708093 0.02146445]\n [0.00820507 0.00705414 0.98474081]\n [0.01325963 0.01819936 0.968541  ]\n [0.00800616 0.00762024 0.9843736 ]\n [0.00877927 0.97879906 0.01242167]\n [0.00609769 0.0072648  0.9866375 ]\n [0.016117   0.96814223 0.01574076]\n [0.00896779 0.00858411 0.98244812]\n [0.01477202 0.9743761  0.0108519 ]\n [0.97891069 0.01037594 0.01071336]\n [0.01397901 0.01195694 0.97406406]\n [0.97863314 0.00981238 0.01155449]\n [0.97753442 0.01235244 0.01011314]\n [0.98039019 0.01090957 0.00870026]\n [0.98144126 0.00842132 0.01013744]\n [0.00681729 0.00649771 0.98668499]\n [0.00826021 0.00796837 0.98377143]\n [0.01322862 0.01515829 0.97161308]\n [0.00985395 0.02078005 0.96936599]\n [0.01791242 0.96691866 0.01516889]\n [0.0082731  0.01652537 0.97520153]\n [0.0117465  0.0245898  0.9636637 ]\n [0.95463114 0.02701928 0.01834958]\n [0.98402263 0.00867234 0.00730505]\n [0.01037871 0.97746187 0.0121594 ]\n [0.97583876 0.01555465 0.00860655]\n [0.01278456 0.96957423 0.01764121]\n [0.00784253 0.01190919 0.9802483 ]\n [0.01730386 0.96994624 0.0127499 ]\n [0.01401061 0.01001356 0.97597584]\n [0.98069501 0.01080642 0.00849857]\n [0.98341238 0.0085593  0.00802833]\n [0.00864839 0.0073152  0.98403643]\n [0.00754652 0.00673281 0.98572066]\n [0.02242355 0.94030638 0.03727004]\n [0.98356789 0.00883616 0.00759597]\n [0.96791421 0.01789938 0.01418642]\n [0.00728376 0.0087965  0.98391976]\n [0.01446334 0.01433335 0.97120331]\n [0.98209657 0.01046083 0.00744263]\n [0.01048578 0.97420783 0.01530636]\n [0.97587903 0.01572969 0.00839126]\n [0.9844267  0.00830385 0.00726948]\n [0.97566867 0.01399002 0.01034127]\n [0.0145969  0.0112832  0.97411991]\n [0.00857077 0.00785226 0.98357698]\n [0.01479496 0.97251366 0.01269135]\n [0.00771437 0.00993916 0.9823465 ]\n [0.00989786 0.00810951 0.98199261]\n [0.00815207 0.00909871 0.98274924]\n [0.9822199  0.01016214 0.00761794]\n [0.02204996 0.95634987 0.02160015]\n [0.00889976 0.01587016 0.97523009]\n [0.94979683 0.02723708 0.02296609]\n [0.00671002 0.00794476 0.98534522]\n [0.01352367 0.00847951 0.9779968 ]\n [0.00629172 0.00642318 0.98728511]\n [0.97922242 0.01165381 0.00912378]\n [0.00951757 0.01330139 0.97718103]\n [0.00741454 0.00751335 0.98507213]\n [0.02017441 0.9592599  0.0205657 ]\n [0.01092969 0.97738671 0.0116836 ]\n [0.98332132 0.00964151 0.00703719]\n [0.00997994 0.01319133 0.97682873]\n [0.98002999 0.00864444 0.01132554]\n [0.00780108 0.00717055 0.98502839]\n [0.01085758 0.9746706  0.01447182]\n [0.00874629 0.00979057 0.98146313]\n [0.01285513 0.00726655 0.97987832]\n [0.0118856  0.01054366 0.97757073]\n [0.00949903 0.01138503 0.97911596]\n [0.97159258 0.01881201 0.00959537]\n [0.00715247 0.00712234 0.98572518]\n [0.98402835 0.00700618 0.00896545]\n [0.96823777 0.01628213 0.01548009]\n [0.97900498 0.01196332 0.00903171]\n [0.95382363 0.02179573 0.02438067]\n [0.00866541 0.0112449  0.98008968]\n [0.00598271 0.00627553 0.98774179]\n [0.01471936 0.96417061 0.02111004]\n [0.01174276 0.00765265 0.98060458]\n [0.00967075 0.01260319 0.97772606]\n [0.96406713 0.02065221 0.01528068]\n [0.9703315  0.01982959 0.00983887]\n [0.98103217 0.00961981 0.009348  ]\n [0.97929549 0.01234289 0.00836163]\n [0.00700412 0.00754193 0.98545397]\n [0.00891109 0.97778829 0.01330061]\n [0.00719896 0.00892148 0.98387958]\n [0.00775826 0.00684735 0.9853944 ]\n [0.0110584  0.01387348 0.9750681 ]\n [0.97592259 0.01430001 0.00977738]\n [0.02108602 0.96579307 0.01312089]\n [0.01229476 0.97411131 0.01359392]\n [0.98135594 0.0089953  0.00964875]\n [0.01208749 0.97853949 0.00937301]\n [0.98140606 0.00964701 0.00894696]\n [0.9842184  0.00825437 0.00752725]\n [0.98065267 0.01131047 0.00803683]\n [0.01184278 0.97823164 0.00992555]\n [0.97857083 0.01088089 0.01054829]\n [0.97635415 0.01510382 0.00854202]\n [0.95182706 0.02236372 0.0258092 ]\n [0.02166972 0.0161407  0.9621896 ]\n [0.00948246 0.97677943 0.01373809]\n [0.01675317 0.96763443 0.0156124 ]]",
      "confidence_scores": "[0.96038815 0.98567692 0.95739224 0.96369966 0.98308984 0.97009003\n 0.98102005 0.97076129 0.98403037 0.97125017 0.9733366  0.96016549\n 0.97373128 0.98552352 0.97769259 0.9806358  0.98172879 0.97362402\n 0.98191517 0.97040406 0.92098063 0.9855029  0.97998409 0.98036427\n 0.98348993 0.97744697 0.97602008 0.98415216 0.97380613 0.98615287\n 0.97450141 0.97766681 0.97429283 0.97011365 0.98269012 0.98508497\n 0.98038281 0.97951944 0.97294762 0.97936715 0.98316752 0.98007033\n 0.98280345 0.98383378 0.96541983 0.97937489 0.97841847 0.97745908\n 0.95145463 0.98474081 0.968541   0.9843736  0.97879906 0.9866375\n 0.96814223 0.98244812 0.9743761  0.97891069 0.97406406 0.97863314\n 0.97753442 0.98039019 0.98144126 0.98668499 0.98377143 0.97161308\n 0.96936599 0.96691866 0.97520153 0.9636637  0.95463114 0.98402263\n 0.97746187 0.97583876 0.96957423 0.9802483  0.96994624 0.97597584\n 0.98069501 0.98341238 0.98403643 0.98572066 0.94030638 0.98356789\n 0.96791421 0.98391976 0.97120331 0.98209657 0.97420783 0.97587903\n 0.9844267  0.97566867 0.97411991 0.98357698 0.97251366 0.9823465\n 0.98199261 0.98274924 0.9822199  0.95634987 0.97523009 0.94979683\n 0.98534522 0.9779968  0.98728511 0.97922242 0.97718103 0.98507213\n 0.9592599  0.97738671 0.98332132 0.97682873 0.98002999 0.98502839\n 0.9746706  0.98146313 0.97987832 0.97757073 0.97911596 0.97159258\n 0.98572518 0.98402835 0.96823777 0.97900498 0.95382363 0.98008968\n 0.98774179 0.96417061 0.98060458 0.97772606 0.96406713 0.9703315\n 0.98103217 0.97929549 0.98545397 0.97778829 0.98387958 0.9853944\n 0.9750681  0.97592259 0.96579307 0.97411131 0.98135594 0.97853949\n 0.98140606 0.9842184  0.98065267 0.97823164 0.97857083 0.97635415\n 0.95182706 0.9621896  0.97677943 0.96763443]",
      "high_confidence_mask": "[ True  True  True  True  True  True  True  True  True  True  True  True\n  True  True  True  True  True  True  True  True  True  True  True  True\n  True  True  True  True  True  True  True  True  True  True  True  True\n  True  True  True  True  True  True  True  True  True  True  True  True\n  True  True  True  True  True  True  True  True  True  True  True  True\n  True  True  True  True  True  True  True  True  True  True  True  True\n  True  True  True  True  True  True  True  True  True  True  True  True\n  True  True  True  True  True  True  True  True  True  True  True  True\n  True  True  True  True  True  True  True  True  True  True  True  True\n  True  True  True  True  True  True  True  True  True  True  True  True\n  True  True  True  True  True  True  True  True  True  True  True  True\n  True  True  True  True  True  True  True  True  True  True  True  True\n  True  True  True  True  True  True  True  True  True  True]",
      "low_confidence_mask": "[False False False False False False False False False False False False\n False False False False False False False False False False False False\n False False False False False False False False False False False False\n False False False False False False False False False False False False\n False False False False False False False False False False False False\n False False False False False False False False False False False False\n False False False False False False False False False False False False\n False False False False False False False False False False False False\n False False False False False False False False False False False False\n False False False False False False False False False False False False\n False False False False False False False False False False False False\n False False False False False False False False False False False False\n False False False False False False False False False False]",
      "high_confidence_count": "154",
      "low_confidence_count": "0",
      "confidence_threshold": 0.6
    },
    "feature_importance": {
      "xgboost": "                 feature  importance\n120       result_encoded    0.363995\n98              home_win    0.124899\n99                  draw    0.113327\n100             away_win    0.104969\n104       home_advantage    0.098458\n..                   ...         ...\n236   team_strength_diff    0.000000\n237   home_team_win_rate    0.000000\n238   away_team_win_rate    0.000000\n239  home_team_avg_goals    0.000000\n240  away_team_avg_goals    0.000000\n\n[241 rows x 2 columns]",
      "lightgbm": "                 feature  importance\n100             away_win         127\n98              home_win         125\n99                  draw         117\n101      goal_difference          42\n232        match_balance          27\n..                   ...         ...\n236   team_strength_diff           0\n237   home_team_win_rate           0\n238   away_team_win_rate           0\n239  home_team_avg_goals           0\n240  away_team_avg_goals           0\n\n[241 rows x 2 columns]",
      "random_forest": "                      feature  importance\n100                  away_win    0.110646\n120            result_encoded    0.100103\n99                       draw    0.092436\n104            home_advantage    0.087865\n101           goal_difference    0.087605\n..                        ...         ...\n215      away_position_change    0.000000\n211   away_clean_sheet_streak    0.000000\n210   home_clean_sheet_streak    0.000000\n214      home_position_change    0.000000\n225  transfer_learning_factor    0.000000\n\n[241 rows x 2 columns]",
      "ensemble": "                      feature  importance\n100                  away_win   44.883344\n98                   home_win   44.176342\n99                       draw   41.354470\n101           goal_difference   14.874905\n232             match_balance    9.579641\n..                        ...         ...\n215      away_position_change    0.000000\n211   away_clean_sheet_streak    0.000000\n210   home_clean_sheet_streak    0.000000\n214      home_position_change    0.000000\n225  transfer_learning_factor    0.000000\n\n[241 rows x 2 columns]"
    },
    "uncertainty_estimates": {
      "mean_prediction": "[[0.3967778  0.16554402 0.43767818]\n [0.35860468 0.16430054 0.47709478]\n [0.39462445 0.16651452 0.43886104]\n [0.30121162 0.26156623 0.43722215]\n [0.31842962 0.26245876 0.41911162]\n [0.32111961 0.20299904 0.47588134]\n [0.35797268 0.24178559 0.40024172]\n [0.33938199 0.18412442 0.47649359]\n [0.35751474 0.10680298 0.53568228]\n [0.32106318 0.18413351 0.4948033 ]\n [0.37692368 0.20395545 0.41912088]\n [0.22413434 0.26076153 0.51510414]\n [0.29909086 0.22345552 0.47745362]\n [0.35955268 0.22295987 0.41748746]\n [0.26202062 0.28008713 0.45789225]\n [0.39677667 0.12625092 0.47697241]\n [0.31944712 0.14668251 0.53387037]\n [0.3197769  0.20380009 0.47642301]\n [0.32086636 0.18296902 0.49616463]\n [0.24304412 0.18430246 0.57265343]\n [0.29992319 0.22337106 0.47670575]\n [0.32012023 0.2605059  0.41937386]\n [0.31932515 0.22279741 0.45787744]\n [0.3776635  0.18315981 0.43917669]\n [0.22247555 0.25985301 0.51767144]\n [0.3598854  0.27861688 0.36149773]\n [0.28240683 0.29838709 0.41920607]\n [0.30121712 0.24024608 0.45853679]\n [0.28119989 0.30045452 0.41834558]\n [0.26195808 0.2983395  0.43970242]\n [0.33888675 0.18399609 0.47711717]\n [0.24353573 0.27832416 0.4781401 ]\n [0.35647659 0.1076177  0.53590571]\n [0.22252023 0.22302698 0.5544528 ]\n [0.22523956 0.29788577 0.47687468]\n [0.20478539 0.29855553 0.49665907]\n [0.35852683 0.14657596 0.49489721]\n [0.3781794  0.18475636 0.43706424]\n [0.31984323 0.28060687 0.3995499 ]\n [0.35862269 0.20318786 0.43818945]\n [0.47298063 0.18615265 0.34086672]\n [0.3589471  0.14552542 0.49552748]\n [0.3013574  0.20241303 0.49622957]\n [0.39720604 0.16612841 0.43666556]\n [0.35751344 0.2039532  0.43853337]\n [0.37706677 0.20377831 0.41915492]\n [0.26165282 0.37655666 0.36179052]\n [0.35709492 0.08960611 0.55329898]\n [0.31966095 0.14670815 0.53363089]\n [0.29938468 0.18595763 0.51465768]\n [0.28134349 0.18205835 0.53659816]\n [0.28095828 0.24249904 0.47654268]\n [0.37656212 0.18477884 0.43865904]\n [0.31974119 0.18453565 0.49572315]\n [0.39553168 0.22385913 0.38060918]\n [0.39608485 0.20504105 0.3988741 ]\n [0.37603881 0.10719801 0.51676319]\n [0.3584853  0.18405479 0.45745991]\n [0.3380678  0.20387464 0.45805756]\n [0.41508261 0.30147649 0.2834409 ]\n [0.30009244 0.30049086 0.3994167 ]\n [0.35868198 0.22315863 0.4181594 ]\n [0.32024412 0.2036337  0.47612218]\n [0.41571252 0.14567259 0.43861488]\n [0.30136797 0.29888592 0.39974611]\n [0.30076712 0.20425355 0.49497933]\n [0.39878714 0.25965692 0.34155595]\n [0.41682441 0.24224229 0.3409333 ]\n [0.37671923 0.20480937 0.41847141]\n [0.35850458 0.10757739 0.53391803]\n [0.33960758 0.20215483 0.4582376 ]\n [0.33799451 0.28035215 0.38165334]\n [0.35872402 0.16514029 0.47613569]\n [0.47307888 0.14657038 0.38035073]\n [0.49296603 0.18516486 0.32186911]\n [0.2819533  0.27906551 0.43898119]\n [0.41567887 0.24163837 0.34268275]\n [0.41623402 0.18516882 0.39859716]\n [0.26147267 0.10709532 0.63143201]\n [0.33906386 0.24002287 0.42091327]\n [0.27975958 0.20327675 0.51696366]\n [0.18449563 0.26106896 0.55443541]\n [0.26033967 0.28224637 0.45741396]\n [0.32018499 0.24141231 0.4384027 ]\n [0.24349567 0.260748   0.49575634]\n [0.33957825 0.16433131 0.49609045]\n [0.35753951 0.16537751 0.47708298]\n [0.20423455 0.29885249 0.49691297]\n [0.31945845 0.16618875 0.5143528 ]\n [0.37749279 0.1267827  0.49572452]\n [0.37684872 0.20451387 0.41863741]\n [0.28204987 0.31851686 0.39943327]\n [0.33988737 0.20411281 0.45599982]\n [0.35942714 0.20359158 0.43698127]\n [0.39702215 0.12691478 0.47606307]\n [0.31994322 0.29999106 0.38006572]\n [0.32054069 0.25908613 0.42037318]\n [0.28090491 0.24234662 0.47674847]\n [0.37765917 0.20362034 0.41872049]\n [0.34016486 0.16451882 0.49531632]\n [0.35902094 0.22270247 0.41827658]\n [0.24131551 0.26296484 0.49571965]\n [0.31899094 0.24354983 0.43745923]\n [0.34021565 0.20247975 0.45730461]\n [0.22366694 0.28062263 0.49571043]\n [0.32052544 0.33799024 0.34148432]\n [0.319467   0.1836362  0.4968968 ]\n [0.33941441 0.31831536 0.34227023]\n [0.45469105 0.1452101  0.40009885]\n [0.43461359 0.20442768 0.36095874]\n [0.37743551 0.16567939 0.4568851 ]\n [0.31986826 0.20321962 0.47691213]\n [0.30018745 0.18412409 0.51568846]\n [0.2618287  0.22263672 0.51553458]\n [0.35677551 0.32074195 0.32248255]\n [0.35867595 0.20445554 0.43686852]\n [0.39779694 0.22232304 0.37988003]\n [0.35917377 0.22135055 0.41947568]\n [0.29922247 0.2607009  0.44007663]\n [0.4161915  0.20413535 0.37967316]\n [0.28093714 0.22153773 0.49752513]\n [0.39777857 0.20334728 0.39887415]\n [0.41573663 0.14657397 0.43768939]\n [0.29999352 0.14623703 0.55376945]\n [0.31822506 0.10784285 0.57393209]\n [0.31969674 0.22316863 0.45713463]\n [0.41705385 0.14559938 0.43734677]\n [0.22478208 0.29814762 0.4770703 ]\n [0.24288712 0.22246507 0.5346478 ]\n [0.26116541 0.30102608 0.43780851]\n [0.28102851 0.2036713  0.51530019]\n [0.30140163 0.2411363  0.45746207]\n [0.28130178 0.25950045 0.45919776]\n [0.26194513 0.1837117  0.55434317]\n [0.49263428 0.16617576 0.34118995]\n [0.28081997 0.26109016 0.45808987]\n [0.26083587 0.28160131 0.45756281]\n [0.28019596 0.18331298 0.53649107]\n [0.31735646 0.18578348 0.49686007]\n [0.28184942 0.2423214  0.47582918]\n [0.26139136 0.14536195 0.59324669]\n [0.2822512  0.22084385 0.49690496]\n [0.37724023 0.16294841 0.45981135]\n [0.32103251 0.3370856  0.3418819 ]\n [0.30112633 0.20356223 0.49531145]\n [0.16591725 0.29913214 0.53495061]\n [0.3205865  0.2787765  0.400637  ]\n [0.29946402 0.26192439 0.43861158]\n [0.3772589  0.18538888 0.43735222]\n [0.32184637 0.22088528 0.45726835]\n [0.37773769 0.18413207 0.43813024]\n [0.31972968 0.12621292 0.5540574 ]\n [0.35744036 0.16533282 0.47722682]\n [0.3770014  0.16677246 0.45622614]]",
      "prediction_variance": "[[0.2236373  0.12353617 0.23060885]\n [0.21394999 0.12200323 0.23288564]\n [0.22200891 0.1246825  0.23131657]\n [0.19510298 0.17653505 0.22971029]\n [0.19992034 0.17629638 0.22660236]\n [0.2026429  0.1459695  0.23225403]\n [0.21288723 0.16658525 0.22441843]\n [0.2083664  0.13547762 0.2331847 ]\n [0.21454454 0.08145128 0.23319369]\n [0.20432435 0.13685437 0.23482901]\n [0.21890564 0.14807995 0.22913995]\n [0.16136449 0.17852994 0.23424734]\n [0.19450803 0.15822043 0.2338603 ]\n [0.21478384 0.15748244 0.2273951 ]\n [0.17807071 0.18474336 0.23140232]\n [0.22341912 0.09619903 0.23362144]\n [0.20262309 0.11070004 0.23218601]\n [0.20284909 0.14786281 0.2335671 ]\n [0.2045014  0.13584079 0.23451461]\n [0.1708564  0.13705504 0.22823748]\n [0.1940298  0.15746751 0.23297759]\n [0.20334236 0.17721108 0.22895884]\n [0.202006   0.15790938 0.23222204]\n [0.21918695 0.13416153 0.23110367]\n [0.15801335 0.17555496 0.23246188]\n [0.2140791  0.18334874 0.21474384]\n [0.18755353 0.19168628 0.22677427]\n [0.19537859 0.16611059 0.23208281]\n [0.18633684 0.19240384 0.22593212]\n [0.17834158 0.19202827 0.22996833]\n [0.2088115  0.13544921 0.23376555]\n [0.16983358 0.18458195 0.23339763]\n [0.21431579 0.08302108 0.23375478]\n [0.15914259 0.15850656 0.23058685]\n [0.1602739  0.19220819 0.2324845 ]\n [0.14803328 0.19150042 0.23217112]\n [0.214802   0.11157395 0.23400748]\n [0.21946362 0.13596344 0.23057595]\n [0.2024064  0.18482822 0.22412502]\n [0.21431801 0.14660484 0.23059943]\n [0.23139211 0.13665495 0.20996242]\n [0.21397301 0.10909426 0.23190477]\n [0.19669302 0.1472471  0.23456866]\n [0.223387   0.1246117  0.23055681]\n [0.21273515 0.14559726 0.22927712]\n [0.21800839 0.14613973 0.2271168 ]\n [0.17687247 0.21671273 0.21559604]\n [0.21431367 0.06809545 0.23003847]\n [0.20284741 0.11166885 0.23237479]\n [0.1952896  0.13676841 0.23367783]\n [0.18753433 0.1332212  0.23192575]\n [0.18728571 0.1685104  0.23350133]\n [0.21702285 0.13427624 0.22914933]\n [0.20169792 0.13559133 0.23302939]\n [0.2224388  0.15839049 0.2205513 ]\n [0.22043735 0.14566507 0.2225041 ]\n [0.21835344 0.08249949 0.23421511]\n [0.21503149 0.13537275 0.23259672]\n [0.20823454 0.14717623 0.23283881]\n [0.22450023 0.19339605 0.1886702 ]\n [0.19471981 0.1933486  0.22447784]\n [0.213144   0.15698961 0.22680381]\n [0.2026249  0.14608088 0.23256298]\n [0.22671711 0.10990873 0.23108487]\n [0.19491247 0.19173951 0.22347414]\n [0.19640129 0.14771724 0.23458473]\n [0.22300365 0.17486661 0.21002761]\n [0.2266078  0.16864643 0.21020437]\n [0.21858538 0.14786934 0.22847263]\n [0.21390835 0.08153218 0.23133167]\n [0.20931572 0.14539387 0.2321018 ]\n [0.2068477  0.18325812 0.21958078]\n [0.21571989 0.12462876 0.23437812]\n [0.23134463 0.1095918  0.22066744]\n [0.23293809 0.13588105 0.20455003]\n [0.18582113 0.18217452 0.22920351]\n [0.22535486 0.16654064 0.21098311]\n [0.22729204 0.13607241 0.2249837 ]\n [0.18062332 0.08282372 0.21685314]\n [0.20774403 0.16476431 0.22695047]\n [0.18613173 0.14668763 0.23380227]\n [0.13646708 0.17787235 0.23093506]\n [0.17672467 0.18498213 0.23073945]\n [0.20355806 0.16858828 0.23220866]\n [0.17111692 0.1774641  0.23387171]\n [0.20766487 0.12144774 0.23273956]\n [0.21333993 0.12264301 0.23339241]\n [0.14941772 0.19329597 0.23435085]\n [0.20298458 0.12414138 0.23389673]\n [0.21943109 0.09727646 0.23422846]\n [0.21915477 0.14800456 0.22840057]\n [0.18726089 0.19930345 0.22398592]\n [0.20990489 0.14785099 0.23259065]\n [0.21479254 0.14702531 0.2302196 ]\n [0.22396578 0.09682999 0.23411854]\n [0.2019425  0.19369657 0.22094659]\n [0.20096631 0.17284326 0.22612594]\n [0.18700697 0.1681473  0.23301598]\n [0.21892639 0.14749866 0.22832188]\n [0.20966916 0.12342553 0.23386184]\n [0.21492474 0.15800779 0.22787866]\n [0.16869171 0.17824888 0.23407007]\n [0.20255659 0.16954502 0.23112676]\n [0.20850089 0.14601987 0.23177191]\n [0.15854624 0.18399466 0.23107004]\n [0.20219011 0.20643593 0.20963215]\n [0.20317836 0.13558183 0.23438496]\n [0.20812184 0.1988584  0.2095813 ]\n [0.22970748 0.10738081 0.22376463]\n [0.22871849 0.14677024 0.21638464]\n [0.22006162 0.1241506  0.23279941]\n [0.20254484 0.14621688 0.23336023]\n [0.19474639 0.13512191 0.23309897]\n [0.17914677 0.15704999 0.23252326]\n [0.21275393 0.20118529 0.20376572]\n [0.21481418 0.14780027 0.23058039]\n [0.22364432 0.1577729  0.22095478]\n [0.21348359 0.15518145 0.22680874]\n [0.19368634 0.17671484 0.2304076 ]\n [0.22631065 0.14616016 0.21961258]\n [0.1858274  0.15535935 0.23273495]\n [0.22505709 0.14764083 0.22549282]\n [0.22715212 0.111121   0.23074684]\n [0.19526931 0.11108484 0.23120531]\n [0.20131148 0.08288895 0.22767622]\n [0.20207869 0.15726334 0.23196408]\n [0.22683625 0.10991888 0.2300072 ]\n [0.15979873 0.19175221 0.2323062 ]\n [0.16969203 0.15800378 0.23188718]\n [0.17821401 0.19402013 0.22972471]\n [0.18742989 0.14659348 0.23286522]\n [0.19582587 0.16685713 0.23203773]\n [0.18588968 0.17254598 0.23025342]\n [0.17842557 0.13470312 0.23002598]\n [0.23218252 0.1235332  0.20974363]\n [0.18733622 0.17684597 0.23257303]\n [0.17776603 0.18694529 0.23203378]\n [0.18793132 0.13523755 0.23304054]\n [0.20096806 0.13622746 0.23390255]\n [0.18842737 0.16891621 0.23413783]\n [0.17930008 0.11027082 0.22498356]\n [0.18853884 0.15625111 0.23335005]\n [0.21849463 0.12016786 0.23246113]\n [0.2035544  0.20639918 0.20983574]\n [0.19598137 0.14720922 0.23392632]\n [0.12405753 0.1931948  0.23085686]\n [0.20314604 0.18487183 0.22497154]\n [0.19569508 0.17858295 0.23140071]\n [0.21952417 0.13646403 0.23058498]\n [0.20439271 0.15707076 0.23216502]\n [0.21785082 0.13402741 0.22944073]\n [0.2033624  0.09615313 0.23098529]\n [0.21542683 0.12511042 0.23533459]\n [0.21828465 0.12225446 0.2305999 ]]",
      "confidence_intervals": {
        "lower": "[[0.00706239 0.00724768 0.00735505]\n [0.00727716 0.00648808 0.00777543]\n [0.00684701 0.00701129 0.00760092]\n [0.0071206  0.00643779 0.00726948]\n [0.00697503 0.00735979 0.0082526 ]\n [0.00632357 0.00704316 0.00731178]\n [0.00712616 0.0065596  0.00733601]\n [0.00738065 0.00658537 0.00761794]\n [0.00687267 0.00727614 0.00809667]\n [0.00629172 0.00641842 0.00713176]\n [0.00629172 0.00641842 0.00709746]\n [0.00684943 0.00643995 0.00708946]\n [0.00610235 0.00632552 0.00809008]\n [0.00661229 0.0066211  0.00760092]\n [0.00727716 0.00675858 0.00726948]\n [0.0067663  0.00685151 0.00744263]\n [0.00731403 0.0069344  0.0074575 ]\n [0.00690317 0.00680511 0.00716292]\n [0.00645201 0.00675858 0.00716786]\n [0.00626819 0.00690923 0.00809008]\n [0.00694937 0.00673281 0.00807826]\n [0.00610877 0.00655271 0.00749359]\n [0.00643969 0.00658049 0.00744597]\n [0.00673415 0.00661047 0.00752725]\n [0.00696169 0.00652278 0.00854395]\n [0.00719896 0.00654959 0.00748207]\n [0.00630033 0.00654794 0.00732748]\n [0.00690317 0.00650032 0.00744597]\n [0.00794103 0.00735342 0.00807167]\n [0.00702461 0.00699885 0.00866382]\n [0.00694937 0.00685151 0.00757632]\n [0.00701746 0.00648808 0.00761794]\n [0.00629172 0.00642318 0.00733935]\n [0.00671002 0.00679267 0.00754278]\n [0.0067663  0.00689577 0.00727748]\n [0.00661471 0.00706436 0.00785326]\n [0.00697005 0.0071544  0.00698877]\n [0.00685933 0.00694167 0.00708946]\n [0.00673415 0.00689388 0.00709746]\n [0.0071215  0.00658657 0.00760092]\n [0.00697005 0.00696695 0.00754271]\n [0.00706704 0.00751978 0.00727748]\n [0.00684701 0.00701129 0.00763999]\n [0.00600858 0.00640419 0.00763999]\n [0.00643969 0.00688309 0.00732748]\n [0.00623546 0.00648808 0.00703719]\n [0.0070375  0.00654959 0.00823935]\n [0.00680957 0.00680511 0.00714746]\n [0.00643969 0.00658537 0.00760092]\n [0.00690317 0.00655061 0.00768333]\n [0.00610018 0.00641703 0.00709746]\n [0.00700412 0.00676274 0.00721369]\n [0.00695167 0.00650032 0.00824275]\n [0.0067663  0.0070005  0.00831789]\n [0.00610877 0.00658537 0.00737545]\n [0.00651443 0.00723437 0.00761775]\n [0.00684701 0.0065596  0.00835854]\n [0.00661471 0.00643779 0.00712841]\n [0.00698338 0.00680511 0.00803024]\n [0.00620263 0.00630737 0.00822094]\n [0.00674665 0.0065596  0.00817791]\n [0.0074463  0.00700618 0.00771027]\n [0.00708345 0.00715479 0.0074697 ]\n [0.00687267 0.00655061 0.00761794]\n [0.00629791 0.00706436 0.00760092]\n [0.00620005 0.00685151 0.00727748]\n [0.0070375  0.00701129 0.00744597]\n [0.008103   0.00699885 0.00754271]\n [0.00687267 0.00690821 0.00746167]\n [0.00677619 0.00705908 0.00746167]\n [0.00619145 0.00688309 0.00758593]\n [0.00678953 0.0073152  0.0084154 ]\n [0.00702461 0.00684735 0.00733935]\n [0.00655843 0.00732851 0.00760092]\n [0.00705403 0.00688309 0.00730844]\n [0.00661229 0.00674296 0.00733935]\n [0.00614135 0.00702664 0.00731178]\n [0.00709208 0.00675858 0.00735505]\n [0.00614994 0.00643995 0.00708946]\n [0.00601107 0.00647691 0.00762927]\n [0.00620005 0.00675858 0.00799258]\n [0.00640997 0.00643995 0.00779193]\n [0.00701388 0.00692007 0.00763616]\n [0.00634547 0.00673281 0.00759597]\n [0.00684701 0.00685151 0.00735505]\n [0.00711299 0.00643518 0.00836832]\n [0.0070375  0.00652278 0.00803683]\n [0.00617049 0.00630737 0.00746167]\n [0.0069518  0.00665809 0.00734294]\n [0.00679429 0.00644256 0.00708946]\n [0.00708345 0.00658953 0.00730844]\n [0.00661182 0.00651862 0.00749359]\n [0.00722939 0.0066319  0.00727748]\n [0.00715247 0.00692355 0.00734294]\n [0.00673415 0.00655061 0.00730844]\n [0.00623546 0.00643779 0.00747319]\n [0.00662461 0.00679267 0.00809008]\n [0.00729841 0.00702664 0.0083683 ]\n [0.00709391 0.00701129 0.00771027]\n [0.00661229 0.00680511 0.00698877]\n [0.0071738  0.00735742 0.00747713]\n [0.00643969 0.00685151 0.0083344 ]\n [0.00638584 0.00701697 0.00830617]\n [0.00661229 0.00708033 0.00754271]\n [0.00722958 0.00712104 0.00875576]\n [0.00689271 0.00657638 0.00744597]\n [0.00676387 0.00692355 0.00733601]\n [0.00705403 0.0073152  0.00732748]\n [0.00771437 0.00650931 0.00759597]\n [0.00638584 0.00641842 0.00749359]\n [0.00629172 0.00659134 0.00832773]\n [0.00706704 0.0069344  0.00754271]\n [0.00708345 0.00651801 0.00831789]\n [0.00634183 0.00644256 0.00752725]\n [0.00626819 0.00658054 0.00733935]\n [0.00711299 0.00752109 0.00731178]\n [0.00700412 0.00705869 0.00727748]\n [0.00684943 0.00655061 0.00698877]\n [0.00698338 0.00644256 0.00754765]\n [0.00727716 0.00675858 0.00807826]\n [0.00663133 0.00641842 0.00703719]\n [0.00696169 0.00689741 0.00704904]\n [0.00678953 0.00686584 0.00751113]\n [0.00614135 0.00651862 0.0074575 ]\n [0.00614135 0.00642318 0.00734294]\n [0.00655843 0.0065596  0.00823935]\n [0.00696015 0.00651385 0.00769325]\n [0.00709391 0.0068313  0.00714746]\n [0.00718201 0.00689741 0.00744022]\n [0.00739053 0.00658537 0.00763999]\n [0.00620005 0.00690821 0.00768333]\n [0.00787058 0.00650931 0.00758785]\n [0.00634183 0.0071544  0.00785326]\n [0.00651443 0.00679267 0.00709746]\n [0.00668753 0.0069556  0.00746167]\n [0.00629174 0.00658537 0.00771219]\n [0.00719896 0.00658953 0.00803683]\n [0.00598271 0.00627553 0.00748207]\n [0.00651443 0.00667969 0.00803683]\n [0.00605224 0.00630875 0.00748207]\n [0.00598271 0.00627553 0.00761794]\n [0.00655843 0.00658537 0.00734788]\n [0.00671002 0.00688144 0.00803024]\n [0.00697005 0.00721675 0.00730505]\n [0.00704796 0.00701129 0.00823935]\n [0.00661229 0.00679267 0.0084154 ]\n [0.00629174 0.0068525  0.00727748]\n [0.00624405 0.00689577 0.00748865]\n [0.00772116 0.00719215 0.00735505]\n [0.0070375  0.00650931 0.00731178]\n [0.00619145 0.0071544  0.00749359]\n [0.00609769 0.00649771 0.00758593]\n [0.00676387 0.00690821 0.00803683]\n [0.0077308  0.00727173 0.00813413]]",
        "upper": "[[0.98392031 0.97814347 0.98528377]\n [0.98362145 0.9776207  0.98561335]\n [0.98339189 0.97805844 0.98643223]\n [0.98417799 0.97813189 0.98551888]\n [0.98149619 0.97867139 0.98546742]\n [0.98386927 0.97760379 0.98589986]\n [0.98292656 0.97676541 0.9856763 ]\n [0.9822199  0.9776207  0.98516472]\n [0.98144126 0.96691866 0.98628529]\n [0.98339189 0.97760379 0.98728511]\n [0.98386483 0.97848128 0.98728511]\n [0.9844267  0.97854431 0.98646903]\n [0.98287683 0.97762007 0.98750401]\n [0.98402835 0.97738671 0.9856424 ]\n [0.9844267  0.97813189 0.98564725]\n [0.98209657 0.97746124 0.98549446]\n [0.98143334 0.97828011 0.98507224]\n [0.98332132 0.97823164 0.98643121]\n [0.98328671 0.97847022 0.98643121]\n [0.98296096 0.97804154 0.98652846]\n [0.98166409 0.97828011 0.98572066]\n [0.9821094  0.97996806 0.98652846]\n [0.98304575 0.97760379 0.98693413]\n [0.9842184  0.97743041 0.9866743 ]\n [0.98062642 0.97834314 0.98549189]\n [0.98386927 0.97854431 0.98426245]\n [0.98417564 0.97828011 0.98575253]\n [0.98326459 0.97600599 0.98642366]\n [0.98134656 0.97874066 0.98052441]\n [0.98135594 0.98002647 0.98540597]\n [0.98177169 0.9782964  0.9855029 ]\n [0.9822199  0.98002647 0.98566065]\n [0.98392031 0.97375184 0.98728511]\n [0.98327391 0.97805782 0.98572518]\n [0.98402706 0.97829702 0.98534522]\n [0.98202551 0.9798988  0.98538367]\n [0.98351241 0.97805844 0.98566416]\n [0.98388532 0.97829702 0.98544056]\n [0.98339189 0.98190246 0.98563776]\n [0.98314407 0.97829702 0.98539613]\n [0.98407204 0.97979904 0.98537094]\n [0.98402263 0.97622307 0.98536805]\n [0.98403705 0.97738671 0.98600663]\n [0.98403705 0.9798988  0.98749333]\n [0.98437983 0.97979904 0.98638222]\n [0.98332132 0.97769793 0.98643121]\n [0.98351095 0.97847022 0.98535821]\n [0.9835329  0.97456648 0.9856763 ]\n [0.98295431 0.97874066 0.98638686]\n [0.98402835 0.97813189 0.98646801]\n [0.98386483 0.97395705 0.98728511]\n [0.98277119 0.97834314 0.98545397]\n [0.98139478 0.9733366  0.9856763 ]\n [0.98320533 0.97744496 0.98538333]\n [0.98383023 0.97829702 0.98615287]\n [0.98304575 0.97630494 0.98571432]\n [0.98344182 0.97650735 0.98572416]\n [0.98328671 0.9733366  0.9856424 ]\n [0.98284381 0.97867139 0.9855649 ]\n [0.98172879 0.98225881 0.98722703]\n [0.98059361 0.98038281 0.98571082]\n [0.98388976 0.97744496 0.98492491]\n [0.98326439 0.97746715 0.98554084]\n [0.9822199  0.97725007 0.98646903]\n [0.98392474 0.9782964  0.98601126]\n [0.98417435 0.98002647 0.98604578]\n [0.98311632 0.9743761  0.98550787]\n [0.98392474 0.97776095 0.98483909]\n [0.98292656 0.97972497 0.98554373]\n [0.98358926 0.97676757 0.9854295 ]\n [0.98339189 0.97677559 0.98643121]\n [0.98062059 0.97420783 0.98528377]\n [0.98402706 0.97834464 0.9853944 ]\n [0.98326459 0.97718368 0.98562675]\n [0.98433707 0.97730832 0.98551888]\n [0.98261121 0.97762007 0.9856424 ]\n [0.98401656 0.97854431 0.9866375 ]\n [0.98392031 0.97746715 0.98564725]\n [0.98356789 0.973901   0.98715009]\n [0.98165617 0.97847022 0.98738428]\n [0.98335122 0.97664975 0.98605664]\n [0.9818768  0.97847022 0.98668499]\n [0.98164066 0.97867139 0.98532189]\n [0.9835329  0.97829702 0.98643121]\n [0.98388532 0.97805782 0.98605664]\n [0.9828529  0.96814223 0.98572066]\n [0.9816449  0.97756129 0.98572518]\n [0.98209657 0.97823164 0.98750401]\n [0.98423346 0.97996806 0.98551888]\n [0.98401656 0.97678666 0.9856424 ]\n [0.98390242 0.97776095 0.9856798 ]\n [0.98202551 0.97867139 0.98598221]\n [0.98417435 0.97762007 0.98535821]\n [0.9835329  0.97745908 0.98571432]\n [0.98292656 0.97197366 0.98571082]\n [0.98388976 0.97867139 0.98638686]\n [0.98314407 0.97677559 0.98571082]\n [0.98143334 0.97867139 0.98541211]\n [0.98314407 0.97746124 0.98496368]\n [0.98339189 0.97769793 0.98571082]\n [0.9835329  0.97787016 0.98544056]\n [0.97978323 0.97874066 0.98600663]\n [0.98165617 0.98225881 0.98643223]\n [0.98407204 0.97762007 0.98563776]\n [0.98143334 0.9776207  0.98521088]\n [0.98403705 0.97762007 0.98605664]\n [0.9830735  0.97708681 0.98571082]\n [0.98398195 0.97867139 0.98540886]\n [0.9834778  0.97329568 0.98549446]\n [0.98165617 0.97771484 0.98684864]\n [0.98127335 0.9774428  0.98703036]\n [0.98392031 0.97837047 0.98537094]\n [0.98144126 0.97730832 0.98558113]\n [0.98403705 0.97677943 0.98692327]\n [0.98383023 0.97996806 0.98652846]\n [0.98407204 0.98160159 0.98506229]\n [0.98433578 0.98177479 0.98566065]\n [0.98328671 0.97769793 0.98646801]\n [0.98417564 0.97854431 0.98572416]\n [0.98284381 0.9769264  0.9856763 ]\n [0.98332132 0.97746187 0.98679008]\n [0.98417564 0.97823164 0.98549189]\n [0.98350651 0.97677943 0.98561335]\n [0.98277911 0.97736415 0.9871394 ]\n [0.98351241 0.97315144 0.98728511]\n [0.98166409 0.97778829 0.98572066]\n [0.9835329  0.97275715 0.98511489]\n [0.98332132 0.97849757 0.98506229]\n [0.98388976 0.97762007 0.98571082]\n [0.98402706 0.97837047 0.98549446]\n [0.98142177 0.98177479 0.98604578]\n [0.98069501 0.97683383 0.98552352]\n [0.98172717 0.97867139 0.98643223]\n [0.98386483 0.97664975 0.98571082]\n [0.98341238 0.97667147 0.98554373]\n [0.98195266 0.97746715 0.98643223]\n [0.98142206 0.97813189 0.98542484]\n [0.98219215 0.9776207  0.98774179]\n [0.98142206 0.98158469 0.98604578]\n [0.98314407 0.97857164 0.98763904]\n [0.9822199  0.97676757 0.98774179]\n [0.98295431 0.98002647 0.98643223]\n [0.98120723 0.97275715 0.98567517]\n [0.98402263 0.97837047 0.98566416]\n [0.98166409 0.97771421 0.98535821]\n [0.98124607 0.97771484 0.98571082]\n [0.98433578 0.97879906 0.98643121]\n [0.98315409 0.98190246 0.98652846]\n [0.98388532 0.97776095 0.9848052 ]\n [0.98417564 0.97766681 0.9856763 ]\n [0.98203679 0.97688854 0.98643223]\n [0.9819412  0.97708681 0.98668499]\n [0.98144126 0.98184406 0.98534522]\n [0.98294968 0.97776095 0.98432378]]",
        "level": 0.95
      },
      "bootstrap_predictions": "[[[0.01397901 0.01195694 0.97406406]\n  [0.95382363 0.02179573 0.02438067]\n  [0.98356789 0.00883616 0.00759597]\n  ...\n  [0.00870771 0.00820246 0.98308984]\n  [0.00784253 0.01190919 0.9802483 ]\n  [0.00877927 0.97879906 0.01242167]]\n\n [[0.97857083 0.01088089 0.01054829]\n  [0.98402835 0.00700618 0.00896545]\n  [0.00866541 0.0112449  0.98008968]\n  ...\n  [0.97380613 0.01789113 0.00830276]\n  [0.95182706 0.02236372 0.0258092 ]\n  [0.00959082 0.01104204 0.97936715]]\n\n [[0.00997736 0.01629136 0.97373128]\n  [0.97857083 0.01088089 0.01054829]\n  [0.01037871 0.97746187 0.0121594 ]\n  ...\n  [0.00939555 0.02051443 0.97009003]\n  [0.00826021 0.00796837 0.98377143]\n  [0.00985395 0.02078005 0.96936599]]\n\n ...\n\n [[0.00928627 0.01072965 0.97998409]\n  [0.01037871 0.97746187 0.0121594 ]\n  [0.98007033 0.01123426 0.0086954 ]\n  ...\n  [0.00857077 0.00785226 0.98357698]\n  [0.02017441 0.9592599  0.0205657 ]\n  [0.00889976 0.01587016 0.97523009]]\n\n [[0.00896779 0.00858411 0.98244812]\n  [0.98036427 0.00935076 0.01028498]\n  [0.94979683 0.02723708 0.02296609]\n  ...\n  [0.0082731  0.01652537 0.97520153]\n  [0.97922242 0.01165381 0.00912378]\n  [0.00826021 0.00796837 0.98377143]]\n\n [[0.0083861  0.00969872 0.98191517]\n  [0.00985395 0.02078005 0.96936599]\n  [0.98039019 0.01090957 0.00870026]\n  ...\n  [0.00866541 0.0112449  0.98008968]\n  [0.98065267 0.01131047 0.00803683]\n  [0.9703315  0.01982959 0.00983887]]]"
    },
    "success": true
  },
  "validation_results": {
    "validation_results": {
      "time_series_cv": {
        "scores": [
          1.0,
          1.0,
          1.0
        ],
        "train_scores": [
          1.0,
          1.0,
          1.0
        ],
        "overfitting_gaps": [
          0.0,
          0.0,
          0.0
        ],
        "fold_details": [
          {
            "fold": 1,
            "train_size": 194,
            "val_size": 191,
            "train_accuracy": 1.0,
            "val_accuracy": 1.0,
            "train_logloss": 0.021131832577601987,
            "val_logloss": 0.05906946057005878,
            "overfitting_gap": 0.0,
            "success": true
          },
          {
            "fold": 2,
            "train_size": 385,
            "val_size": 191,
            "train_accuracy": 1.0,
            "val_accuracy": 1.0,
            "train_logloss": 0.010346159414796765,
            "val_logloss": 0.03955406941464675,
            "overfitting_gap": 0.0,
            "success": true
          },
          {
            "fold": 3,
            "train_size": 576,
            "val_size": 191,
            "train_accuracy": 1.0,
            "val_accuracy": 1.0,
            "train_logloss": 0.008813937554395645,
            "val_logloss": 0.02754603049793664,
            "overfitting_gap": 0.0,
            "success": true
          }
        ],
        "successful_folds": 3,
        "failed_folds": 0,
        "mean_score": 1.0,
        "std_score": 0.0,
        "mean_overfitting": 0.0,
        "max_overfitting": 0.0,
        "coefficient_of_variation": 0.0
      },
      "walk_forward": {
        "scores": [
          1.0,
          1.0,
          1.0
        ],
        "train_sizes": [
          191,
          382,
          573
        ],
        "val_sizes": [
          191,
          191,
          191
        ],
        "fold_details": [
          {
            "fold": 1,
            "train_size": 191,
            "val_size": 191,
            "val_accuracy": 1.0,
            "val_logloss": 0.06264508402232245,
            "success": true
          },
          {
            "fold": 2,
            "train_size": 382,
            "val_size": 191,
            "val_accuracy": 1.0,
            "val_logloss": 0.04479573792018792,
            "success": true
          },
          {
            "fold": 3,
            "train_size": 573,
            "val_size": 191,
            "val_accuracy": 1.0,
            "val_logloss": 0.029647262044423264,
            "success": true
          }
        ],
        "successful_folds": 3,
        "failed_folds": 0,
        "mean_score": 1.0,
        "std_score": 0.0,
        "coefficient_of_variation": 0.0
      },
      "bootstrap": {
        "scores": [
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0
        ],
        "out_of_bag_scores": [
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0,
          1.0
        ],
        "bootstrap_details": [
          {
            "bootstrap": 1,
            "bootstrap_size": 613,
            "oob_size": 345,
            "bootstrap_accuracy": 1.0,
            "oob_accuracy": 1.0,
            "bootstrap_logloss": 0.006685534073050898,
            "oob_logloss": 0.03147659884521001,
            "success": true
          },
          {
            "bootstrap": 2,
            "bootstrap_size": 613,
            "oob_size": 346,
            "bootstrap_accuracy": 1.0,
            "oob_accuracy": 1.0,
            "bootstrap_logloss": 0.006379906260783054,
            "oob_logloss": 0.032451086159346464,
            "success": true
          },
          {
            "bootstrap": 3,
            "bootstrap_size": 613,
            "oob_size": 339,
            "bootstrap_accuracy": 1.0,
            "oob_accuracy": 1.0,
            "bootstrap_logloss": 0.005485896072064275,
            "oob_logloss": 0.029033622269946362,
            "success": true
          },
          {
            "bootstrap": 4,
            "bootstrap_size": 613,
            "oob_size": 368,
            "bootstrap_accuracy": 1.0,
            "oob_accuracy": 1.0,
            "bootstrap_logloss": 0.004794836822614417,
            "oob_logloss": 0.026829394441652644,
            "success": true
          },
          {
            "bootstrap": 5,
            "bootstrap_size": 613,
            "oob_size": 335,
            "bootstrap_accuracy": 1.0,
            "oob_accuracy": 1.0,
            "bootstrap_logloss": 0.006169583476005362,
            "oob_logloss": 0.027362578670141247,
            "success": true
          },
          {
            "bootstrap": 6,
            "bootstrap_size": 613,
            "oob_size": 342,
            "bootstrap_accuracy": 1.0,
            "oob_accuracy": 1.0,
            "bootstrap_logloss": 0.005727426619907035,
            "oob_logloss": 0.022372165210075314,
            "success": true
          },
          {
            "bootstrap": 7,
            "bootstrap_size": 613,
            "oob_size": 346,
            "bootstrap_accuracy": 1.0,
            "oob_accuracy": 1.0,
            "bootstrap_logloss": 0.005893320386796385,
            "oob_logloss": 0.02625457251529012,
            "success": true
          },
          {
            "bootstrap": 8,
            "bootstrap_size": 613,
            "oob_size": 352,
            "bootstrap_accuracy": 1.0,
            "oob_accuracy": 1.0,
            "bootstrap_logloss": 0.006551393926681578,
            "oob_logloss": 0.030328685527397915,
            "success": true
          },
          {
            "bootstrap": 9,
            "bootstrap_size": 613,
            "oob_size": 343,
            "bootstrap_accuracy": 1.0,
            "oob_accuracy": 1.0,
            "bootstrap_logloss": 0.005885477321385654,
            "oob_logloss": 0.026093860295858688,
            "success": true
          },
          {
            "bootstrap": 10,
            "bootstrap_size": 613,
            "oob_size": 354,
            "bootstrap_accuracy": 1.0,
            "oob_accuracy": 1.0,
            "bootstrap_logloss": 0.005820233407203623,
            "oob_logloss": 0.023927759538384472,
            "success": true
          },
          {
            "bootstrap": 11,
            "bootstrap_size": 613,
            "oob_size": 341,
            "bootstrap_accuracy": 1.0,
            "oob_accuracy": 1.0,
            "bootstrap_logloss": 0.005816605058983332,
            "oob_logloss": 0.029290800083548677,
            "success": true
          },
          {
            "bootstrap": 12,
            "bootstrap_size": 613,
            "oob_size": 347,
            "bootstrap_accuracy": 1.0,
            "oob_accuracy": 1.0,
            "bootstrap_logloss": 0.006179358114290325,
            "oob_logloss": 0.028111720580322526,
            "success": true
          },
          {
            "bootstrap": 13,
            "bootstrap_size": 613,
            "oob_size": 355,
            "bootstrap_accuracy": 1.0,
            "oob_accuracy": 1.0,
            "bootstrap_logloss": 0.005085954693518423,
            "oob_logloss": 0.02328409274861756,
            "success": true
          },
          {
            "bootstrap": 14,
            "bootstrap_size": 613,
            "oob_size": 346,
            "bootstrap_accuracy": 1.0,
            "oob_accuracy": 1.0,
            "bootstrap_logloss": 0.007029452550597916,
            "oob_logloss": 0.031646350246810555,
            "success": true
          },
          {
            "bootstrap": 15,
            "bootstrap_size": 613,
            "oob_size": 343,
            "bootstrap_accuracy": 1.0,
            "oob_accuracy": 1.0,
            "bootstrap_logloss": 0.005471692784467059,
            "oob_logloss": 0.020064530758840828,
            "success": true
          },
          {
            "bootstrap": 16,
            "bootstrap_size": 613,
            "oob_size": 346,
            "bootstrap_accuracy": 1.0,
            "oob_accuracy": 1.0,
            "bootstrap_logloss": 0.006125025040280944,
            "oob_logloss": 0.0222901030355215,
            "success": true
          },
          {
            "bootstrap": 17,
            "bootstrap_size": 613,
            "oob_size": 355,
            "bootstrap_accuracy": 1.0,
            "oob_accuracy": 1.0,
            "bootstrap_logloss": 0.00666438412283261,
            "oob_logloss": 0.02931969629233512,
            "success": true
          },
          {
            "bootstrap": 18,
            "bootstrap_size": 613,
            "oob_size": 342,
            "bootstrap_accuracy": 1.0,
            "oob_accuracy": 1.0,
            "bootstrap_logloss": 0.00572682186742734,
            "oob_logloss": 0.03103975755281476,
            "success": true
          },
          {
            "bootstrap": 19,
            "bootstrap_size": 613,
            "oob_size": 347,
            "bootstrap_accuracy": 1.0,
            "oob_accuracy": 1.0,
            "bootstrap_logloss": 0.005182895245357778,
            "oob_logloss": 0.025258625100605024,
            "success": true
          },
          {
            "bootstrap": 20,
            "bootstrap_size": 613,
            "oob_size": 337,
            "bootstrap_accuracy": 1.0,
            "oob_accuracy": 1.0,
            "bootstrap_logloss": 0.006581149288444765,
            "oob_logloss": 0.026586299588433452,
            "success": true
          },
          {
            "bootstrap": 21,
            "bootstrap_size": 613,
            "oob_size": 351,
            "bootstrap_accuracy": 1.0,
            "oob_accuracy": 1.0,
            "bootstrap_logloss": 0.006281422004902439,
            "oob_logloss": 0.027646842703214613,
            "success": true
          },
          {
            "bootstrap": 22,
            "bootstrap_size": 613,
            "oob_size": 342,
            "bootstrap_accuracy": 1.0,
            "oob_accuracy": 1.0,
            "bootstrap_logloss": 0.00568785689552868,
            "oob_logloss": 0.028527389873623472,
            "success": true
          },
          {
            "bootstrap": 23,
            "bootstrap_size": 613,
            "oob_size": 346,
            "bootstrap_accuracy": 1.0,
            "oob_accuracy": 1.0,
            "bootstrap_logloss": 0.005758582469581023,
            "oob_logloss": 0.02707141022332031,
            "success": true
          },
          {
            "bootstrap": 24,
            "bootstrap_size": 613,
            "oob_size": 346,
            "bootstrap_accuracy": 1.0,
            "oob_accuracy": 1.0,
            "bootstrap_logloss": 0.006116823871439057,
            "oob_logloss": 0.03057374907872713,
            "success": true
          },
          {
            "bootstrap": 25,
            "bootstrap_size": 613,
            "oob_size": 333,
            "bootstrap_accuracy": 1.0,
            "oob_accuracy": 1.0,
            "bootstrap_logloss": 0.0062683588251875155,
            "oob_logloss": 0.02267927770564697,
            "success": true
          },
          {
            "bootstrap": 26,
            "bootstrap_size": 613,
            "oob_size": 343,
            "bootstrap_accuracy": 1.0,
            "oob_accuracy": 1.0,
            "bootstrap_logloss": 0.005682766305261538,
            "oob_logloss": 0.027348913133397795,
            "success": true
          },
          {
            "bootstrap": 27,
            "bootstrap_size": 613,
            "oob_size": 341,
            "bootstrap_accuracy": 1.0,
            "oob_accuracy": 1.0,
            "bootstrap_logloss": 0.005814703128446733,
            "oob_logloss": 0.027764369351022142,
            "success": true
          },
          {
            "bootstrap": 28,
            "bootstrap_size": 613,
            "oob_size": 351,
            "bootstrap_accuracy": 1.0,
            "oob_accuracy": 1.0,
            "bootstrap_logloss": 0.005615239810586684,
            "oob_logloss": 0.024058976680042615,
            "success": true
          },
          {
            "bootstrap": 29,
            "bootstrap_size": 613,
            "oob_size": 348,
            "bootstrap_accuracy": 1.0,
            "oob_accuracy": 1.0,
            "bootstrap_logloss": 0.006867592416383635,
            "oob_logloss": 0.029838590496837167,
            "success": true
          },
          {
            "bootstrap": 30,
            "bootstrap_size": 613,
            "oob_size": 350,
            "bootstrap_accuracy": 1.0,
            "oob_accuracy": 1.0,
            "bootstrap_logloss": 0.005235661733001682,
            "oob_logloss": 0.02226989349661187,
            "success": true
          },
          {
            "bootstrap": 31,
            "bootstrap_size": 613,
            "oob_size": 347,
            "bootstrap_accuracy": 1.0,
            "oob_accuracy": 1.0,
            "bootstrap_logloss": 0.0059530766083124,
            "oob_logloss": 0.0323878355127989,
            "success": true
          },
          {
            "bootstrap": 32,
            "bootstrap_size": 613,
            "oob_size": 349,
            "bootstrap_accuracy": 1.0,
            "oob_accuracy": 1.0,
            "bootstrap_logloss": 0.006022964037093902,
            "oob_logloss": 0.029203475092036073,
            "success": true
          },
          {
            "bootstrap": 33,
            "bootstrap_size": 613,
            "oob_size": 337,
            "bootstrap_accuracy": 1.0,
            "oob_accuracy": 1.0,
            "bootstrap_logloss": 0.005752645525922664,
            "oob_logloss": 0.024059583861648397,
            "success": true
          },
          {
            "bootstrap": 34,
            "bootstrap_size": 613,
            "oob_size": 358,
            "bootstrap_accuracy": 1.0,
            "oob_accuracy": 1.0,
            "bootstrap_logloss": 0.004785273407411799,
            "oob_logloss": 0.026279772580940036,
            "success": true
          },
          {
            "bootstrap": 35,
            "bootstrap_size": 613,
            "oob_size": 347,
            "bootstrap_accuracy": 1.0,
            "oob_accuracy": 1.0,
            "bootstrap_logloss": 0.006750953709304914,
            "oob_logloss": 0.028784715952768408,
            "success": true
          },
          {
            "bootstrap": 36,
            "bootstrap_size": 613,
            "oob_size": 356,
            "bootstrap_accuracy": 1.0,
            "oob_accuracy": 1.0,
            "bootstrap_logloss": 0.005424245745177371,
            "oob_logloss": 0.03217819628177859,
            "success": true
          },
          {
            "bootstrap": 37,
            "bootstrap_size": 613,
            "oob_size": 352,
            "bootstrap_accuracy": 1.0,
            "oob_accuracy": 1.0,
            "bootstrap_logloss": 0.006288083377627122,
            "oob_logloss": 0.030466264998673984,
            "success": true
          },
          {
            "bootstrap": 38,
            "bootstrap_size": 613,
            "oob_size": 355,
            "bootstrap_accuracy": 1.0,
            "oob_accuracy": 1.0,
            "bootstrap_logloss": 0.00591066591577865,
            "oob_logloss": 0.026963688826316338,
            "success": true
          },
          {
            "bootstrap": 39,
            "bootstrap_size": 613,
            "oob_size": 347,
            "bootstrap_accuracy": 1.0,
            "oob_accuracy": 1.0,
            "bootstrap_logloss": 0.006712148172186245,
            "oob_logloss": 0.03217976770284368,
            "success": true
          },
          {
            "bootstrap": 40,
            "bootstrap_size": 613,
            "oob_size": 343,
            "bootstrap_accuracy": 1.0,
            "oob_accuracy": 1.0,
            "bootstrap_logloss": 0.005979196334973484,
            "oob_logloss": 0.02591406040735959,
            "success": true
          },
          {
            "bootstrap": 41,
            "bootstrap_size": 613,
            "oob_size": 354,
            "bootstrap_accuracy": 1.0,
            "oob_accuracy": 1.0,
            "bootstrap_logloss": 0.005636088173570969,
            "oob_logloss": 0.030255583498600542,
            "success": true
          },
          {
            "bootstrap": 42,
            "bootstrap_size": 613,
            "oob_size": 360,
            "bootstrap_accuracy": 1.0,
            "oob_accuracy": 1.0,
            "bootstrap_logloss": 0.005680188493507964,
            "oob_logloss": 0.027629820571992893,
            "success": true
          },
          {
            "bootstrap": 43,
            "bootstrap_size": 613,
            "oob_size": 341,
            "bootstrap_accuracy": 1.0,
            "oob_accuracy": 1.0,
            "bootstrap_logloss": 0.007314746969386564,
            "oob_logloss": 0.02979496239620961,
            "success": true
          },
          {
            "bootstrap": 44,
            "bootstrap_size": 613,
            "oob_size": 338,
            "bootstrap_accuracy": 1.0,
            "oob_accuracy": 1.0,
            "bootstrap_logloss": 0.006924432258886162,
            "oob_logloss": 0.030443160596624233,
            "success": true
          },
          {
            "bootstrap": 45,
            "bootstrap_size": 613,
            "oob_size": 334,
            "bootstrap_accuracy": 1.0,
            "oob_accuracy": 1.0,
            "bootstrap_logloss": 0.006344577511718186,
            "oob_logloss": 0.02272482269414457,
            "success": true
          },
          {
            "bootstrap": 46,
            "bootstrap_size": 613,
            "oob_size": 351,
            "bootstrap_accuracy": 1.0,
            "oob_accuracy": 1.0,
            "bootstrap_logloss": 0.00540440861821513,
            "oob_logloss": 0.02446610222117215,
            "success": true
          },
          {
            "bootstrap": 47,
            "bootstrap_size": 613,
            "oob_size": 355,
            "bootstrap_accuracy": 1.0,
            "oob_accuracy": 1.0,
            "bootstrap_logloss": 0.006062210026294128,
            "oob_logloss": 0.02776574260517601,
            "success": true
          },
          {
            "bootstrap": 48,
            "bootstrap_size": 613,
            "oob_size": 346,
            "bootstrap_accuracy": 1.0,
            "oob_accuracy": 1.0,
            "bootstrap_logloss": 0.005488488548182014,
            "oob_logloss": 0.02984095300096755,
            "success": true
          },
          {
            "bootstrap": 49,
            "bootstrap_size": 613,
            "oob_size": 334,
            "bootstrap_accuracy": 1.0,
            "oob_accuracy": 1.0,
            "bootstrap_logloss": 0.006730643010387874,
            "oob_logloss": 0.031277985445868024,
            "success": true
          },
          {
            "bootstrap": 50,
            "bootstrap_size": 613,
            "oob_size": 367,
            "bootstrap_accuracy": 1.0,
            "oob_accuracy": 1.0,
            "bootstrap_logloss": 0.006147898107689532,
            "oob_logloss": 0.030228093975371302,
            "success": true
          }
        ],
        "successful_bootstraps": 50,
        "failed_bootstraps": 0,
        "mean_score": 1.0,
        "std_score": 0.0,
        "mean_oob_score": 1.0,
        "std_oob_score": 0.0,
        "optimism": 0.0
      },
      "calibration": {
        "original": {
          "brier_score": 0.002025974025974026,
          "reliability": 0.0,
          "sharpness": 0.20904473304473306,
          "calibration_error": 0.020779220779220786
        },
        "calibrated": {
          "brier_score": 1.0543760445201431e-05,
          "reliability": 0.0,
          "sharpness": 0.22201388948379752,
          "calibration_error": 0.00031777098785973346
        },
        "improvement": {
          "brier_score": 0.0020154302655288245,
          "calibration_error": 0.020461449791361053
        }
      },
      "overfitting": {
        "detected": false,
        "severity": "none",
        "metrics": {
          "mean_overfitting": 0.0,
          "max_overfitting": 0.0
        },
        "recommendations": []
      },
      "stability": {
        "stable": true,
        "metrics": {
          "coefficient_of_variation": 0.0,
          "std_score": 0.0,
          "walk_forward_cv": 0.0,
          "optimism": 0.0
        },
        "issues": []
      },
      "overall": {
        "validation_passed": true,
        "overall_score": 1.0,
        "issues": [],
        "recommendations": [
          "Model validation passed successfully"
        ]
      }
    },
    "report": "================================================================================\nNON-MAJOR LEAGUE MODEL VALIDATION REPORT\n================================================================================\n\nOVERALL ASSESSMENT:\n  Validation Status: PASSED\n  Overall Score: 1.0000\n\n  Recommendations:\n    - Model validation passed successfully\n\nDETAILED VALIDATION RESULTS:\n----------------------------------------\n\nTime Series Cross-Validation:\n  Successful folds: 3\n  Failed folds: 0\n  Mean accuracy: 1.0000\n  Std accuracy: 0.0000\n  Coefficient of variation: 0.0000\n  Mean overfitting: 0.0000\n  Max overfitting: 0.0000\n\nWalk-Forward Validation:\n  Successful folds: 3\n  Failed folds: 0\n  Mean accuracy: 1.0000\n  Std accuracy: 0.0000\n  Coefficient of variation: 0.0000\n\nBootstrap Validation:\n  Successful bootstraps: 50\n  Failed bootstraps: 0\n  Mean accuracy: 1.0000\n  Mean OOB accuracy: 1.0000\n  Optimism: 0.0000\n\nModel Calibration:\n  Original Brier score: 0.0020\n  Original calibration error: 0.0208\n  Calibrated Brier score: 0.0000\n  Calibrated calibration error: 0.0003\n  Brier score improvement: +0.0020\n  Calibration error improvement: +0.0205\n\nOverfitting Detection:\n  Overfitting detected: NO\n  Severity: none\n\nStability Analysis:\n  Model stable: YES\n\n================================================================================",
    "success": true
  },
  "pipeline_metrics": {
    "total_components": 5,
    "successful_components": 4,
    "pipeline_success": true,
    "best_model": "ensemble",
    "best_score": 1.0,
    "recommendations": [
      "Phase 2 pipeline completed successfully",
      "Proceed to Phase 3: Validation and Testing"
    ]
  },
  "component_results": {
    "model_architecture": {
      "base_models": {
        "xgboost": "XGBClassifier(base_score=None, booster=None, callbacks=None,\n              colsample_bylevel=None, colsample_bynode=None,\n              colsample_bytree=0.8, device=None, early_stopping_rounds=None,\n              enable_categorical=False, eval_metric='mlogloss',\n              feature_types=None, feature_weights=None, gamma=None,\n              grow_policy=None, importance_type=None,\n              interaction_constraints=None, learning_rate=0.05, max_bin=None,\n              max_cat_threshold=None, max_cat_to_onehot=None,\n              max_delta_step=None, max_depth=6, max_leaves=None,\n              min_child_weight=3, missing=nan, monotone_constraints=None,\n              multi_strategy=None, n_estimators=200, n_jobs=None,\n              num_parallel_tree=None, ...)",
        "lightgbm": "LGBMClassifier(bagging_fraction=0.8, bagging_freq=5, feature_fraction=0.8,\n               lambda_l1=0.1, lambda_l2=1.0, learning_rate=0.05, max_depth=6,\n               metric='multi_logloss', min_data_in_leaf=20, n_estimators=200,\n               num_class=3, num_leaves=50, objective='multiclass',\n               random_state=42, verbose=-1)",
        "random_forest": "RandomForestClassifier(class_weight='balanced', max_depth=10,\n                       min_samples_leaf=2, min_samples_split=5,\n                       random_state=42)",
        "logistic_regression": "LogisticRegression(class_weight='balanced', max_iter=1000, random_state=42)"
      },
      "training_results": {
        "xgboost": {
          "model": "XGBClassifier(base_score=None, booster=None, callbacks=None,\n              colsample_bylevel=None, colsample_bynode=None,\n              colsample_bytree=0.8, device=None, early_stopping_rounds=None,\n              enable_categorical=False, eval_metric='mlogloss',\n              feature_types=None, feature_weights=None, gamma=None,\n              grow_policy=None, importance_type=None,\n              interaction_constraints=None, learning_rate=0.05, max_bin=None,\n              max_cat_threshold=None, max_cat_to_onehot=None,\n              max_delta_step=None, max_depth=6, max_leaves=None,\n              min_child_weight=3, missing=nan, monotone_constraints=None,\n              multi_strategy=None, n_estimators=200, n_jobs=None,\n              num_parallel_tree=None, ...)",
          "train_accuracy": 1.0,
          "val_accuracy": 1.0,
          "train_logloss": 0.010888727160769005,
          "val_logloss": 0.010889639223756489,
          "success": true
        },
        "lightgbm": {
          "model": "LGBMClassifier(bagging_fraction=0.8, bagging_freq=5, feature_fraction=0.8,\n               lambda_l1=0.1, lambda_l2=1.0, learning_rate=0.05, max_depth=6,\n               metric='multi_logloss', min_data_in_leaf=20, n_estimators=200,\n               num_class=3, num_leaves=50, objective='multiclass',\n               random_state=42, verbose=-1)",
          "train_accuracy": 1.0,
          "val_accuracy": 1.0,
          "train_logloss": 0.0012882157434589115,
          "val_logloss": 0.001284234129843217,
          "success": true
        },
        "random_forest": {
          "model": "RandomForestClassifier(class_weight='balanced', max_depth=10,\n                       min_samples_leaf=2, min_samples_split=5,\n                       random_state=42)",
          "train_accuracy": 1.0,
          "val_accuracy": 1.0,
          "train_logloss": 0.015337317409059753,
          "val_logloss": 0.029023413348350846,
          "success": true
        },
        "logistic_regression": {
          "model": "LogisticRegression(class_weight='balanced', max_iter=1000, random_state=42)",
          "train_accuracy": 1.0,
          "val_accuracy": 1.0,
          "train_logloss": 0.003133323409808377,
          "val_logloss": 0.00409310790726011,
          "success": true
        }
      },
      "ensemble_model": "VotingClassifier(estimators=[('xgboost',\n                              XGBClassifier(base_score=None, booster=None,\n                                            callbacks=None,\n                                            colsample_bylevel=None,\n                                            colsample_bynode=None,\n                                            colsample_bytree=0.8, device=None,\n                                            early_stopping_rounds=None,\n                                            enable_categorical=False,\n                                            eval_metric='mlogloss',\n                                            feature_types=None,\n                                            feature_weights=None, gamma=None,\n                                            grow_policy=None,\n                                            importance_type=None,\n                                            interactio...\n                                             min_data_in_leaf=20,\n                                             n_estimators=200, num_class=3,\n                                             num_leaves=50,\n                                             objective='multiclass',\n                                             random_state=42, verbose=-1)),\n                             ('random_forest',\n                              RandomForestClassifier(class_weight='balanced',\n                                                     max_depth=10,\n                                                     min_samples_leaf=2,\n                                                     min_samples_split=5,\n                                                     random_state=42)),\n                             ('logistic_regression',\n                              LogisticRegression(class_weight='balanced',\n                                                 max_iter=1000,\n                                                 random_state=42))],\n                 voting='soft')",
      "ensemble_results": {
        "model": "VotingClassifier(estimators=[('xgboost',\n                              XGBClassifier(base_score=None, booster=None,\n                                            callbacks=None,\n                                            colsample_bylevel=None,\n                                            colsample_bynode=None,\n                                            colsample_bytree=0.8, device=None,\n                                            early_stopping_rounds=None,\n                                            enable_categorical=False,\n                                            eval_metric='mlogloss',\n                                            feature_types=None,\n                                            feature_weights=None, gamma=None,\n                                            grow_policy=None,\n                                            importance_type=None,\n                                            interactio...\n                                             min_data_in_leaf=20,\n                                             n_estimators=200, num_class=3,\n                                             num_leaves=50,\n                                             objective='multiclass',\n                                             random_state=42, verbose=-1)),\n                             ('random_forest',\n                              RandomForestClassifier(class_weight='balanced',\n                                                     max_depth=10,\n                                                     min_samples_leaf=2,\n                                                     min_samples_split=5,\n                                                     random_state=42)),\n                             ('logistic_regression',\n                              LogisticRegression(class_weight='balanced',\n                                                 max_iter=1000,\n                                                 random_state=42))],\n                 voting='soft')",
        "train_accuracy": 1.0,
        "val_accuracy": 1.0,
        "train_logloss": 0.007579608973137412,
        "val_logloss": 0.011132555756122146,
        "success": true
      },
      "calibrated_models": {
        "xgboost": "CalibratedClassifierCV(cv=3,\n                       estimator=XGBClassifier(base_score=None, booster=None,\n                                               callbacks=None,\n                                               colsample_bylevel=None,\n                                               colsample_bynode=None,\n                                               colsample_bytree=0.8,\n                                               device=None,\n                                               early_stopping_rounds=None,\n                                               enable_categorical=False,\n                                               eval_metric='mlogloss',\n                                               feature_types=None,\n                                               feature_weights=None, gamma=None,\n                                               grow_policy=None,\n                                               importance_type=None,\n                                               interaction_constraints=None,\n                                               learning_rate=0.05, max_bin=None,\n                                               max_cat_threshold=None,\n                                               max_cat_to_onehot=None,\n                                               max_delta_step=None, max_depth=6,\n                                               max_leaves=None,\n                                               min_child_weight=3, missing=nan,\n                                               monotone_constraints=None,\n                                               multi_strategy=None,\n                                               n_estimators=200, n_jobs=None,\n                                               num_parallel_tree=None, ...),\n                       method='isotonic')",
        "lightgbm": "CalibratedClassifierCV(cv=3,\n                       estimator=LGBMClassifier(bagging_fraction=0.8,\n                                                bagging_freq=5,\n                                                feature_fraction=0.8,\n                                                lambda_l1=0.1, lambda_l2=1.0,\n                                                learning_rate=0.05, max_depth=6,\n                                                metric='multi_logloss',\n                                                min_data_in_leaf=20,\n                                                n_estimators=200, num_class=3,\n                                                num_leaves=50,\n                                                objective='multiclass',\n                                                random_state=42, verbose=-1),\n                       method='isotonic')",
        "random_forest": "CalibratedClassifierCV(cv=3,\n                       estimator=RandomForestClassifier(class_weight='balanced',\n                                                        max_depth=10,\n                                                        min_samples_leaf=2,\n                                                        min_samples_split=5,\n                                                        random_state=42),\n                       method='isotonic')",
        "logistic_regression": "CalibratedClassifierCV(cv=3,\n                       estimator=LogisticRegression(class_weight='balanced',\n                                                    max_iter=1000,\n                                                    random_state=42),\n                       method='isotonic')",
        "ensemble": "CalibratedClassifierCV(cv=3,\n                       estimator=VotingClassifier(estimators=[('xgboost',\n                                                               XGBClassifier(base_score=None,\n                                                                             booster=None,\n                                                                             callbacks=None,\n                                                                             colsample_bylevel=None,\n                                                                             colsample_bynode=None,\n                                                                             colsample_bytree=0.8,\n                                                                             device=None,\n                                                                             early_stopping_rounds=None,\n                                                                             enable_categorical=False,\n                                                                             eval_metric='mlogloss',\n                                                                             feature_types=None,\n                                                                             feature_weights=None,\n                                                                             gamma=None,\n                                                                             grow_polic...\n                                                                              n_estimators=200,\n                                                                              num_class=3,\n                                                                              num_leaves=50,\n                                                                              objective='multiclass',\n                                                                              random_state=42,\n                                                                              verbose=-1)),\n                                                              ('random_forest',\n                                                               RandomForestClassifier(class_weight='balanced',\n                                                                                      max_depth=10,\n                                                                                      min_samples_leaf=2,\n                                                                                      min_samples_split=5,\n                                                                                      random_state=42)),\n                                                              ('logistic_regression',\n                                                               LogisticRegression(class_weight='balanced',\n                                                                                  max_iter=1000,\n                                                                                  random_state=42))],\n                                                  voting='soft'),\n                       method='isotonic')"
      },
      "evaluation_results": {
        "xgboost": {
          "accuracy": 1.0,
          "log_loss": 0.010854325623035364,
          "predictions": "[0 2 2 0 2 2 2 1 2 1 1 1 2 2 0 2 0 2 2 0 1 2 2 0 2 2 0 2 0 2 2 1 2 1 2 2 1\n 2 2 2 0 0 1 2 0 2 0 1 0 2 2 2 1 2 1 2 1 0 2 0 0 0 0 2 2 2 2 1 2 2 0 0 1 0\n 1 2 1 2 0 0 2 2 1 0 0 2 2 0 1 0 0 0 2 2 1 2 2 2 0 1 2 0 2 2 2 0 2 2 1 1 0\n 2 0 2 1 2 2 2 2 0 2 0 0 0 0 2 2 1 2 2 0 0 0 0 2 1 2 2 2 0 1 1 0 1 0 0 0 1\n 0 0 0 2 1 1]",
          "probabilities": "[[0.9891468  0.00551647 0.00533676]\n [0.00514989 0.00537398 0.9894761 ]\n [0.00529043 0.00543968 0.9892699 ]\n [0.9891398  0.00552345 0.00533673]\n [0.00515026 0.00530303 0.9895467 ]\n [0.00514997 0.00535859 0.98949146]\n [0.00526134 0.00543304 0.9893056 ]\n [0.00563348 0.98873377 0.00563272]\n [0.00526124 0.00545167 0.989287  ]\n [0.00558653 0.9887966  0.00561686]\n [0.00563348 0.98873377 0.00563272]\n [0.00560248 0.98876464 0.00563289]\n [0.00514989 0.00537398 0.9894761 ]\n [0.00515007 0.00534028 0.9895097 ]\n [0.98926765 0.00542446 0.00530787]\n [0.00515018 0.00531825 0.9895315 ]\n [0.9892453  0.00544692 0.00530775]\n [0.00529055 0.00541733 0.98929214]\n [0.00514989 0.00537398 0.9894761 ]\n [0.9892559  0.00543625 0.00530781]\n [0.00660472 0.9866099  0.00678545]\n [0.00515011 0.00533165 0.9895182 ]\n [0.00526104 0.00548996 0.989249  ]\n [0.9891122  0.00546496 0.00542282]\n [0.00526134 0.00543304 0.9893056 ]\n [0.00515003 0.00534697 0.989503  ]\n [0.9891517  0.00551146 0.00533679]\n [0.00515015 0.00532492 0.98952496]\n [0.98922056 0.00547187 0.00530762]\n [0.00514989 0.00537398 0.9894761 ]\n [0.00526104 0.00548996 0.989249  ]\n [0.0055738  0.98869985 0.00572632]\n [0.00515026 0.00530303 0.9895467 ]\n [0.00558585 0.9886755  0.0057387 ]\n [0.00515018 0.00531825 0.9895315 ]\n [0.00515018 0.00531825 0.9895315 ]\n [0.00550131 0.9889675  0.00553117]\n [0.00515018 0.00531825 0.9895315 ]\n [0.00515026 0.00530303 0.9895467 ]\n [0.0052613  0.00543984 0.9892988 ]\n [0.9892267  0.0054656  0.00530765]\n [0.98922414 0.00543871 0.00533718]\n [0.00553109 0.98881793 0.00565099]\n [0.00526134 0.00543304 0.9893056 ]\n [0.98914635 0.00543069 0.00542301]\n [0.00514997 0.00535859 0.98949146]\n [0.98922414 0.00543871 0.00533718]\n [0.00553582 0.98889834 0.00556587]\n [0.9891468  0.00551647 0.00533676]\n [0.00515018 0.00531825 0.9895315 ]\n [0.00529043 0.00543968 0.9892699 ]\n [0.00515011 0.00533165 0.9895182 ]\n [0.0055366  0.98695403 0.00750937]\n [0.00514989 0.00537398 0.9894761 ]\n [0.00560248 0.98876464 0.00563289]\n [0.00514997 0.00535859 0.98949146]\n [0.00563279 0.98861235 0.0057549 ]\n [0.9891089  0.00543807 0.00545299]\n [0.00515003 0.00534697 0.989503  ]\n [0.98916745 0.00549569 0.00533688]\n [0.98922414 0.00543871 0.00533718]\n [0.9892831  0.00540893 0.00530796]\n [0.98905224 0.00549505 0.00545267]\n [0.00515026 0.00530303 0.9895467 ]\n [0.00517884 0.00529137 0.9895298 ]\n [0.00517855 0.00534681 0.98947465]\n [0.0051504  0.00527637 0.9895732 ]\n [0.00560465 0.9886692  0.00572614]\n [0.00526112 0.00547424 0.98926467]\n [0.00514997 0.00535859 0.98949146]\n [0.98917425 0.00548881 0.00533691]\n [0.9891517  0.00551146 0.00533679]\n [0.00553582 0.98889834 0.00556587]\n [0.9892831  0.00540893 0.00530796]\n [0.00555861 0.9888526  0.00558879]\n [0.00526142 0.00541748 0.9893211 ]\n [0.00554405 0.98876023 0.00569576]\n [0.00517863 0.0053315  0.98948985]\n [0.9891398  0.00552345 0.00533673]\n [0.9892111  0.00548129 0.00530757]\n [0.00514997 0.00535859 0.98949146]\n [0.00515018 0.00531825 0.9895315 ]\n [0.00550065 0.9888482  0.00565116]\n [0.98916745 0.00549569 0.00533688]\n [0.9892151  0.00542417 0.00536079]\n [0.00514989 0.00537398 0.9894761 ]\n [0.00529017 0.0054898  0.9892201 ]\n [0.9891122  0.00546496 0.00542282]\n [0.0055738  0.98869985 0.00572632]\n [0.98924035 0.00545186 0.00530773]\n [0.9892267  0.0054656  0.00530765]\n [0.9892831  0.00540893 0.00530796]\n [0.00526104 0.00548996 0.989249  ]\n [0.00515032 0.00529152 0.98955816]\n [0.00557448 0.9888208  0.00560474]\n [0.00514997 0.00535859 0.98949146]\n [0.0051787  0.0053181  0.98950315]\n [0.00515011 0.00533165 0.9895182 ]\n [0.9892831  0.00540893 0.00530796]\n [0.00560248 0.98876464 0.00563289]\n [0.00526112 0.00547424 0.98926467]\n [0.98915523 0.00550795 0.00533681]\n [0.00514989 0.00537398 0.9894761 ]\n [0.0051787  0.0053181  0.98950315]\n [0.0051787  0.0053181  0.98950315]\n [0.98926765 0.00542446 0.00530787]\n [0.00529029 0.00546716 0.9892426 ]\n [0.00515018 0.00531825 0.9895315 ]\n [0.00558653 0.9887966  0.00561686]\n [0.00555793 0.988732   0.00571002]\n [0.98919934 0.0054932  0.00530751]\n [0.00526112 0.00547424 0.98926467]\n [0.98926765 0.00542446 0.00530787]\n [0.00515003 0.00534697 0.989503  ]\n [0.00551702 0.988936   0.00554697]\n [0.00515011 0.00533165 0.9895182 ]\n [0.0051504  0.00527637 0.9895732 ]\n [0.00517878 0.00530288 0.98951834]\n [0.00526112 0.00547424 0.98926467]\n [0.9891812  0.00548188 0.00533695]\n [0.00515011 0.00533165 0.9895182 ]\n [0.9892831  0.00540893 0.00530796]\n [0.9891812  0.00548188 0.00533695]\n [0.9891517  0.00551146 0.00533679]\n [0.9892018  0.00546123 0.00533706]\n [0.00526157 0.00539025 0.9893482 ]\n [0.00515003 0.00534697 0.989503  ]\n [0.00550131 0.9889675  0.00553117]\n [0.00515018 0.00531825 0.9895315 ]\n [0.00526112 0.00547424 0.98926467]\n [0.9891685  0.0054083  0.00542313]\n [0.9892831  0.00540893 0.00530796]\n [0.98913085 0.00544629 0.00542292]\n [0.98912406 0.0055393  0.00533664]\n [0.00515018 0.00531825 0.9895315 ]\n [0.00558906 0.9887666  0.00564432]\n [0.00514997 0.00535859 0.98949146]\n [0.00515032 0.00529152 0.98955816]\n [0.0051504  0.00527637 0.9895732 ]\n [0.9892336  0.00545876 0.00530769]\n [0.00561745 0.9887659  0.00561668]\n [0.00550065 0.9888482  0.00565116]\n [0.98922414 0.00543871 0.00533718]\n [0.00557951 0.98881066 0.0056098 ]\n [0.9892111  0.00548129 0.00530757]\n [0.9892267  0.0054656  0.00530765]\n [0.9891967  0.00546618 0.00533703]\n [0.0055738  0.98869985 0.00572632]\n [0.9892831  0.00540893 0.00530796]\n [0.9892831  0.00540893 0.00530796]\n [0.98905224 0.00549505 0.00545267]\n [0.00517884 0.00529137 0.9895298 ]\n [0.00553176 0.98893726 0.005531  ]\n [0.00553176 0.98893726 0.005531  ]]"
        },
        "lightgbm": {
          "accuracy": 1.0,
          "log_loss": 0.0012796861394519484,
          "predictions": "[0 2 2 0 2 2 2 1 2 1 1 1 2 2 0 2 0 2 2 0 1 2 2 0 2 2 0 2 0 2 2 1 2 1 2 2 1\n 2 2 2 0 0 1 2 0 2 0 1 0 2 2 2 1 2 1 2 1 0 2 0 0 0 0 2 2 2 2 1 2 2 0 0 1 0\n 1 2 1 2 0 0 2 2 1 0 0 2 2 0 1 0 0 0 2 2 1 2 2 2 0 1 2 0 2 2 2 0 2 2 1 1 0\n 2 0 2 1 2 2 2 2 0 2 0 0 0 0 2 2 1 2 2 0 0 0 0 2 1 2 2 2 0 1 1 0 1 0 0 0 1\n 0 0 0 2 1 1]",
          "probabilities": "[[9.98695110e-01 6.48569085e-04 6.56320948e-04]\n [6.18282640e-04 6.23597953e-04 9.98758119e-01]\n [6.18283350e-04 6.22450468e-04 9.98759266e-01]\n [9.98693915e-01 6.49764689e-04 6.56320163e-04]\n [6.18283350e-04 6.22450468e-04 9.98759266e-01]\n [6.18283350e-04 6.22450468e-04 9.98759266e-01]\n [6.18283350e-04 6.22450468e-04 9.98759266e-01]\n [6.52200159e-04 9.98681881e-01 6.65919282e-04]\n [6.18283350e-04 6.22450468e-04 9.98759266e-01]\n [6.52200159e-04 9.98681881e-01 6.65919282e-04]\n [6.52200159e-04 9.98681881e-01 6.65919282e-04]\n [6.52200159e-04 9.98681881e-01 6.65919282e-04]\n [6.18283350e-04 6.22450468e-04 9.98759266e-01]\n [6.18282640e-04 6.23597953e-04 9.98758119e-01]\n [9.98693915e-01 6.49764689e-04 6.56320163e-04]\n [6.18283350e-04 6.22450468e-04 9.98759266e-01]\n [9.98695110e-01 6.48569085e-04 6.56320948e-04]\n [6.18283350e-04 6.22450468e-04 9.98759266e-01]\n [6.18283350e-04 6.22450468e-04 9.98759266e-01]\n [9.98693915e-01 6.49764689e-04 6.56320163e-04]\n [6.78434794e-04 9.98655663e-01 6.65901801e-04]\n [6.18282640e-04 6.23597953e-04 9.98758119e-01]\n [6.18283350e-04 6.22450468e-04 9.98759266e-01]\n [9.98695110e-01 6.48569085e-04 6.56320948e-04]\n [6.18283350e-04 6.22450468e-04 9.98759266e-01]\n [6.18283350e-04 6.22450468e-04 9.98759266e-01]\n [9.98693915e-01 6.49764689e-04 6.56320163e-04]\n [6.18282640e-04 6.23597953e-04 9.98758119e-01]\n [9.98693915e-01 6.49764689e-04 6.56320163e-04]\n [6.18282640e-04 6.23597953e-04 9.98758119e-01]\n [6.18283350e-04 6.22450468e-04 9.98759266e-01]\n [6.52200159e-04 9.98681881e-01 6.65919282e-04]\n [6.18283350e-04 6.22450468e-04 9.98759266e-01]\n [6.52200159e-04 9.98681881e-01 6.65919282e-04]\n [6.18283350e-04 6.22450468e-04 9.98759266e-01]\n [6.18283350e-04 6.22450468e-04 9.98759266e-01]\n [6.52200159e-04 9.98681881e-01 6.65919282e-04]\n [6.18283350e-04 6.22450468e-04 9.98759266e-01]\n [6.18283350e-04 6.22450468e-04 9.98759266e-01]\n [6.18282640e-04 6.23597953e-04 9.98758119e-01]\n [9.98695110e-01 6.48569085e-04 6.56320948e-04]\n [9.98695110e-01 6.48569085e-04 6.56320948e-04]\n [6.52200159e-04 9.98681881e-01 6.65919282e-04]\n [6.18282640e-04 6.23597953e-04 9.98758119e-01]\n [9.98695110e-01 6.48569085e-04 6.56320948e-04]\n [6.18282640e-04 6.23597953e-04 9.98758119e-01]\n [9.98693915e-01 6.49764689e-04 6.56320163e-04]\n [6.52200159e-04 9.98681881e-01 6.65919282e-04]\n [9.98693915e-01 6.49764689e-04 6.56320163e-04]\n [6.18282640e-04 6.23597953e-04 9.98758119e-01]\n [6.18283350e-04 6.22450468e-04 9.98759266e-01]\n [6.18282640e-04 6.23597953e-04 9.98758119e-01]\n [6.52200159e-04 9.98681881e-01 6.65919282e-04]\n [6.18282640e-04 6.23597953e-04 9.98758119e-01]\n [6.52200159e-04 9.98681881e-01 6.65919282e-04]\n [6.18283350e-04 6.22450468e-04 9.98759266e-01]\n [6.52200159e-04 9.98681881e-01 6.65919282e-04]\n [9.98695110e-01 6.48569085e-04 6.56320948e-04]\n [6.18283350e-04 6.22450468e-04 9.98759266e-01]\n [9.98695110e-01 6.48569085e-04 6.56320948e-04]\n [9.98695110e-01 6.48569085e-04 6.56320948e-04]\n [9.98695110e-01 6.48569085e-04 6.56320948e-04]\n [9.98695110e-01 6.48569085e-04 6.56320948e-04]\n [6.18282640e-04 6.23597953e-04 9.98758119e-01]\n [6.18282640e-04 6.23597953e-04 9.98758119e-01]\n [6.18282640e-04 6.23597953e-04 9.98758119e-01]\n [6.18283350e-04 6.22450468e-04 9.98759266e-01]\n [6.52200159e-04 9.98681881e-01 6.65919282e-04]\n [6.18283350e-04 6.22450468e-04 9.98759266e-01]\n [6.18283350e-04 6.22450468e-04 9.98759266e-01]\n [9.98693915e-01 6.49764689e-04 6.56320163e-04]\n [9.98695110e-01 6.48569085e-04 6.56320948e-04]\n [6.52200159e-04 9.98681881e-01 6.65919282e-04]\n [9.98693915e-01 6.49764689e-04 6.56320163e-04]\n [6.52200159e-04 9.98681881e-01 6.65919282e-04]\n [6.18283350e-04 6.22450468e-04 9.98759266e-01]\n [6.52200159e-04 9.98681881e-01 6.65919282e-04]\n [6.18282640e-04 6.23597953e-04 9.98758119e-01]\n [9.98693915e-01 6.49764689e-04 6.56320163e-04]\n [9.98695110e-01 6.48569085e-04 6.56320948e-04]\n [6.18283350e-04 6.22450468e-04 9.98759266e-01]\n [6.18282640e-04 6.23597953e-04 9.98758119e-01]\n [6.52200159e-04 9.98681881e-01 6.65919282e-04]\n [9.98695110e-01 6.48569085e-04 6.56320948e-04]\n [9.98693915e-01 6.49764689e-04 6.56320163e-04]\n [6.18282640e-04 6.23597953e-04 9.98758119e-01]\n [6.18283350e-04 6.22450468e-04 9.98759266e-01]\n [9.98695110e-01 6.48569085e-04 6.56320948e-04]\n [6.52200159e-04 9.98681881e-01 6.65919282e-04]\n [9.98693915e-01 6.49764689e-04 6.56320163e-04]\n [9.98695110e-01 6.48569085e-04 6.56320948e-04]\n [9.98693915e-01 6.49764689e-04 6.56320163e-04]\n [6.18283350e-04 6.22450468e-04 9.98759266e-01]\n [6.18282640e-04 6.23597953e-04 9.98758119e-01]\n [6.52200159e-04 9.98681881e-01 6.65919282e-04]\n [6.18282640e-04 6.23597953e-04 9.98758119e-01]\n [6.18282640e-04 6.23597953e-04 9.98758119e-01]\n [6.18282640e-04 6.23597953e-04 9.98758119e-01]\n [9.98695110e-01 6.48569085e-04 6.56320948e-04]\n [6.52200159e-04 9.98681881e-01 6.65919282e-04]\n [6.18283350e-04 6.22450468e-04 9.98759266e-01]\n [9.98693915e-01 6.49764689e-04 6.56320163e-04]\n [6.18282640e-04 6.23597953e-04 9.98758119e-01]\n [6.18282640e-04 6.23597953e-04 9.98758119e-01]\n [6.18282640e-04 6.23597953e-04 9.98758119e-01]\n [9.98695110e-01 6.48569085e-04 6.56320948e-04]\n [6.18282640e-04 6.23597953e-04 9.98758119e-01]\n [6.18282640e-04 6.23597953e-04 9.98758119e-01]\n [6.52200159e-04 9.98681881e-01 6.65919282e-04]\n [6.52200159e-04 9.98681881e-01 6.65919282e-04]\n [9.98695110e-01 6.48569085e-04 6.56320948e-04]\n [6.18283350e-04 6.22450468e-04 9.98759266e-01]\n [9.98693915e-01 6.49764689e-04 6.56320163e-04]\n [6.18282640e-04 6.23597953e-04 9.98758119e-01]\n [6.52200159e-04 9.98681881e-01 6.65919282e-04]\n [6.18273576e-04 6.38248172e-04 9.98743478e-01]\n [6.18283350e-04 6.22450468e-04 9.98759266e-01]\n [6.18282640e-04 6.23597953e-04 9.98758119e-01]\n [6.18282640e-04 6.23597953e-04 9.98758119e-01]\n [9.98693915e-01 6.49764689e-04 6.56320163e-04]\n [6.18282640e-04 6.23597953e-04 9.98758119e-01]\n [9.98695110e-01 6.48569085e-04 6.56320948e-04]\n [9.98695110e-01 6.48569085e-04 6.56320948e-04]\n [9.98693915e-01 6.49764689e-04 6.56320163e-04]\n [9.98695110e-01 6.48569085e-04 6.56320948e-04]\n [6.18282640e-04 6.23597953e-04 9.98758119e-01]\n [6.18282640e-04 6.23597953e-04 9.98758119e-01]\n [6.52200159e-04 9.98681881e-01 6.65919282e-04]\n [6.18283350e-04 6.22450468e-04 9.98759266e-01]\n [6.18283350e-04 6.22450468e-04 9.98759266e-01]\n [9.98695110e-01 6.48569085e-04 6.56320948e-04]\n [9.98693915e-01 6.49764689e-04 6.56320163e-04]\n [9.98695110e-01 6.48569085e-04 6.56320948e-04]\n [9.98695110e-01 6.48569085e-04 6.56320948e-04]\n [6.18282640e-04 6.23597953e-04 9.98758119e-01]\n [6.52200159e-04 9.98681881e-01 6.65919282e-04]\n [6.18282640e-04 6.23597953e-04 9.98758119e-01]\n [6.18282640e-04 6.23597953e-04 9.98758119e-01]\n [6.18273576e-04 6.38248172e-04 9.98743478e-01]\n [9.98693915e-01 6.49764689e-04 6.56320163e-04]\n [6.52200159e-04 9.98681881e-01 6.65919282e-04]\n [6.52200159e-04 9.98681881e-01 6.65919282e-04]\n [9.98695110e-01 6.48569085e-04 6.56320948e-04]\n [6.52200159e-04 9.98681881e-01 6.65919282e-04]\n [9.98693915e-01 6.49764689e-04 6.56320163e-04]\n [9.98695110e-01 6.48569085e-04 6.56320948e-04]\n [9.98695110e-01 6.48569085e-04 6.56320948e-04]\n [6.52200159e-04 9.98681881e-01 6.65919282e-04]\n [9.98693915e-01 6.49764689e-04 6.56320163e-04]\n [9.98693915e-01 6.49764689e-04 6.56320163e-04]\n [9.98695110e-01 6.48569085e-04 6.56320948e-04]\n [6.18283350e-04 6.22450468e-04 9.98759266e-01]\n [6.52200159e-04 9.98681881e-01 6.65919282e-04]\n [6.52200159e-04 9.98681881e-01 6.65919282e-04]]"
        },
        "random_forest": {
          "accuracy": 1.0,
          "log_loss": 0.028180794713455214,
          "predictions": "[0 2 2 0 2 2 2 1 2 1 1 1 2 2 0 2 0 2 2 0 1 2 2 0 2 2 0 2 0 2 2 1 2 1 2 2 1\n 2 2 2 0 0 1 2 0 2 0 1 0 2 2 2 1 2 1 2 1 0 2 0 0 0 0 2 2 2 2 1 2 2 0 0 1 0\n 1 2 1 2 0 0 2 2 1 0 0 2 2 0 1 0 0 0 2 2 1 2 2 2 0 1 2 0 2 2 2 0 2 2 1 1 0\n 2 0 2 1 2 2 2 2 0 2 0 0 0 0 2 2 1 2 2 0 0 0 0 2 1 2 2 2 0 1 1 0 1 0 0 0 1\n 0 0 0 2 1 1]",
          "probabilities": "[[0.88220821 0.0335545  0.08423729]\n [0.00253731 0.         0.99746269]\n [0.04659256 0.01448053 0.93892691]\n [0.9112649  0.02769951 0.0610356 ]\n [0.03487495 0.01528571 0.94983933]\n [0.01       0.02       0.97      ]\n [0.01       0.02       0.97      ]\n [0.01475524 0.94953047 0.03571429]\n [0.         0.         1.        ]\n [0.         0.99       0.01      ]\n [0.01       0.95428571 0.03571429]\n [0.02496806 0.95503194 0.02      ]\n [0.00311927 0.01688073 0.98      ]\n [0.00576271 0.         0.99423729]\n [0.98311927 0.00688073 0.01      ]\n [0.01576271 0.         0.98423729]\n [0.99       0.         0.01      ]\n [0.0180315  0.         0.9819685 ]\n [0.00475524 0.01       0.98524476]\n [0.98       0.02       0.        ]\n [0.05439907 0.89299567 0.05260526]\n [0.         0.         1.        ]\n [0.00475524 0.02371229 0.97153247]\n [0.97576271 0.02       0.00423729]\n [0.         0.         1.        ]\n [0.         0.01       0.99      ]\n [0.96972776 0.02027224 0.01      ]\n [0.01       0.01       0.98      ]\n [0.98       0.01       0.01      ]\n [0.01       0.         0.99      ]\n [0.03475524 0.00524476 0.96      ]\n [0.0164455  0.9835545  0.        ]\n [0.         0.0275     0.9725    ]\n [0.         0.976      0.024     ]\n [0.         0.         1.        ]\n [0.01911224 0.         0.98088776]\n [0.00153499 0.99846501 0.        ]\n [0.         0.01       0.99      ]\n [0.0080315  0.01628571 0.97568279]\n [0.00774522 0.02255977 0.96969501]\n [1.         0.         0.        ]\n [0.96       0.02       0.02      ]\n [0.01       0.99       0.        ]\n [0.         0.         1.        ]\n [0.9252732  0.06524476 0.00948204]\n [0.         0.         1.        ]\n [0.98       0.01       0.01      ]\n [0.01475524 0.98124476 0.004     ]\n [0.86808732 0.05791268 0.074     ]\n [0.01       0.01       0.98      ]\n [0.01082985 0.01448053 0.97468962]\n [0.00731183 0.         0.99268817]\n [0.         0.99       0.01      ]\n [0.         0.         1.        ]\n [0.         0.9475     0.0525    ]\n [0.00576271 0.01       0.98423729]\n [0.0064455  0.9835545  0.01      ]\n [0.99       0.01       0.        ]\n [0.02307454 0.006      0.97092546]\n [0.98787451 0.00524476 0.00688073]\n [0.9864455  0.0135545  0.        ]\n [1.         0.         0.        ]\n [0.99       0.01       0.        ]\n [0.         0.         1.        ]\n [0.00576271 0.006      0.98823729]\n [0.         0.01       0.99      ]\n [0.02729256 0.02524476 0.94746269]\n [0.01376731 0.95623269 0.03      ]\n [0.         0.03       0.97      ]\n [0.02       0.09778571 0.88221429]\n [0.9364455  0.0335545  0.03      ]\n [0.993309   0.00547445 0.00121655]\n [0.03259386 0.96740614 0.        ]\n [0.99671053 0.         0.00328947]\n [0.01972234 0.90370158 0.07657609]\n [0.         0.         1.        ]\n [0.01146577 0.97124476 0.01728947]\n [0.02247324 0.01       0.96752676]\n [0.99       0.006      0.004     ]\n [0.94       0.0375     0.0225    ]\n [0.02406274 0.006      0.96993726]\n [0.         0.         1.        ]\n [0.05314405 0.84853777 0.09831818]\n [0.9980315  0.         0.0019685 ]\n [0.95576271 0.01       0.03423729]\n [0.00184783 0.         0.99815217]\n [0.02576271 0.01       0.96423729]\n [0.99       0.         0.01      ]\n [0.04120074 0.94879926 0.01      ]\n [0.99       0.01       0.        ]\n [0.99475524 0.00524476 0.        ]\n [0.96576271 0.01       0.02423729]\n [0.04278674 0.04521229 0.91200097]\n [0.         0.         1.        ]\n [0.         1.         0.        ]\n [0.         0.         1.        ]\n [0.         0.0175     0.9825    ]\n [0.         0.02       0.98      ]\n [0.97949727 0.00524476 0.01525798]\n [0.02639707 0.88788865 0.08571429]\n [0.00279835 0.01848053 0.97872111]\n [0.84808732 0.05791268 0.094     ]\n [0.         0.01       0.99      ]\n [0.01       0.01       0.98      ]\n [0.         0.00882353 0.99117647]\n [0.9980315  0.         0.0019685 ]\n [0.01       0.01292308 0.97707692]\n [0.         0.0335     0.9665    ]\n [0.01465425 0.96067908 0.02466667]\n [0.0064455  0.9935545  0.        ]\n [0.99       0.01       0.        ]\n [0.00475524 0.00428571 0.99095904]\n [0.9880315  0.01       0.0019685 ]\n [0.         0.         1.        ]\n [0.00475524 0.98524476 0.01      ]\n [0.02852828 0.         0.97147172]\n [0.         0.         1.        ]\n [0.00279835 0.00617284 0.99102881]\n [0.00871795 0.01       0.98128205]\n [0.97       0.03       0.        ]\n [0.00576271 0.         0.99423729]\n [0.98949727 0.         0.01050273]\n [0.92685732 0.03567999 0.03746269]\n [0.97972776 0.02027224 0.        ]\n [0.9029609  0.04980467 0.04723443]\n [0.         0.017      0.983     ]\n [0.         0.         1.        ]\n [0.05646958 0.86581614 0.07771429]\n [0.00671053 0.01230769 0.98098178]\n [0.01       0.02       0.97      ]\n [0.97573476 0.01705198 0.00721326]\n [0.99       0.01       0.        ]\n [0.98981033 0.         0.01018967]\n [0.9864455  0.0135545  0.        ]\n [0.         0.         1.        ]\n [0.         0.98285714 0.01714286]\n [0.00871795 0.         0.99128205]\n [0.01       0.         0.99      ]\n [0.01207912 0.01       0.97792088]\n [0.99671053 0.         0.00328947]\n [0.03       0.96018182 0.00981818]\n [0.00376731 0.99623269 0.        ]\n [0.99534332 0.00268817 0.0019685 ]\n [0.         0.995      0.005     ]\n [0.97880286 0.01524476 0.00595238]\n [0.99       0.01       0.        ]\n [0.98731183 0.00868817 0.004     ]\n [0.         0.992      0.008     ]\n [0.99       0.01       0.        ]\n [1.         0.         0.        ]\n [0.9137619  0.05316886 0.03306923]\n [0.02307454 0.00428571 0.97263975]\n [0.01       0.98       0.01      ]\n [0.05462366 0.8775117  0.06786464]]"
        },
        "logistic_regression": {
          "accuracy": 1.0,
          "log_loss": 0.0035368560315809676,
          "predictions": "[0 2 2 0 2 2 2 1 2 1 1 1 2 2 0 2 0 2 2 0 1 2 2 0 2 2 0 2 0 2 2 1 2 1 2 2 1\n 2 2 2 0 0 1 2 0 2 0 1 0 2 2 2 1 2 1 2 1 0 2 0 0 0 0 2 2 2 2 1 2 2 0 0 1 0\n 1 2 1 2 0 0 2 2 1 0 0 2 2 0 1 0 0 0 2 2 1 2 2 2 0 1 2 0 2 2 2 0 2 2 1 1 0\n 2 0 2 1 2 2 2 2 0 2 0 0 0 0 2 2 1 2 2 0 0 0 0 2 1 2 2 2 0 1 1 0 1 0 0 0 1\n 0 0 0 2 1 1]",
          "probabilities": "[[9.93425530e-01 4.26490773e-03 2.30956230e-03]\n [3.60447087e-06 5.54465110e-05 9.99940949e-01]\n [1.91374580e-03 5.95701300e-03 9.92129241e-01]\n [9.98824724e-01 9.33867546e-04 2.41408319e-04]\n [6.83378180e-05 3.31372319e-04 9.99600290e-01]\n [8.72877517e-04 4.08967620e-03 9.95037446e-01]\n [7.75292834e-04 2.62450107e-03 9.96600206e-01]\n [2.49402649e-03 9.93777388e-01 3.72858562e-03]\n [7.36841698e-05 4.22738784e-04 9.99503577e-01]\n [2.99403875e-03 9.94957570e-01 2.04839129e-03]\n [3.39100305e-03 9.93658229e-01 2.95076752e-03]\n [2.88010572e-03 9.93756558e-01 3.36333669e-03]\n [6.91599867e-04 3.69986307e-03 9.95608537e-01]\n [8.05143240e-06 2.26112044e-04 9.99765837e-01]\n [9.97125355e-01 2.30940287e-03 5.65242337e-04]\n [9.78678708e-05 3.55675530e-04 9.99546457e-01]\n [9.96818110e-01 2.64532477e-03 5.36564875e-04]\n [4.33117247e-03 6.87959277e-03 9.88789235e-01]\n [5.97565970e-04 4.65756040e-03 9.94744874e-01]\n [9.89200564e-01 9.50336011e-03 1.29607582e-03]\n [4.66279321e-03 9.88407809e-01 6.92939765e-03]\n [4.09761020e-07 1.28146260e-05 9.99986776e-01]\n [6.30838965e-04 3.14106012e-03 9.96228101e-01]\n [9.96938580e-01 2.60271829e-03 4.58701361e-04]\n [2.36504963e-04 9.55887881e-04 9.98807607e-01]\n [1.56377614e-03 5.47043671e-03 9.92965787e-01]\n [9.99353772e-01 5.69201861e-04 7.70264281e-05]\n [4.26841674e-06 3.41489364e-05 9.99961583e-01]\n [9.92478558e-01 6.72088565e-03 8.00556736e-04]\n [4.29461088e-06 1.39627843e-04 9.99856078e-01]\n [1.05834000e-03 3.47025725e-03 9.95471403e-01]\n [1.74698943e-03 9.95663393e-01 2.58961744e-03]\n [1.25483979e-03 2.08549328e-03 9.96659667e-01]\n [1.06241852e-03 9.96244735e-01 2.69284608e-03]\n [1.35444237e-04 1.91169752e-04 9.99673386e-01]\n [3.54642741e-05 1.55216154e-04 9.99809320e-01]\n [1.14489857e-03 9.97632061e-01 1.22304043e-03]\n [4.57388608e-05 7.67327592e-05 9.99877528e-01]\n [1.97926812e-03 7.60662073e-03 9.90414111e-01]\n [1.36505051e-03 4.52163495e-03 9.94113315e-01]\n [9.99990892e-01 8.89289114e-06 2.14838125e-07]\n [9.99996925e-01 2.92860292e-06 1.46729585e-07]\n [7.56097323e-05 9.99783391e-01 1.40999193e-04]\n [6.27455637e-05 2.46449761e-04 9.99690805e-01]\n [9.94568415e-01 4.77962673e-03 6.51958521e-04]\n [3.68586305e-05 4.06592943e-03 9.95897212e-01]\n [9.94165238e-01 5.75110565e-03 8.36561196e-05]\n [1.62337464e-03 9.96950717e-01 1.42590793e-03]\n [9.93911368e-01 4.56881252e-03 1.51981929e-03]\n [5.21320143e-07 1.09267840e-05 9.99988552e-01]\n [3.69081220e-03 4.82725656e-03 9.91481931e-01]\n [9.76866720e-06 1.70134063e-04 9.99820097e-01]\n [8.33389384e-04 9.97943247e-01 1.22336394e-03]\n [3.52409320e-08 1.22315011e-06 9.99998742e-01]\n [1.79826948e-03 9.96507654e-01 1.69407625e-03]\n [4.66575765e-05 3.65687018e-04 9.99587655e-01]\n [4.22226331e-03 9.92247664e-01 3.53007225e-03]\n [9.96447086e-01 2.28441281e-03 1.26850119e-03]\n [1.94286130e-03 3.62842834e-03 9.94428710e-01]\n [9.99974004e-01 2.23364452e-05 3.65988251e-06]\n [9.96447731e-01 2.83232813e-03 7.19940686e-04]\n [9.96754112e-01 2.74132989e-03 5.04558234e-04]\n [9.99110255e-01 8.07361791e-04 8.23827321e-05]\n [2.52993931e-06 3.46698461e-05 9.99962800e-01]\n [9.29152318e-04 3.39969035e-03 9.95671157e-01]\n [2.98042440e-03 1.57415447e-02 9.81278031e-01]\n [1.59108534e-03 4.32616486e-03 9.94082750e-01]\n [2.77363762e-03 9.92297436e-01 4.92892669e-03]\n [1.68797725e-03 7.69843750e-03 9.90613585e-01]\n [1.50026359e-03 7.25944613e-03 9.91240290e-01]\n [9.92088844e-01 7.54325158e-03 3.67903947e-04]\n [9.99663272e-01 3.09854159e-04 2.68740337e-05]\n [7.97445638e-04 9.97290481e-01 1.91207333e-03]\n [9.96101344e-01 3.14352010e-03 7.55135796e-04]\n [3.23855362e-03 9.93635221e-01 3.12622497e-03]\n [1.82428424e-03 5.50601405e-03 9.92669702e-01]\n [4.50275664e-03 9.92108447e-01 3.38879639e-03]\n [1.65517374e-03 3.56853847e-03 9.94776288e-01]\n [9.99060306e-01 8.36641457e-04 1.03052923e-04]\n [9.99932009e-01 6.75199493e-05 4.71055203e-07]\n [3.57150692e-05 3.40659398e-04 9.99623626e-01]\n [7.99254372e-05 6.21369238e-04 9.99298705e-01]\n [1.77720904e-03 9.94047198e-01 4.17559254e-03]\n [9.99990041e-01 9.86718454e-06 9.16271857e-08]\n [9.94247939e-01 4.90491979e-03 8.47140864e-04]\n [1.86657047e-06 1.08873002e-04 9.99889260e-01]\n [7.10473268e-04 5.39564993e-03 9.93893877e-01]\n [9.99410224e-01 5.62141091e-04 2.76345767e-05]\n [4.62340083e-04 9.91204565e-01 8.33309540e-03]\n [9.94036783e-01 5.04866764e-03 9.14549802e-04]\n [9.99994833e-01 5.03607190e-06 1.30916462e-07]\n [9.97718874e-01 1.83852227e-03 4.42603367e-04]\n [6.20935326e-04 3.24207111e-03 9.96136994e-01]\n [1.52056600e-04 2.46362133e-04 9.99601581e-01]\n [4.38546563e-03 9.92519152e-01 3.09538231e-03]\n [1.22170105e-03 4.92512709e-03 9.93853172e-01]\n [1.35054319e-04 5.62165341e-04 9.99302780e-01]\n [1.47758671e-03 3.85424721e-03 9.94668166e-01]\n [9.98073841e-01 1.54014545e-03 3.86013765e-04]\n [3.13355541e-03 9.93660220e-01 3.20622492e-03]\n [2.69390813e-03 5.94070132e-03 9.91365391e-01]\n [9.96154334e-01 2.84774548e-03 9.97920543e-04]\n [2.27351376e-06 1.01548345e-04 9.99896178e-01]\n [1.39585829e-07 2.73722391e-06 9.99997123e-01]\n [3.08163632e-08 1.46786096e-06 9.99998501e-01]\n [9.97146916e-01 2.45730440e-03 3.95779836e-04]\n [6.61237865e-04 1.80677470e-03 9.97531987e-01]\n [3.43177673e-08 6.38126486e-07 9.99999328e-01]\n [3.53439412e-03 9.93775341e-01 2.69026518e-03]\n [1.56193729e-03 9.96152964e-01 2.28509897e-03]\n [9.99613313e-01 3.64964751e-04 2.17218356e-05]\n [1.88367998e-03 6.01049333e-03 9.92105827e-01]\n [9.98862377e-01 9.33200700e-04 2.04422202e-04]\n [4.14275172e-05 4.77806930e-04 9.99480766e-01]\n [1.96117058e-03 9.95182401e-01 2.85642866e-03]\n [2.02351046e-04 1.06492992e-03 9.98732719e-01]\n [9.71973459e-05 1.98827354e-04 9.99703975e-01]\n [1.77913197e-03 2.86941682e-03 9.95351451e-01]\n [3.52676614e-04 1.09254367e-03 9.98554780e-01]\n [9.93192986e-01 5.89897548e-03 9.08038704e-04]\n [1.13599052e-04 4.75591515e-04 9.99410809e-01]\n [9.99957496e-01 3.97977510e-05 2.70585241e-06]\n [9.95531004e-01 3.06371836e-03 1.40527773e-03]\n [9.98865243e-01 1.00739182e-03 1.27364806e-04]\n [9.93678495e-01 4.30179192e-03 2.01971335e-03]\n [1.82971777e-03 4.23716029e-03 9.93933122e-01]\n [8.28319212e-06 5.97146569e-05 9.99932002e-01]\n [1.16949019e-03 9.96556590e-01 2.27392016e-03]\n [1.71076592e-04 5.62160303e-04 9.99266763e-01]\n [1.95915258e-03 5.20962692e-03 9.92831221e-01]\n [9.97401759e-01 2.04215527e-03 5.56085606e-04]\n [9.95645649e-01 3.54134491e-03 8.13006282e-04]\n [9.99979744e-01 1.98622066e-05 3.94027204e-07]\n [9.95078144e-01 4.04975886e-03 8.72097484e-04]\n [3.17324469e-04 2.18507977e-03 9.97497596e-01]\n [1.06990470e-03 9.97898686e-01 1.03140977e-03]\n [4.67311557e-04 1.73998726e-03 9.97792701e-01]\n [1.23186808e-04 3.32504000e-04 9.99544309e-01]\n [6.27946812e-04 1.39495093e-03 9.97977102e-01]\n [9.96578217e-01 2.23111259e-03 1.19067008e-03]\n [4.12652673e-03 9.93653950e-01 2.21952306e-03]\n [1.07821855e-03 9.97353852e-01 1.56792985e-03]\n [9.98733709e-01 9.70941572e-04 2.95349015e-04]\n [4.16114462e-03 9.93429146e-01 2.40970942e-03]\n [9.99413353e-01 5.55684058e-04 3.09630743e-05]\n [9.99992735e-01 7.12504747e-06 1.40442310e-07]\n [9.97468782e-01 2.00610728e-03 5.25110323e-04]\n [6.21248635e-04 9.98856962e-01 5.21789285e-04]\n [9.98995845e-01 9.29578802e-04 7.45766310e-05]\n [9.95176757e-01 3.93061467e-03 8.92628674e-04]\n [9.97575719e-01 1.64371552e-03 7.80565536e-04]\n [2.37200842e-03 6.84830646e-03 9.90779685e-01]\n [8.25352567e-04 9.97745017e-01 1.42963003e-03]\n [1.51249397e-03 9.97059906e-01 1.42760026e-03]]"
        },
        "ensemble": {
          "accuracy": 1.0,
          "log_loss": 0.010718459009873949,
          "predictions": "[0 2 2 0 2 2 2 1 2 1 1 1 2 2 0 2 0 2 2 0 1 2 2 0 2 2 0 2 0 2 2 1 2 1 2 2 1\n 2 2 2 0 0 1 2 0 2 0 1 0 2 2 2 1 2 1 2 1 0 2 0 0 0 0 2 2 2 2 1 2 2 0 0 1 0\n 1 2 1 2 0 0 2 2 1 0 0 2 2 0 1 0 0 0 2 2 1 2 2 2 0 1 2 0 2 2 2 0 2 2 1 1 0\n 2 0 2 1 2 2 2 2 0 2 0 0 0 0 2 2 1 2 2 0 0 0 0 2 1 2 2 2 0 1 1 0 1 0 0 0 1\n 0 0 0 2 1 1]",
          "probabilities": "[[0.96573164 0.01102815 0.0232402 ]\n [0.00186724 0.0012603  0.99687245]\n [0.01335862 0.00634851 0.98029288]\n [0.97433484 0.00874249 0.01692268]\n [0.00996787 0.0051422  0.98488991]\n [0.00395802 0.00727677 0.98876521]\n [0.00393366 0.00690191 0.98916445]\n [0.0060458  0.9823008  0.01165339]\n [0.00125047 0.00134362 0.9974059 ]\n [0.00248206 0.99272445 0.00479349]\n [0.0050813  0.98347104 0.01144766]\n [0.00869121 0.98367971 0.00762909]\n [0.00219249 0.00640512 0.99140239]\n [0.00268249 0.00131088 0.99600663]\n [0.99189284 0.00387241 0.00423475]\n [0.0051972  0.00132685 0.99347594]\n [0.99351541 0.00224701 0.00423756]\n [0.00682269 0.00296337 0.99021394]\n [0.002578   0.00491874 0.99250325]\n [0.98913335 0.00894919 0.00191747]\n [0.01690884 0.9661361  0.01695507]\n [0.00123214 0.00124528 0.99752257]\n [0.00258632 0.00796769 0.98944599]\n [0.99000325 0.00722932 0.00276742]\n [0.00129894 0.00149033 0.99721074]\n [0.00163076 0.00511757 0.99325166]\n [0.98908127 0.00679604 0.00412267]\n [0.00373314 0.00374214 0.99252473]\n [0.98994285 0.00575352 0.00430363]\n [0.00373309 0.00128135 0.99498556]\n [0.01019319 0.0034331  0.9863737 ]\n [0.00627126 0.99129926 0.00242948]\n [0.00155352 0.00864774 0.98979876]\n [0.00198715 0.98957044 0.0084424 ]\n [0.00126591 0.00128573 0.99744835]\n [0.00601898 0.00127674 0.99270427]\n [0.00238571 0.99554527 0.00206903]\n [0.00124349 0.00375712 0.99499938]\n [0.0037425  0.00722445 0.98903307]\n [0.00350961 0.00800948 0.98848092]\n [0.9968186  0.00158796 0.00159343]\n [0.98683092 0.00657561 0.00659348]\n [0.00423038 0.99397543 0.00179419]\n [0.00124774 0.00129955 0.9974527 ]\n [0.97678833 0.01908474 0.00412692]\n [0.00124123 0.00226292 0.99649584]\n [0.99037003 0.00550555 0.00412443]\n [0.00581805 0.99105442 0.00312754]\n [0.96233288 0.01719372 0.0204734 ]\n [0.00373218 0.00374066 0.99252714]\n [0.00486221 0.00606607 0.98907173]\n [0.00306243 0.00128461 0.99565294]\n [0.00192873 0.99300948 0.00506179]\n [0.00123202 0.00124675 0.99752122]\n [0.00218305 0.98247575 0.01534119]\n [0.00268436 0.00383786 0.99347777]\n [0.00438849 0.9904598  0.0051517 ]\n [0.99345046 0.00463893 0.00191061]\n [0.00749414 0.00366267 0.9888432 ]\n [0.99377301 0.00290245 0.00332451]\n [0.99255203 0.00566448 0.0017835 ]\n [0.99602722 0.00225316 0.00171962]\n [0.99409552 0.00428043 0.00162405]\n [0.00123268 0.0012466  0.9975207 ]\n [0.00291279 0.0035969  0.9934903 ]\n [0.00198492 0.00768534 0.99032972]\n [0.00846074 0.00863971 0.98289955]\n [0.00585837 0.98362732 0.01051431]\n [0.00165398 0.01067664 0.98766938]\n [0.00661484 0.02752127 0.9658639 ]\n [0.97894891 0.01186577 0.00918532]\n [0.995071   0.00302472 0.00190427]\n [0.01006356 0.98769515 0.00224129]\n [0.99504166 0.00235371 0.00260463]\n [0.00746336 0.97082899 0.02170765]\n [0.00169591 0.00262228 0.99568183]\n [0.0057096  0.98735581 0.0069346 ]\n [0.00727189 0.0046477  0.98808042]\n [0.99407751 0.00329331 0.00262919]\n [0.98181093 0.01097052 0.00721854]\n [0.00725663 0.00283161 0.98991176]\n [0.00125203 0.00139327 0.99735468]\n [0.01549486 0.95711197 0.02739317]\n [0.99633331 0.00158111 0.00208557]\n [0.98433435 0.0053011  0.01036455]\n [0.00169444 0.00127366 0.9970319 ]\n [0.00785807 0.00510889 0.98703305]\n [0.99418049 0.00171918 0.00410033]\n [0.01213132 0.98152234 0.00634635]\n [0.9928324  0.00533546 0.00183212]\n [0.9955084  0.00289819 0.00159341]\n [0.9876921  0.00453445 0.00777345]\n [0.01209171 0.01336794 0.97454034]\n [0.00127007 0.00129927 0.99743066]\n [0.00281957 0.99463571 0.00254472]\n [0.00153741 0.00248331 0.99597927]\n [0.00126582 0.00575347 0.9929807 ]\n [0.00160143 0.00720564 0.99119292]\n [0.99123147 0.00326406 0.00550448]\n [0.00910421 0.96689647 0.02399931]\n [0.00261284 0.00736528 0.99002188]\n [0.95789362 0.01676346 0.02534293]\n [0.00123258 0.00377183 0.99499558]\n [0.00373209 0.00373862 0.99252928]\n [0.00123206 0.00344418 0.99532374]\n [0.99563329 0.00218216 0.00218455]\n [0.00389736 0.0049204  0.99118223]\n [0.00123206 0.00961309 0.98915483]\n [0.00627633 0.98509619 0.00862749]\n [0.003725   0.99392165 0.00235335]\n [0.99421865 0.00418257 0.00159878]\n [0.00289172 0.00382608 0.9932822 ]\n [0.99356216 0.00430113 0.00213671]\n [0.00124239 0.00136153 0.99739607]\n [0.00339484 0.99162779 0.00497738]\n [0.00863997 0.001879   0.98948103]\n [0.00125635 0.00128883 0.99745483]\n [0.0023764  0.00350404 0.99411957]\n [0.00350745 0.00402749 0.99246505]\n [0.98763145 0.0105481  0.00182046]\n [0.00270887 0.00137446 0.99591668]\n [0.99418539 0.00158477 0.00422984]\n [0.97742029 0.01125922 0.01132048]\n [0.99145913 0.00690559 0.00163526]\n [0.97098778 0.01510526 0.01390696]\n [0.0016895  0.00654289 0.99176762]\n [0.0012341  0.00125701 0.99750887]\n [0.01611364 0.96213811 0.02174825]\n [0.00296021 0.00446883 0.99257098]\n [0.00422956 0.00756238 0.98820806]\n [0.99010583 0.00634835 0.00354581]\n [0.9932431  0.00496021 0.00179669]\n [0.99427545 0.00158361 0.00414094]\n [0.99220331 0.00598527 0.00181141]\n [0.00131136 0.00178975 0.9968989 ]\n [0.00199498 0.99168622 0.0063188 ]\n [0.00352833 0.00168144 0.99479023]\n [0.00376287 0.00131673 0.99492041]\n [0.00463413 0.00445022 0.99091565]\n [0.99513751 0.00214911 0.00271338]\n [0.01025311 0.98495707 0.00478981]\n [0.00292286 0.99492622 0.00215092]\n [0.99534798 0.00248255 0.00216948]\n [0.00275768 0.9936137  0.00362862]\n [0.99137494 0.00553585 0.00308922]\n [0.99431906 0.00408752 0.00159341]\n [0.99302836 0.00424692 0.00272473]\n [0.00187852 0.99422016 0.00390132]\n [0.99408065 0.00430727 0.00161209]\n [0.99561589 0.00255747 0.00182663]\n [0.97465237 0.01528173 0.01006591]\n [0.00760147 0.00403049 0.98836805]\n [0.00442201 0.99094615 0.00463183]\n [0.01574978 0.9651639  0.01908633]]"
        },
        "xgboost_calibrated": {
          "accuracy": 1.0,
          "log_loss": 8.377435439819955e-06,
          "predictions": "[0 2 2 0 2 2 2 1 2 1 1 1 2 2 0 2 0 2 2 0 1 2 2 0 2 2 0 2 0 2 2 1 2 1 2 2 1\n 2 2 2 0 0 1 2 0 2 0 1 0 2 2 2 1 2 1 2 1 0 2 0 0 0 0 2 2 2 2 1 2 2 0 0 1 0\n 1 2 1 2 0 0 2 2 1 0 0 2 2 0 1 0 0 0 2 2 1 2 2 2 0 1 2 0 2 2 2 0 2 2 1 1 0\n 2 0 2 1 2 2 2 2 0 2 0 0 0 0 2 2 1 2 2 0 0 0 0 2 1 2 2 2 0 1 1 0 1 0 0 0 1\n 0 0 0 2 1 1]",
          "probabilities": "[[1.00000000e+00 0.00000000e+00 0.00000000e+00]\n [0.00000000e+00 0.00000000e+00 1.00000000e+00]\n [0.00000000e+00 0.00000000e+00 1.00000000e+00]\n [1.00000000e+00 0.00000000e+00 0.00000000e+00]\n [0.00000000e+00 0.00000000e+00 1.00000000e+00]\n [0.00000000e+00 0.00000000e+00 1.00000000e+00]\n [0.00000000e+00 0.00000000e+00 1.00000000e+00]\n [0.00000000e+00 1.00000000e+00 0.00000000e+00]\n [0.00000000e+00 0.00000000e+00 1.00000000e+00]\n [0.00000000e+00 1.00000000e+00 0.00000000e+00]\n [0.00000000e+00 1.00000000e+00 0.00000000e+00]\n [0.00000000e+00 1.00000000e+00 0.00000000e+00]\n [0.00000000e+00 0.00000000e+00 1.00000000e+00]\n [0.00000000e+00 0.00000000e+00 1.00000000e+00]\n [1.00000000e+00 0.00000000e+00 0.00000000e+00]\n [0.00000000e+00 0.00000000e+00 1.00000000e+00]\n [1.00000000e+00 0.00000000e+00 0.00000000e+00]\n [0.00000000e+00 0.00000000e+00 1.00000000e+00]\n [0.00000000e+00 0.00000000e+00 1.00000000e+00]\n [1.00000000e+00 0.00000000e+00 0.00000000e+00]\n [5.31221760e-04 9.99109984e-01 3.58794702e-04]\n [0.00000000e+00 0.00000000e+00 1.00000000e+00]\n [0.00000000e+00 0.00000000e+00 1.00000000e+00]\n [1.00000000e+00 0.00000000e+00 0.00000000e+00]\n [0.00000000e+00 0.00000000e+00 1.00000000e+00]\n [0.00000000e+00 0.00000000e+00 1.00000000e+00]\n [1.00000000e+00 0.00000000e+00 0.00000000e+00]\n [0.00000000e+00 0.00000000e+00 1.00000000e+00]\n [1.00000000e+00 0.00000000e+00 0.00000000e+00]\n [0.00000000e+00 0.00000000e+00 1.00000000e+00]\n [0.00000000e+00 0.00000000e+00 1.00000000e+00]\n [0.00000000e+00 1.00000000e+00 0.00000000e+00]\n [0.00000000e+00 0.00000000e+00 1.00000000e+00]\n [0.00000000e+00 1.00000000e+00 0.00000000e+00]\n [0.00000000e+00 0.00000000e+00 1.00000000e+00]\n [0.00000000e+00 0.00000000e+00 1.00000000e+00]\n [0.00000000e+00 1.00000000e+00 0.00000000e+00]\n [0.00000000e+00 0.00000000e+00 1.00000000e+00]\n [0.00000000e+00 0.00000000e+00 1.00000000e+00]\n [0.00000000e+00 0.00000000e+00 1.00000000e+00]\n [1.00000000e+00 0.00000000e+00 0.00000000e+00]\n [1.00000000e+00 0.00000000e+00 0.00000000e+00]\n [0.00000000e+00 1.00000000e+00 0.00000000e+00]\n [0.00000000e+00 0.00000000e+00 1.00000000e+00]\n [1.00000000e+00 0.00000000e+00 0.00000000e+00]\n [0.00000000e+00 0.00000000e+00 1.00000000e+00]\n [1.00000000e+00 0.00000000e+00 0.00000000e+00]\n [0.00000000e+00 1.00000000e+00 0.00000000e+00]\n [1.00000000e+00 0.00000000e+00 0.00000000e+00]\n [0.00000000e+00 0.00000000e+00 1.00000000e+00]\n [0.00000000e+00 0.00000000e+00 1.00000000e+00]\n [0.00000000e+00 0.00000000e+00 1.00000000e+00]\n [0.00000000e+00 9.99601112e-01 3.98888493e-04]\n [0.00000000e+00 0.00000000e+00 1.00000000e+00]\n [0.00000000e+00 1.00000000e+00 0.00000000e+00]\n [0.00000000e+00 0.00000000e+00 1.00000000e+00]\n [0.00000000e+00 1.00000000e+00 0.00000000e+00]\n [1.00000000e+00 0.00000000e+00 0.00000000e+00]\n [0.00000000e+00 0.00000000e+00 1.00000000e+00]\n [1.00000000e+00 0.00000000e+00 0.00000000e+00]\n [1.00000000e+00 0.00000000e+00 0.00000000e+00]\n [1.00000000e+00 0.00000000e+00 0.00000000e+00]\n [1.00000000e+00 0.00000000e+00 0.00000000e+00]\n [0.00000000e+00 0.00000000e+00 1.00000000e+00]\n [0.00000000e+00 0.00000000e+00 1.00000000e+00]\n [0.00000000e+00 0.00000000e+00 1.00000000e+00]\n [0.00000000e+00 0.00000000e+00 1.00000000e+00]\n [0.00000000e+00 1.00000000e+00 0.00000000e+00]\n [0.00000000e+00 0.00000000e+00 1.00000000e+00]\n [0.00000000e+00 0.00000000e+00 1.00000000e+00]\n [1.00000000e+00 0.00000000e+00 0.00000000e+00]\n [1.00000000e+00 0.00000000e+00 0.00000000e+00]\n [0.00000000e+00 1.00000000e+00 0.00000000e+00]\n [1.00000000e+00 0.00000000e+00 0.00000000e+00]\n [0.00000000e+00 1.00000000e+00 0.00000000e+00]\n [0.00000000e+00 0.00000000e+00 1.00000000e+00]\n [0.00000000e+00 1.00000000e+00 0.00000000e+00]\n [0.00000000e+00 0.00000000e+00 1.00000000e+00]\n [1.00000000e+00 0.00000000e+00 0.00000000e+00]\n [1.00000000e+00 0.00000000e+00 0.00000000e+00]\n [0.00000000e+00 0.00000000e+00 1.00000000e+00]\n [0.00000000e+00 0.00000000e+00 1.00000000e+00]\n [0.00000000e+00 1.00000000e+00 0.00000000e+00]\n [1.00000000e+00 0.00000000e+00 0.00000000e+00]\n [1.00000000e+00 0.00000000e+00 0.00000000e+00]\n [0.00000000e+00 0.00000000e+00 1.00000000e+00]\n [0.00000000e+00 0.00000000e+00 1.00000000e+00]\n [1.00000000e+00 0.00000000e+00 0.00000000e+00]\n [0.00000000e+00 1.00000000e+00 0.00000000e+00]\n [1.00000000e+00 0.00000000e+00 0.00000000e+00]\n [1.00000000e+00 0.00000000e+00 0.00000000e+00]\n [1.00000000e+00 0.00000000e+00 0.00000000e+00]\n [0.00000000e+00 0.00000000e+00 1.00000000e+00]\n [0.00000000e+00 0.00000000e+00 1.00000000e+00]\n [0.00000000e+00 1.00000000e+00 0.00000000e+00]\n [0.00000000e+00 0.00000000e+00 1.00000000e+00]\n [0.00000000e+00 0.00000000e+00 1.00000000e+00]\n [0.00000000e+00 0.00000000e+00 1.00000000e+00]\n [1.00000000e+00 0.00000000e+00 0.00000000e+00]\n [0.00000000e+00 1.00000000e+00 0.00000000e+00]\n [0.00000000e+00 0.00000000e+00 1.00000000e+00]\n [1.00000000e+00 0.00000000e+00 0.00000000e+00]\n [0.00000000e+00 0.00000000e+00 1.00000000e+00]\n [0.00000000e+00 0.00000000e+00 1.00000000e+00]\n [0.00000000e+00 0.00000000e+00 1.00000000e+00]\n [1.00000000e+00 0.00000000e+00 0.00000000e+00]\n [0.00000000e+00 0.00000000e+00 1.00000000e+00]\n [0.00000000e+00 0.00000000e+00 1.00000000e+00]\n [0.00000000e+00 1.00000000e+00 0.00000000e+00]\n [0.00000000e+00 1.00000000e+00 0.00000000e+00]\n [1.00000000e+00 0.00000000e+00 0.00000000e+00]\n [0.00000000e+00 0.00000000e+00 1.00000000e+00]\n [1.00000000e+00 0.00000000e+00 0.00000000e+00]\n [0.00000000e+00 0.00000000e+00 1.00000000e+00]\n [0.00000000e+00 1.00000000e+00 0.00000000e+00]\n [0.00000000e+00 0.00000000e+00 1.00000000e+00]\n [0.00000000e+00 0.00000000e+00 1.00000000e+00]\n [0.00000000e+00 0.00000000e+00 1.00000000e+00]\n [0.00000000e+00 0.00000000e+00 1.00000000e+00]\n [1.00000000e+00 0.00000000e+00 0.00000000e+00]\n [0.00000000e+00 0.00000000e+00 1.00000000e+00]\n [1.00000000e+00 0.00000000e+00 0.00000000e+00]\n [1.00000000e+00 0.00000000e+00 0.00000000e+00]\n [1.00000000e+00 0.00000000e+00 0.00000000e+00]\n [1.00000000e+00 0.00000000e+00 0.00000000e+00]\n [0.00000000e+00 0.00000000e+00 1.00000000e+00]\n [0.00000000e+00 0.00000000e+00 1.00000000e+00]\n [0.00000000e+00 1.00000000e+00 0.00000000e+00]\n [0.00000000e+00 0.00000000e+00 1.00000000e+00]\n [0.00000000e+00 0.00000000e+00 1.00000000e+00]\n [1.00000000e+00 0.00000000e+00 0.00000000e+00]\n [1.00000000e+00 0.00000000e+00 0.00000000e+00]\n [1.00000000e+00 0.00000000e+00 0.00000000e+00]\n [9.99999256e-01 7.44225499e-07 0.00000000e+00]\n [0.00000000e+00 0.00000000e+00 1.00000000e+00]\n [0.00000000e+00 1.00000000e+00 0.00000000e+00]\n [0.00000000e+00 0.00000000e+00 1.00000000e+00]\n [0.00000000e+00 0.00000000e+00 1.00000000e+00]\n [0.00000000e+00 0.00000000e+00 1.00000000e+00]\n [1.00000000e+00 0.00000000e+00 0.00000000e+00]\n [0.00000000e+00 1.00000000e+00 0.00000000e+00]\n [0.00000000e+00 1.00000000e+00 0.00000000e+00]\n [1.00000000e+00 0.00000000e+00 0.00000000e+00]\n [0.00000000e+00 1.00000000e+00 0.00000000e+00]\n [1.00000000e+00 0.00000000e+00 0.00000000e+00]\n [1.00000000e+00 0.00000000e+00 0.00000000e+00]\n [1.00000000e+00 0.00000000e+00 0.00000000e+00]\n [0.00000000e+00 1.00000000e+00 0.00000000e+00]\n [1.00000000e+00 0.00000000e+00 0.00000000e+00]\n [1.00000000e+00 0.00000000e+00 0.00000000e+00]\n [1.00000000e+00 0.00000000e+00 0.00000000e+00]\n [0.00000000e+00 0.00000000e+00 1.00000000e+00]\n [0.00000000e+00 1.00000000e+00 0.00000000e+00]\n [0.00000000e+00 1.00000000e+00 0.00000000e+00]]"
        },
        "lightgbm_calibrated": {
          "accuracy": 1.0,
          "log_loss": 2.2204460492503136e-16,
          "predictions": "[0 2 2 0 2 2 2 1 2 1 1 1 2 2 0 2 0 2 2 0 1 2 2 0 2 2 0 2 0 2 2 1 2 1 2 2 1\n 2 2 2 0 0 1 2 0 2 0 1 0 2 2 2 1 2 1 2 1 0 2 0 0 0 0 2 2 2 2 1 2 2 0 0 1 0\n 1 2 1 2 0 0 2 2 1 0 0 2 2 0 1 0 0 0 2 2 1 2 2 2 0 1 2 0 2 2 2 0 2 2 1 1 0\n 2 0 2 1 2 2 2 2 0 2 0 0 0 0 2 2 1 2 2 0 0 0 0 2 1 2 2 2 0 1 1 0 1 0 0 0 1\n 0 0 0 2 1 1]",
          "probabilities": "[[1. 0. 0.]\n [0. 0. 1.]\n [0. 0. 1.]\n [1. 0. 0.]\n [0. 0. 1.]\n [0. 0. 1.]\n [0. 0. 1.]\n [0. 1. 0.]\n [0. 0. 1.]\n [0. 1. 0.]\n [0. 1. 0.]\n [0. 1. 0.]\n [0. 0. 1.]\n [0. 0. 1.]\n [1. 0. 0.]\n [0. 0. 1.]\n [1. 0. 0.]\n [0. 0. 1.]\n [0. 0. 1.]\n [1. 0. 0.]\n [0. 1. 0.]\n [0. 0. 1.]\n [0. 0. 1.]\n [1. 0. 0.]\n [0. 0. 1.]\n [0. 0. 1.]\n [1. 0. 0.]\n [0. 0. 1.]\n [1. 0. 0.]\n [0. 0. 1.]\n [0. 0. 1.]\n [0. 1. 0.]\n [0. 0. 1.]\n [0. 1. 0.]\n [0. 0. 1.]\n [0. 0. 1.]\n [0. 1. 0.]\n [0. 0. 1.]\n [0. 0. 1.]\n [0. 0. 1.]\n [1. 0. 0.]\n [1. 0. 0.]\n [0. 1. 0.]\n [0. 0. 1.]\n [1. 0. 0.]\n [0. 0. 1.]\n [1. 0. 0.]\n [0. 1. 0.]\n [1. 0. 0.]\n [0. 0. 1.]\n [0. 0. 1.]\n [0. 0. 1.]\n [0. 1. 0.]\n [0. 0. 1.]\n [0. 1. 0.]\n [0. 0. 1.]\n [0. 1. 0.]\n [1. 0. 0.]\n [0. 0. 1.]\n [1. 0. 0.]\n [1. 0. 0.]\n [1. 0. 0.]\n [1. 0. 0.]\n [0. 0. 1.]\n [0. 0. 1.]\n [0. 0. 1.]\n [0. 0. 1.]\n [0. 1. 0.]\n [0. 0. 1.]\n [0. 0. 1.]\n [1. 0. 0.]\n [1. 0. 0.]\n [0. 1. 0.]\n [1. 0. 0.]\n [0. 1. 0.]\n [0. 0. 1.]\n [0. 1. 0.]\n [0. 0. 1.]\n [1. 0. 0.]\n [1. 0. 0.]\n [0. 0. 1.]\n [0. 0. 1.]\n [0. 1. 0.]\n [1. 0. 0.]\n [1. 0. 0.]\n [0. 0. 1.]\n [0. 0. 1.]\n [1. 0. 0.]\n [0. 1. 0.]\n [1. 0. 0.]\n [1. 0. 0.]\n [1. 0. 0.]\n [0. 0. 1.]\n [0. 0. 1.]\n [0. 1. 0.]\n [0. 0. 1.]\n [0. 0. 1.]\n [0. 0. 1.]\n [1. 0. 0.]\n [0. 1. 0.]\n [0. 0. 1.]\n [1. 0. 0.]\n [0. 0. 1.]\n [0. 0. 1.]\n [0. 0. 1.]\n [1. 0. 0.]\n [0. 0. 1.]\n [0. 0. 1.]\n [0. 1. 0.]\n [0. 1. 0.]\n [1. 0. 0.]\n [0. 0. 1.]\n [1. 0. 0.]\n [0. 0. 1.]\n [0. 1. 0.]\n [0. 0. 1.]\n [0. 0. 1.]\n [0. 0. 1.]\n [0. 0. 1.]\n [1. 0. 0.]\n [0. 0. 1.]\n [1. 0. 0.]\n [1. 0. 0.]\n [1. 0. 0.]\n [1. 0. 0.]\n [0. 0. 1.]\n [0. 0. 1.]\n [0. 1. 0.]\n [0. 0. 1.]\n [0. 0. 1.]\n [1. 0. 0.]\n [1. 0. 0.]\n [1. 0. 0.]\n [1. 0. 0.]\n [0. 0. 1.]\n [0. 1. 0.]\n [0. 0. 1.]\n [0. 0. 1.]\n [0. 0. 1.]\n [1. 0. 0.]\n [0. 1. 0.]\n [0. 1. 0.]\n [1. 0. 0.]\n [0. 1. 0.]\n [1. 0. 0.]\n [1. 0. 0.]\n [1. 0. 0.]\n [0. 1. 0.]\n [1. 0. 0.]\n [1. 0. 0.]\n [1. 0. 0.]\n [0. 0. 1.]\n [0. 1. 0.]\n [0. 1. 0.]]"
        },
        "random_forest_calibrated": {
          "accuracy": 1.0,
          "log_loss": 0.0004747600931415896,
          "predictions": "[0 2 2 0 2 2 2 1 2 1 1 1 2 2 0 2 0 2 2 0 1 2 2 0 2 2 0 2 0 2 2 1 2 1 2 2 1\n 2 2 2 0 0 1 2 0 2 0 1 0 2 2 2 1 2 1 2 1 0 2 0 0 0 0 2 2 2 2 1 2 2 0 0 1 0\n 1 2 1 2 0 0 2 2 1 0 0 2 2 0 1 0 0 0 2 2 1 2 2 2 0 1 2 0 2 2 2 0 2 2 1 1 0\n 2 0 2 1 2 2 2 2 0 2 0 0 0 0 2 2 1 2 2 0 0 0 0 2 1 2 2 2 0 1 1 0 1 0 0 0 1\n 0 0 0 2 1 1]",
          "probabilities": "[[1.         0.         0.        ]\n [0.         0.         1.        ]\n [0.         0.         1.        ]\n [1.         0.         0.        ]\n [0.         0.         1.        ]\n [0.         0.         1.        ]\n [0.         0.         1.        ]\n [0.         1.         0.        ]\n [0.         0.         1.        ]\n [0.         1.         0.        ]\n [0.         1.         0.        ]\n [0.         1.         0.        ]\n [0.         0.         1.        ]\n [0.         0.         1.        ]\n [1.         0.         0.        ]\n [0.         0.         1.        ]\n [1.         0.         0.        ]\n [0.         0.         1.        ]\n [0.         0.         1.        ]\n [1.         0.         0.        ]\n [0.02137295 0.96772477 0.01090228]\n [0.         0.         1.        ]\n [0.         0.         1.        ]\n [1.         0.         0.        ]\n [0.         0.         1.        ]\n [0.         0.         1.        ]\n [1.         0.         0.        ]\n [0.         0.         1.        ]\n [1.         0.         0.        ]\n [0.         0.         1.        ]\n [0.         0.         1.        ]\n [0.         1.         0.        ]\n [0.         0.         1.        ]\n [0.         1.         0.        ]\n [0.         0.         1.        ]\n [0.         0.         1.        ]\n [0.         1.         0.        ]\n [0.         0.         1.        ]\n [0.         0.         1.        ]\n [0.         0.         1.        ]\n [1.         0.         0.        ]\n [1.         0.         0.        ]\n [0.         1.         0.        ]\n [0.         0.         1.        ]\n [1.         0.         0.        ]\n [0.         0.         1.        ]\n [1.         0.         0.        ]\n [0.         1.         0.        ]\n [1.         0.         0.        ]\n [0.         0.         1.        ]\n [0.         0.         1.        ]\n [0.         0.         1.        ]\n [0.         1.         0.        ]\n [0.         0.         1.        ]\n [0.         1.         0.        ]\n [0.         0.         1.        ]\n [0.         1.         0.        ]\n [1.         0.         0.        ]\n [0.         0.         1.        ]\n [1.         0.         0.        ]\n [1.         0.         0.        ]\n [1.         0.         0.        ]\n [1.         0.         0.        ]\n [0.         0.         1.        ]\n [0.         0.         1.        ]\n [0.         0.         1.        ]\n [0.         0.         1.        ]\n [0.         1.         0.        ]\n [0.         0.         1.        ]\n [0.         0.         1.        ]\n [1.         0.         0.        ]\n [1.         0.         0.        ]\n [0.         1.         0.        ]\n [1.         0.         0.        ]\n [0.         1.         0.        ]\n [0.         0.         1.        ]\n [0.         1.         0.        ]\n [0.         0.         1.        ]\n [1.         0.         0.        ]\n [1.         0.         0.        ]\n [0.         0.         1.        ]\n [0.         0.         1.        ]\n [0.01960255 0.97798046 0.00241699]\n [1.         0.         0.        ]\n [1.         0.         0.        ]\n [0.         0.         1.        ]\n [0.         0.         1.        ]\n [1.         0.         0.        ]\n [0.         1.         0.        ]\n [1.         0.         0.        ]\n [1.         0.         0.        ]\n [1.         0.         0.        ]\n [0.         0.         1.        ]\n [0.         0.         1.        ]\n [0.         1.         0.        ]\n [0.         0.         1.        ]\n [0.         0.         1.        ]\n [0.         0.         1.        ]\n [1.         0.         0.        ]\n [0.         1.         0.        ]\n [0.         0.         1.        ]\n [1.         0.         0.        ]\n [0.         0.         1.        ]\n [0.         0.         1.        ]\n [0.         0.         1.        ]\n [1.         0.         0.        ]\n [0.         0.         1.        ]\n [0.         0.         1.        ]\n [0.         1.         0.        ]\n [0.         1.         0.        ]\n [1.         0.         0.        ]\n [0.         0.         1.        ]\n [1.         0.         0.        ]\n [0.         0.         1.        ]\n [0.         1.         0.        ]\n [0.         0.         1.        ]\n [0.         0.         1.        ]\n [0.         0.         1.        ]\n [0.         0.         1.        ]\n [1.         0.         0.        ]\n [0.         0.         1.        ]\n [1.         0.         0.        ]\n [1.         0.         0.        ]\n [1.         0.         0.        ]\n [1.         0.         0.        ]\n [0.         0.         1.        ]\n [0.         0.         1.        ]\n [0.         1.         0.        ]\n [0.         0.         1.        ]\n [0.         0.         1.        ]\n [0.99194069 0.00805931 0.        ]\n [1.         0.         0.        ]\n [1.         0.         0.        ]\n [1.         0.         0.        ]\n [0.         0.         1.        ]\n [0.         1.         0.        ]\n [0.         0.         1.        ]\n [0.         0.         1.        ]\n [0.         0.         1.        ]\n [1.         0.         0.        ]\n [0.         1.         0.        ]\n [0.         1.         0.        ]\n [1.         0.         0.        ]\n [0.         1.         0.        ]\n [1.         0.         0.        ]\n [1.         0.         0.        ]\n [1.         0.         0.        ]\n [0.         1.         0.        ]\n [1.         0.         0.        ]\n [1.         0.         0.        ]\n [0.99010138 0.00989862 0.        ]\n [0.         0.         1.        ]\n [0.         1.         0.        ]\n [0.         1.         0.        ]]"
        },
        "logistic_regression_calibrated": {
          "accuracy": 1.0,
          "log_loss": 0.0013072101164344292,
          "predictions": "[0 2 2 0 2 2 2 1 2 1 1 1 2 2 0 2 0 2 2 0 1 2 2 0 2 2 0 2 0 2 2 1 2 1 2 2 1\n 2 2 2 0 0 1 2 0 2 0 1 0 2 2 2 1 2 1 2 1 0 2 0 0 0 0 2 2 2 2 1 2 2 0 0 1 0\n 1 2 1 2 0 0 2 2 1 0 0 2 2 0 1 0 0 0 2 2 1 2 2 2 0 1 2 0 2 2 2 0 2 2 1 1 0\n 2 0 2 1 2 2 2 2 0 2 0 0 0 0 2 2 1 2 2 0 0 0 0 2 1 2 2 2 0 1 1 0 1 0 0 0 1\n 0 0 0 2 1 1]",
          "probabilities": "[[1.00000000e+00 0.00000000e+00 0.00000000e+00]\n [0.00000000e+00 0.00000000e+00 1.00000000e+00]\n [0.00000000e+00 0.00000000e+00 1.00000000e+00]\n [1.00000000e+00 0.00000000e+00 0.00000000e+00]\n [0.00000000e+00 0.00000000e+00 1.00000000e+00]\n [0.00000000e+00 0.00000000e+00 1.00000000e+00]\n [0.00000000e+00 0.00000000e+00 1.00000000e+00]\n [0.00000000e+00 1.00000000e+00 0.00000000e+00]\n [0.00000000e+00 0.00000000e+00 1.00000000e+00]\n [0.00000000e+00 1.00000000e+00 0.00000000e+00]\n [0.00000000e+00 1.00000000e+00 0.00000000e+00]\n [0.00000000e+00 1.00000000e+00 0.00000000e+00]\n [0.00000000e+00 0.00000000e+00 1.00000000e+00]\n [0.00000000e+00 0.00000000e+00 1.00000000e+00]\n [1.00000000e+00 0.00000000e+00 0.00000000e+00]\n [0.00000000e+00 0.00000000e+00 1.00000000e+00]\n [1.00000000e+00 0.00000000e+00 0.00000000e+00]\n [0.00000000e+00 0.00000000e+00 1.00000000e+00]\n [0.00000000e+00 0.00000000e+00 1.00000000e+00]\n [1.00000000e+00 0.00000000e+00 0.00000000e+00]\n [0.00000000e+00 1.00000000e+00 0.00000000e+00]\n [0.00000000e+00 0.00000000e+00 1.00000000e+00]\n [0.00000000e+00 0.00000000e+00 1.00000000e+00]\n [1.00000000e+00 0.00000000e+00 0.00000000e+00]\n [0.00000000e+00 0.00000000e+00 1.00000000e+00]\n [0.00000000e+00 0.00000000e+00 1.00000000e+00]\n [1.00000000e+00 0.00000000e+00 0.00000000e+00]\n [0.00000000e+00 0.00000000e+00 1.00000000e+00]\n [9.89364733e-01 1.06352674e-02 0.00000000e+00]\n [0.00000000e+00 0.00000000e+00 1.00000000e+00]\n [0.00000000e+00 0.00000000e+00 1.00000000e+00]\n [0.00000000e+00 1.00000000e+00 0.00000000e+00]\n [0.00000000e+00 0.00000000e+00 1.00000000e+00]\n [0.00000000e+00 1.00000000e+00 0.00000000e+00]\n [0.00000000e+00 0.00000000e+00 1.00000000e+00]\n [0.00000000e+00 0.00000000e+00 1.00000000e+00]\n [0.00000000e+00 1.00000000e+00 0.00000000e+00]\n [0.00000000e+00 0.00000000e+00 1.00000000e+00]\n [0.00000000e+00 0.00000000e+00 1.00000000e+00]\n [0.00000000e+00 0.00000000e+00 1.00000000e+00]\n [1.00000000e+00 0.00000000e+00 0.00000000e+00]\n [1.00000000e+00 0.00000000e+00 0.00000000e+00]\n [0.00000000e+00 1.00000000e+00 0.00000000e+00]\n [0.00000000e+00 0.00000000e+00 1.00000000e+00]\n [1.00000000e+00 0.00000000e+00 0.00000000e+00]\n [0.00000000e+00 5.33712295e-02 9.46628770e-01]\n [1.00000000e+00 0.00000000e+00 0.00000000e+00]\n [0.00000000e+00 1.00000000e+00 0.00000000e+00]\n [1.00000000e+00 0.00000000e+00 0.00000000e+00]\n [0.00000000e+00 0.00000000e+00 1.00000000e+00]\n [0.00000000e+00 0.00000000e+00 1.00000000e+00]\n [0.00000000e+00 0.00000000e+00 1.00000000e+00]\n [0.00000000e+00 1.00000000e+00 0.00000000e+00]\n [0.00000000e+00 0.00000000e+00 1.00000000e+00]\n [0.00000000e+00 1.00000000e+00 0.00000000e+00]\n [0.00000000e+00 0.00000000e+00 1.00000000e+00]\n [0.00000000e+00 1.00000000e+00 0.00000000e+00]\n [1.00000000e+00 0.00000000e+00 0.00000000e+00]\n [0.00000000e+00 0.00000000e+00 1.00000000e+00]\n [1.00000000e+00 0.00000000e+00 0.00000000e+00]\n [1.00000000e+00 0.00000000e+00 0.00000000e+00]\n [1.00000000e+00 0.00000000e+00 0.00000000e+00]\n [1.00000000e+00 0.00000000e+00 0.00000000e+00]\n [0.00000000e+00 0.00000000e+00 1.00000000e+00]\n [0.00000000e+00 0.00000000e+00 1.00000000e+00]\n [0.00000000e+00 0.00000000e+00 1.00000000e+00]\n [0.00000000e+00 0.00000000e+00 1.00000000e+00]\n [0.00000000e+00 1.00000000e+00 0.00000000e+00]\n [0.00000000e+00 0.00000000e+00 1.00000000e+00]\n [0.00000000e+00 0.00000000e+00 1.00000000e+00]\n [9.86416876e-01 1.35831242e-02 0.00000000e+00]\n [1.00000000e+00 0.00000000e+00 0.00000000e+00]\n [0.00000000e+00 1.00000000e+00 0.00000000e+00]\n [1.00000000e+00 0.00000000e+00 0.00000000e+00]\n [0.00000000e+00 1.00000000e+00 0.00000000e+00]\n [0.00000000e+00 0.00000000e+00 1.00000000e+00]\n [0.00000000e+00 1.00000000e+00 0.00000000e+00]\n [0.00000000e+00 0.00000000e+00 1.00000000e+00]\n [1.00000000e+00 0.00000000e+00 0.00000000e+00]\n [1.00000000e+00 0.00000000e+00 0.00000000e+00]\n [0.00000000e+00 0.00000000e+00 1.00000000e+00]\n [0.00000000e+00 0.00000000e+00 1.00000000e+00]\n [0.00000000e+00 1.00000000e+00 0.00000000e+00]\n [1.00000000e+00 0.00000000e+00 0.00000000e+00]\n [1.00000000e+00 0.00000000e+00 0.00000000e+00]\n [0.00000000e+00 0.00000000e+00 1.00000000e+00]\n [0.00000000e+00 0.00000000e+00 1.00000000e+00]\n [1.00000000e+00 0.00000000e+00 0.00000000e+00]\n [0.00000000e+00 9.05256623e-01 9.47433769e-02]\n [1.00000000e+00 0.00000000e+00 0.00000000e+00]\n [1.00000000e+00 0.00000000e+00 0.00000000e+00]\n [1.00000000e+00 0.00000000e+00 0.00000000e+00]\n [0.00000000e+00 0.00000000e+00 1.00000000e+00]\n [0.00000000e+00 0.00000000e+00 1.00000000e+00]\n [0.00000000e+00 1.00000000e+00 0.00000000e+00]\n [0.00000000e+00 0.00000000e+00 1.00000000e+00]\n [0.00000000e+00 0.00000000e+00 1.00000000e+00]\n [0.00000000e+00 0.00000000e+00 1.00000000e+00]\n [1.00000000e+00 0.00000000e+00 0.00000000e+00]\n [0.00000000e+00 1.00000000e+00 0.00000000e+00]\n [0.00000000e+00 0.00000000e+00 1.00000000e+00]\n [1.00000000e+00 0.00000000e+00 0.00000000e+00]\n [0.00000000e+00 0.00000000e+00 1.00000000e+00]\n [0.00000000e+00 0.00000000e+00 1.00000000e+00]\n [0.00000000e+00 0.00000000e+00 1.00000000e+00]\n [1.00000000e+00 0.00000000e+00 0.00000000e+00]\n [0.00000000e+00 0.00000000e+00 1.00000000e+00]\n [0.00000000e+00 0.00000000e+00 1.00000000e+00]\n [0.00000000e+00 1.00000000e+00 0.00000000e+00]\n [0.00000000e+00 1.00000000e+00 0.00000000e+00]\n [1.00000000e+00 0.00000000e+00 0.00000000e+00]\n [0.00000000e+00 0.00000000e+00 1.00000000e+00]\n [1.00000000e+00 0.00000000e+00 0.00000000e+00]\n [0.00000000e+00 0.00000000e+00 1.00000000e+00]\n [0.00000000e+00 1.00000000e+00 0.00000000e+00]\n [0.00000000e+00 0.00000000e+00 1.00000000e+00]\n [0.00000000e+00 0.00000000e+00 1.00000000e+00]\n [0.00000000e+00 0.00000000e+00 1.00000000e+00]\n [0.00000000e+00 0.00000000e+00 1.00000000e+00]\n [9.78376352e-01 2.16236477e-02 0.00000000e+00]\n [0.00000000e+00 0.00000000e+00 1.00000000e+00]\n [1.00000000e+00 0.00000000e+00 0.00000000e+00]\n [1.00000000e+00 0.00000000e+00 0.00000000e+00]\n [1.00000000e+00 0.00000000e+00 0.00000000e+00]\n [1.00000000e+00 0.00000000e+00 0.00000000e+00]\n [0.00000000e+00 0.00000000e+00 1.00000000e+00]\n [0.00000000e+00 0.00000000e+00 1.00000000e+00]\n [0.00000000e+00 1.00000000e+00 0.00000000e+00]\n [0.00000000e+00 0.00000000e+00 1.00000000e+00]\n [0.00000000e+00 0.00000000e+00 1.00000000e+00]\n [1.00000000e+00 0.00000000e+00 0.00000000e+00]\n [1.00000000e+00 0.00000000e+00 0.00000000e+00]\n [1.00000000e+00 0.00000000e+00 0.00000000e+00]\n [1.00000000e+00 0.00000000e+00 0.00000000e+00]\n [0.00000000e+00 0.00000000e+00 1.00000000e+00]\n [0.00000000e+00 1.00000000e+00 0.00000000e+00]\n [0.00000000e+00 0.00000000e+00 1.00000000e+00]\n [0.00000000e+00 0.00000000e+00 1.00000000e+00]\n [0.00000000e+00 0.00000000e+00 1.00000000e+00]\n [1.00000000e+00 0.00000000e+00 0.00000000e+00]\n [6.95723856e-04 9.99304276e-01 0.00000000e+00]\n [0.00000000e+00 1.00000000e+00 0.00000000e+00]\n [1.00000000e+00 0.00000000e+00 0.00000000e+00]\n [0.00000000e+00 1.00000000e+00 0.00000000e+00]\n [1.00000000e+00 0.00000000e+00 0.00000000e+00]\n [1.00000000e+00 0.00000000e+00 0.00000000e+00]\n [1.00000000e+00 0.00000000e+00 0.00000000e+00]\n [0.00000000e+00 1.00000000e+00 0.00000000e+00]\n [1.00000000e+00 0.00000000e+00 0.00000000e+00]\n [1.00000000e+00 0.00000000e+00 0.00000000e+00]\n [1.00000000e+00 0.00000000e+00 0.00000000e+00]\n [0.00000000e+00 0.00000000e+00 1.00000000e+00]\n [0.00000000e+00 1.00000000e+00 0.00000000e+00]\n [0.00000000e+00 1.00000000e+00 0.00000000e+00]]"
        },
        "ensemble_calibrated": {
          "accuracy": 1.0,
          "log_loss": 8.528479789981625e-05,
          "predictions": "[0 2 2 0 2 2 2 1 2 1 1 1 2 2 0 2 0 2 2 0 1 2 2 0 2 2 0 2 0 2 2 1 2 1 2 2 1\n 2 2 2 0 0 1 2 0 2 0 1 0 2 2 2 1 2 1 2 1 0 2 0 0 0 0 2 2 2 2 1 2 2 0 0 1 0\n 1 2 1 2 0 0 2 2 1 0 0 2 2 0 1 0 0 0 2 2 1 2 2 2 0 1 2 0 2 2 2 0 2 2 1 1 0\n 2 0 2 1 2 2 2 2 0 2 0 0 0 0 2 2 1 2 2 0 0 0 0 2 1 2 2 2 0 1 1 0 1 0 0 0 1\n 0 0 0 2 1 1]",
          "probabilities": "[[1.00000000e+00 0.00000000e+00 0.00000000e+00]\n [0.00000000e+00 0.00000000e+00 1.00000000e+00]\n [0.00000000e+00 0.00000000e+00 1.00000000e+00]\n [1.00000000e+00 0.00000000e+00 0.00000000e+00]\n [0.00000000e+00 0.00000000e+00 1.00000000e+00]\n [0.00000000e+00 0.00000000e+00 1.00000000e+00]\n [0.00000000e+00 0.00000000e+00 1.00000000e+00]\n [0.00000000e+00 1.00000000e+00 0.00000000e+00]\n [0.00000000e+00 0.00000000e+00 1.00000000e+00]\n [0.00000000e+00 1.00000000e+00 0.00000000e+00]\n [0.00000000e+00 1.00000000e+00 0.00000000e+00]\n [0.00000000e+00 1.00000000e+00 0.00000000e+00]\n [0.00000000e+00 0.00000000e+00 1.00000000e+00]\n [0.00000000e+00 0.00000000e+00 1.00000000e+00]\n [1.00000000e+00 0.00000000e+00 0.00000000e+00]\n [0.00000000e+00 0.00000000e+00 1.00000000e+00]\n [1.00000000e+00 0.00000000e+00 0.00000000e+00]\n [0.00000000e+00 0.00000000e+00 1.00000000e+00]\n [0.00000000e+00 0.00000000e+00 1.00000000e+00]\n [1.00000000e+00 0.00000000e+00 0.00000000e+00]\n [3.76700824e-03 9.93021297e-01 3.21169517e-03]\n [0.00000000e+00 0.00000000e+00 1.00000000e+00]\n [0.00000000e+00 0.00000000e+00 1.00000000e+00]\n [1.00000000e+00 0.00000000e+00 0.00000000e+00]\n [0.00000000e+00 0.00000000e+00 1.00000000e+00]\n [0.00000000e+00 0.00000000e+00 1.00000000e+00]\n [1.00000000e+00 0.00000000e+00 0.00000000e+00]\n [0.00000000e+00 0.00000000e+00 1.00000000e+00]\n [1.00000000e+00 0.00000000e+00 0.00000000e+00]\n [0.00000000e+00 0.00000000e+00 1.00000000e+00]\n [0.00000000e+00 0.00000000e+00 1.00000000e+00]\n [0.00000000e+00 1.00000000e+00 0.00000000e+00]\n [0.00000000e+00 0.00000000e+00 1.00000000e+00]\n [0.00000000e+00 1.00000000e+00 0.00000000e+00]\n [0.00000000e+00 0.00000000e+00 1.00000000e+00]\n [0.00000000e+00 0.00000000e+00 1.00000000e+00]\n [0.00000000e+00 1.00000000e+00 0.00000000e+00]\n [0.00000000e+00 0.00000000e+00 1.00000000e+00]\n [0.00000000e+00 0.00000000e+00 1.00000000e+00]\n [0.00000000e+00 0.00000000e+00 1.00000000e+00]\n [1.00000000e+00 0.00000000e+00 0.00000000e+00]\n [1.00000000e+00 0.00000000e+00 0.00000000e+00]\n [0.00000000e+00 1.00000000e+00 0.00000000e+00]\n [0.00000000e+00 0.00000000e+00 1.00000000e+00]\n [1.00000000e+00 0.00000000e+00 0.00000000e+00]\n [0.00000000e+00 0.00000000e+00 1.00000000e+00]\n [1.00000000e+00 0.00000000e+00 0.00000000e+00]\n [0.00000000e+00 1.00000000e+00 0.00000000e+00]\n [1.00000000e+00 0.00000000e+00 0.00000000e+00]\n [0.00000000e+00 0.00000000e+00 1.00000000e+00]\n [0.00000000e+00 0.00000000e+00 1.00000000e+00]\n [0.00000000e+00 0.00000000e+00 1.00000000e+00]\n [0.00000000e+00 1.00000000e+00 0.00000000e+00]\n [0.00000000e+00 0.00000000e+00 1.00000000e+00]\n [0.00000000e+00 1.00000000e+00 0.00000000e+00]\n [0.00000000e+00 0.00000000e+00 1.00000000e+00]\n [0.00000000e+00 1.00000000e+00 0.00000000e+00]\n [1.00000000e+00 0.00000000e+00 0.00000000e+00]\n [0.00000000e+00 0.00000000e+00 1.00000000e+00]\n [1.00000000e+00 0.00000000e+00 0.00000000e+00]\n [1.00000000e+00 0.00000000e+00 0.00000000e+00]\n [1.00000000e+00 0.00000000e+00 0.00000000e+00]\n [1.00000000e+00 0.00000000e+00 0.00000000e+00]\n [0.00000000e+00 0.00000000e+00 1.00000000e+00]\n [0.00000000e+00 0.00000000e+00 1.00000000e+00]\n [0.00000000e+00 0.00000000e+00 1.00000000e+00]\n [0.00000000e+00 0.00000000e+00 1.00000000e+00]\n [0.00000000e+00 1.00000000e+00 0.00000000e+00]\n [0.00000000e+00 0.00000000e+00 1.00000000e+00]\n [0.00000000e+00 0.00000000e+00 1.00000000e+00]\n [1.00000000e+00 0.00000000e+00 0.00000000e+00]\n [1.00000000e+00 0.00000000e+00 0.00000000e+00]\n [0.00000000e+00 1.00000000e+00 0.00000000e+00]\n [1.00000000e+00 0.00000000e+00 0.00000000e+00]\n [0.00000000e+00 1.00000000e+00 0.00000000e+00]\n [0.00000000e+00 0.00000000e+00 1.00000000e+00]\n [0.00000000e+00 1.00000000e+00 0.00000000e+00]\n [0.00000000e+00 0.00000000e+00 1.00000000e+00]\n [1.00000000e+00 0.00000000e+00 0.00000000e+00]\n [1.00000000e+00 0.00000000e+00 0.00000000e+00]\n [0.00000000e+00 0.00000000e+00 1.00000000e+00]\n [0.00000000e+00 0.00000000e+00 1.00000000e+00]\n [1.98498624e-03 9.97227747e-01 7.87266351e-04]\n [1.00000000e+00 0.00000000e+00 0.00000000e+00]\n [1.00000000e+00 0.00000000e+00 0.00000000e+00]\n [0.00000000e+00 0.00000000e+00 1.00000000e+00]\n [0.00000000e+00 0.00000000e+00 1.00000000e+00]\n [1.00000000e+00 0.00000000e+00 0.00000000e+00]\n [0.00000000e+00 1.00000000e+00 0.00000000e+00]\n [1.00000000e+00 0.00000000e+00 0.00000000e+00]\n [1.00000000e+00 0.00000000e+00 0.00000000e+00]\n [1.00000000e+00 0.00000000e+00 0.00000000e+00]\n [0.00000000e+00 0.00000000e+00 1.00000000e+00]\n [0.00000000e+00 0.00000000e+00 1.00000000e+00]\n [0.00000000e+00 1.00000000e+00 0.00000000e+00]\n [0.00000000e+00 0.00000000e+00 1.00000000e+00]\n [0.00000000e+00 0.00000000e+00 1.00000000e+00]\n [0.00000000e+00 0.00000000e+00 1.00000000e+00]\n [1.00000000e+00 0.00000000e+00 0.00000000e+00]\n [0.00000000e+00 1.00000000e+00 0.00000000e+00]\n [0.00000000e+00 0.00000000e+00 1.00000000e+00]\n [1.00000000e+00 0.00000000e+00 0.00000000e+00]\n [0.00000000e+00 0.00000000e+00 1.00000000e+00]\n [0.00000000e+00 0.00000000e+00 1.00000000e+00]\n [0.00000000e+00 0.00000000e+00 1.00000000e+00]\n [1.00000000e+00 0.00000000e+00 0.00000000e+00]\n [0.00000000e+00 0.00000000e+00 1.00000000e+00]\n [0.00000000e+00 0.00000000e+00 1.00000000e+00]\n [0.00000000e+00 1.00000000e+00 0.00000000e+00]\n [0.00000000e+00 1.00000000e+00 0.00000000e+00]\n [1.00000000e+00 0.00000000e+00 0.00000000e+00]\n [0.00000000e+00 0.00000000e+00 1.00000000e+00]\n [1.00000000e+00 0.00000000e+00 0.00000000e+00]\n [0.00000000e+00 0.00000000e+00 1.00000000e+00]\n [0.00000000e+00 1.00000000e+00 0.00000000e+00]\n [0.00000000e+00 0.00000000e+00 1.00000000e+00]\n [0.00000000e+00 0.00000000e+00 1.00000000e+00]\n [0.00000000e+00 0.00000000e+00 1.00000000e+00]\n [0.00000000e+00 0.00000000e+00 1.00000000e+00]\n [1.00000000e+00 0.00000000e+00 0.00000000e+00]\n [0.00000000e+00 0.00000000e+00 1.00000000e+00]\n [1.00000000e+00 0.00000000e+00 0.00000000e+00]\n [1.00000000e+00 0.00000000e+00 0.00000000e+00]\n [1.00000000e+00 0.00000000e+00 0.00000000e+00]\n [1.00000000e+00 0.00000000e+00 0.00000000e+00]\n [0.00000000e+00 0.00000000e+00 1.00000000e+00]\n [0.00000000e+00 0.00000000e+00 1.00000000e+00]\n [0.00000000e+00 1.00000000e+00 0.00000000e+00]\n [0.00000000e+00 0.00000000e+00 1.00000000e+00]\n [0.00000000e+00 0.00000000e+00 1.00000000e+00]\n [9.98560846e-01 1.43915384e-03 0.00000000e+00]\n [1.00000000e+00 0.00000000e+00 0.00000000e+00]\n [1.00000000e+00 0.00000000e+00 0.00000000e+00]\n [1.00000000e+00 0.00000000e+00 0.00000000e+00]\n [0.00000000e+00 0.00000000e+00 1.00000000e+00]\n [0.00000000e+00 1.00000000e+00 0.00000000e+00]\n [0.00000000e+00 0.00000000e+00 1.00000000e+00]\n [0.00000000e+00 0.00000000e+00 1.00000000e+00]\n [0.00000000e+00 0.00000000e+00 1.00000000e+00]\n [1.00000000e+00 0.00000000e+00 0.00000000e+00]\n [0.00000000e+00 1.00000000e+00 0.00000000e+00]\n [0.00000000e+00 1.00000000e+00 0.00000000e+00]\n [1.00000000e+00 0.00000000e+00 0.00000000e+00]\n [0.00000000e+00 1.00000000e+00 0.00000000e+00]\n [1.00000000e+00 0.00000000e+00 0.00000000e+00]\n [1.00000000e+00 0.00000000e+00 0.00000000e+00]\n [1.00000000e+00 0.00000000e+00 0.00000000e+00]\n [0.00000000e+00 1.00000000e+00 0.00000000e+00]\n [1.00000000e+00 0.00000000e+00 0.00000000e+00]\n [1.00000000e+00 0.00000000e+00 0.00000000e+00]\n [9.98087434e-01 1.91256632e-03 0.00000000e+00]\n [0.00000000e+00 0.00000000e+00 1.00000000e+00]\n [0.00000000e+00 1.00000000e+00 0.00000000e+00]\n [0.00000000e+00 1.00000000e+00 0.00000000e+00]]"
        }
      },
      "feature_importance": {
        "xgboost": "                 feature  importance\n120       result_encoded    0.331848\n232        match_balance    0.137195\n101      goal_difference    0.125426\n98              home_win    0.120615\n99                  draw    0.108038\n..                   ...         ...\n235   away_team_strength    0.000000\n237   home_team_win_rate    0.000000\n238   away_team_win_rate    0.000000\n239  home_team_avg_goals    0.000000\n240  away_team_avg_goals    0.000000\n\n[241 rows x 2 columns]",
        "lightgbm": "                 feature  importance\n100             away_win         169\n98              home_win         164\n99                  draw         157\n101      goal_difference          51\n232        match_balance          37\n..                   ...         ...\n236   team_strength_diff           0\n237   home_team_win_rate           0\n238   away_team_win_rate           0\n239  home_team_avg_goals           0\n240  away_team_avg_goals           0\n\n[241 rows x 2 columns]",
        "random_forest": "                      feature  importance\n100                  away_win    0.105030\n101           goal_difference    0.093980\n104            home_advantage    0.091204\n99                       draw    0.089655\n232             match_balance    0.083351\n..                        ...         ...\n215      away_position_change    0.000000\n211   away_clean_sheet_streak    0.000000\n210   home_clean_sheet_streak    0.000000\n214      home_position_change    0.000000\n225  transfer_learning_factor    0.000000\n\n[241 rows x 2 columns]"
      },
      "success": true
    },
    "transfer_learning": {
      "success": false,
      "error": "No source data files"
    },
    "hyperparameter_tuning": {
      "optimization_results": {
        "xgboost": {
          "best_params": {
            "n_estimators": 182,
            "max_depth": 5,
            "learning_rate": 0.03822754945608633,
            "subsample": 0.9489588389616428,
            "colsample_bytree": 0.9948184317827552,
            "min_child_weight": 2,
            "reg_alpha": 0.7311119726517566,
            "reg_lambda": 1.1780281152900898
          },
          "best_value": 0.01023306955660576,
          "n_trials": 30,
          "study": "<optuna.study.study.Study object at 0x7ffb299ee120>",
          "model_type": "xgboost"
        },
        "lightgbm": {
          "best_params": {
            "n_estimators": 199,
            "max_depth": 4,
            "learning_rate": 0.09939936498022436,
            "num_leaves": 71,
            "feature_fraction": 0.7944598432202841,
            "bagging_fraction": 0.8275521263940526,
            "bagging_freq": 7,
            "lambda_l1": 0.10846079552796235,
            "lambda_l2": 1.3790065449478965,
            "min_data_in_leaf": 42
          },
          "best_value": 0.0014072020231248384,
          "n_trials": 30,
          "study": "<optuna.study.study.Study object at 0x7ffb29545e50>",
          "model_type": "lightgbm"
        },
        "random_forest": {
          "best_params": {
            "n_estimators": 152,
            "max_depth": 14,
            "min_samples_split": 16,
            "min_samples_leaf": 2,
            "max_features": null
          },
          "best_value": 2.2204460492503136e-16,
          "n_trials": 30,
          "study": "<optuna.study.study.Study object at 0x7ffb29546ad0>",
          "model_type": "random_forest"
        },
        "logistic_regression": {
          "best_params": {
            "C": 9.976267815765638,
            "max_iter": 893,
            "solver": "lbfgs"
          },
          "best_value": 0.0020167308279405416,
          "n_trials": 30,
          "study": "<optuna.study.study.Study object at 0x7ffb25c5cb00>",
          "model_type": "logistic_regression"
        }
      },
      "validation_results": {
        "models": {
          "xgboost": {
            "validation_passed": true,
            "issues": []
          },
          "lightgbm": {
            "validation_passed": true,
            "issues": []
          },
          "random_forest": {
            "validation_passed": true,
            "issues": []
          },
          "logistic_regression": {
            "validation_passed": true,
            "issues": []
          }
        },
        "overall": {
          "validation_passed": true,
          "issues": []
        }
      },
      "comparison_results": {
        "models": {
          "xgboost": {
            "score": 0.01023306955660576,
            "params": {
              "n_estimators": 182,
              "max_depth": 5,
              "learning_rate": 0.03822754945608633,
              "subsample": 0.9489588389616428,
              "colsample_bytree": 0.9948184317827552,
              "min_child_weight": 2,
              "reg_alpha": 0.7311119726517566,
              "reg_lambda": 1.1780281152900898
            }
          },
          "lightgbm": {
            "score": 0.0014072020231248384,
            "params": {
              "n_estimators": 199,
              "max_depth": 4,
              "learning_rate": 0.09939936498022436,
              "num_leaves": 71,
              "feature_fraction": 0.7944598432202841,
              "bagging_fraction": 0.8275521263940526,
              "bagging_freq": 7,
              "lambda_l1": 0.10846079552796235,
              "lambda_l2": 1.3790065449478965,
              "min_data_in_leaf": 42
            }
          },
          "random_forest": {
            "score": 2.2204460492503136e-16,
            "params": {
              "n_estimators": 152,
              "max_depth": 14,
              "min_samples_split": 16,
              "min_samples_leaf": 2,
              "max_features": null
            }
          },
          "logistic_regression": {
            "score": 0.0020167308279405416,
            "params": {
              "C": 9.976267815765638,
              "max_iter": 893,
              "solver": "lbfgs"
            }
          }
        },
        "best_model": "random_forest",
        "best_score": 2.2204460492503136e-16
      },
      "evaluation_results": {
        "xgboost": {
          "mean_score": 0.01023722162467019,
          "std_score": 0.0044520051711102165,
          "scores": [
            0.01634205762231578,
            0.00851850454688483,
            0.005851102704809961
          ],
          "model": "XGBClassifier(base_score=None, booster=None, callbacks=None,\n              colsample_bylevel=None, colsample_bynode=None,\n              colsample_bytree=0.9948184317827552, device=None,\n              early_stopping_rounds=None, enable_categorical=False,\n              eval_metric='mlogloss', feature_types=None, feature_weights=None,\n              gamma=None, grow_policy=None, importance_type=None,\n              interaction_constraints=None, learning_rate=0.03822754945608633,\n              max_bin=None, max_cat_threshold=None, max_cat_to_onehot=None,\n              max_delta_step=None, max_depth=5, max_leaves=None,\n              min_child_weight=2, missing=nan, monotone_constraints=None,\n              multi_strategy=None, n_estimators=182, n_jobs=None,\n              num_parallel_tree=None, ...)"
        },
        "lightgbm": {
          "mean_score": 0.0014065491739287466,
          "std_score": 0.0006281130893479764,
          "scores": [
            0.0022584238838878187,
            0.0011986053702632328,
            0.0007626182676351891
          ],
          "model": "LGBMClassifier(bagging_fraction=0.8275521263940526, bagging_freq=7,\n               feature_fraction=0.7944598432202841,\n               lambda_l1=0.10846079552796235, lambda_l2=1.3790065449478965,\n               learning_rate=0.09939936498022436, max_depth=4,\n               min_data_in_leaf=42, n_estimators=199, num_leaves=71,\n               verbose=-1)"
        },
        "random_forest": {
          "mean_score": 2.2204460492503136e-16,
          "std_score": 0.0,
          "scores": [
            2.2204460492503136e-16,
            2.2204460492503136e-16,
            2.2204460492503136e-16
          ],
          "model": "RandomForestClassifier(max_depth=14, max_features=None, min_samples_leaf=2,\n                       min_samples_split=16, n_estimators=152)"
        },
        "logistic_regression": {
          "mean_score": 0.0020167308279405416,
          "std_score": 0.0016610782600570402,
          "scores": [
            0.004347770853846778,
            0.001103125041001909,
            0.0005992965889729385
          ],
          "model": "LogisticRegression(C=9.976267815765638, max_iter=893)"
        }
      },
      "importance_results": {
        "xgboost": {
          "importance": {
            "n_estimators": 0.5638587936015486,
            "learning_rate": 0.2871614882383129,
            "min_child_weight": 0.05216372218236044,
            "subsample": 0.04494135723604435,
            "colsample_bytree": 0.04451737811666808,
            "reg_alpha": 0.005431661746462462,
            "max_depth": 0.0010895584317984436,
            "reg_lambda": 0.000836040446804723
          },
          "best_params": {
            "n_estimators": 182,
            "max_depth": 5,
            "learning_rate": 0.03822754945608633,
            "subsample": 0.9489588389616428,
            "colsample_bytree": 0.9948184317827552,
            "min_child_weight": 2,
            "reg_alpha": 0.7311119726517566,
            "reg_lambda": 1.1780281152900898
          },
          "best_value": 0.01023306955660576
        },
        "lightgbm": {
          "importance": {
            "bagging_fraction": 0.29380350455179305,
            "learning_rate": 0.281566517082108,
            "num_leaves": 0.16149205443981657,
            "lambda_l1": 0.10471881249562245,
            "n_estimators": 0.08536584745942874,
            "bagging_freq": 0.028081862798343262,
            "min_data_in_leaf": 0.022597311344752017,
            "max_depth": 0.01694319179097299,
            "lambda_l2": 0.005033182863298278,
            "feature_fraction": 0.00039771517386453554
          },
          "best_params": {
            "n_estimators": 199,
            "max_depth": 4,
            "learning_rate": 0.09939936498022436,
            "num_leaves": 71,
            "feature_fraction": 0.7944598432202841,
            "bagging_fraction": 0.8275521263940526,
            "bagging_freq": 7,
            "lambda_l1": 0.10846079552796235,
            "lambda_l2": 1.3790065449478965,
            "min_data_in_leaf": 42
          },
          "best_value": 0.0014072020231248384
        },
        "random_forest": {
          "importance": {
            "max_features": 0.7510081828851517,
            "min_samples_leaf": 0.21596912747711353,
            "min_samples_split": 0.0316442778039048,
            "max_depth": 0.0007454389782344947,
            "n_estimators": 0.0006329728555954796
          },
          "best_params": {
            "n_estimators": 152,
            "max_depth": 14,
            "min_samples_split": 16,
            "min_samples_leaf": 2,
            "max_features": null
          },
          "best_value": 2.2204460492503136e-16
        },
        "logistic_regression": {
          "importance": {
            "solver": 0.8599107078878894,
            "C": 0.12528584031404363,
            "max_iter": 0.014803451798066997
          },
          "best_params": {
            "C": 9.976267815765638,
            "max_iter": 893,
            "solver": "lbfgs"
          },
          "best_value": 0.0020167308279405416
        }
      },
      "success": true
    },
    "ensemble": {
      "base_models": {
        "xgboost": "XGBClassifier(base_score=None, booster=None, callbacks=None,\n              colsample_bylevel=None, colsample_bynode=None,\n              colsample_bytree=0.8, device=None, early_stopping_rounds=None,\n              enable_categorical=False, eval_metric='mlogloss',\n              feature_types=None, feature_weights=None, gamma=None,\n              grow_policy=None, importance_type=None,\n              interaction_constraints=None, learning_rate=0.03, max_bin=None,\n              max_cat_threshold=None, max_cat_to_onehot=None,\n              max_delta_step=None, max_depth=5, max_leaves=None,\n              min_child_weight=5, missing=nan, monotone_constraints=None,\n              multi_strategy=None, n_estimators=150, n_jobs=None,\n              num_parallel_tree=None, ...)",
        "lightgbm": "LGBMClassifier(bagging_fraction=0.8, bagging_freq=5, feature_fraction=0.8,\n               lambda_l1=0.2, lambda_l2=1.5, learning_rate=0.03, max_depth=5,\n               metric='multi_logloss', min_data_in_leaf=10, n_estimators=150,\n               num_class=3, num_leaves=30, objective='multiclass',\n               random_state=42, verbose=-1)",
        "random_forest": "RandomForestClassifier(max_depth=8, min_samples_leaf=5, min_samples_split=10,\n                       random_state=42)",
        "logistic_regression": "LogisticRegression(C=0.1, max_iter=1000, random_state=42)"
      },
      "training_results": {
        "xgboost": {
          "model": "XGBClassifier(base_score=None, booster=None, callbacks=None,\n              colsample_bylevel=None, colsample_bynode=None,\n              colsample_bytree=0.8, device=None, early_stopping_rounds=None,\n              enable_categorical=False, eval_metric='mlogloss',\n              feature_types=None, feature_weights=None, gamma=None,\n              grow_policy=None, importance_type=None,\n              interaction_constraints=None, learning_rate=0.03, max_bin=None,\n              max_cat_threshold=None, max_cat_to_onehot=None,\n              max_delta_step=None, max_depth=5, max_leaves=None,\n              min_child_weight=5, missing=nan, monotone_constraints=None,\n              multi_strategy=None, n_estimators=150, n_jobs=None,\n              num_parallel_tree=None, ...)",
          "train_accuracy": 1.0,
          "val_accuracy": 1.0,
          "train_logloss": 0.019125269932739202,
          "val_logloss": 0.01904938496786702,
          "overfitting": 0.0,
          "success": true
        },
        "lightgbm": {
          "model": "LGBMClassifier(bagging_fraction=0.8, bagging_freq=5, feature_fraction=0.8,\n               lambda_l1=0.2, lambda_l2=1.5, learning_rate=0.03, max_depth=5,\n               metric='multi_logloss', min_data_in_leaf=10, n_estimators=150,\n               num_class=3, num_leaves=30, objective='multiclass',\n               random_state=42, verbose=-1)",
          "train_accuracy": 1.0,
          "val_accuracy": 1.0,
          "train_logloss": 0.007724634666161266,
          "val_logloss": 0.007729591544659295,
          "overfitting": 0.0,
          "success": true
        },
        "random_forest": {
          "model": "RandomForestClassifier(max_depth=8, min_samples_leaf=5, min_samples_split=10,\n                       random_state=42)",
          "train_accuracy": 1.0,
          "val_accuracy": 1.0,
          "train_logloss": 0.0331503380843572,
          "val_logloss": 0.04629859119306572,
          "overfitting": 0.0,
          "success": true
        },
        "logistic_regression": {
          "model": "LogisticRegression(C=0.1, max_iter=1000, random_state=42)",
          "train_accuracy": 1.0,
          "val_accuracy": 1.0,
          "train_logloss": 0.021689907893286876,
          "val_logloss": 0.026479359246040435,
          "overfitting": 0.0,
          "success": true
        }
      },
      "conservative_ensemble": {
        "models": {
          "xgboost": "XGBClassifier(base_score=None, booster=None, callbacks=None,\n              colsample_bylevel=None, colsample_bynode=None,\n              colsample_bytree=0.8, device=None, early_stopping_rounds=None,\n              enable_categorical=False, eval_metric='mlogloss',\n              feature_types=None, feature_weights=None, gamma=None,\n              grow_policy=None, importance_type=None,\n              interaction_constraints=None, learning_rate=0.03, max_bin=None,\n              max_cat_threshold=None, max_cat_to_onehot=None,\n              max_delta_step=None, max_depth=5, max_leaves=None,\n              min_child_weight=5, missing=nan, monotone_constraints=None,\n              multi_strategy=None, n_estimators=150, n_jobs=None,\n              num_parallel_tree=None, ...)",
          "lightgbm": "LGBMClassifier(bagging_fraction=0.8, bagging_freq=5, feature_fraction=0.8,\n               lambda_l1=0.2, lambda_l2=1.5, learning_rate=0.03, max_depth=5,\n               metric='multi_logloss', min_data_in_leaf=10, n_estimators=150,\n               num_class=3, num_leaves=30, objective='multiclass',\n               random_state=42, verbose=-1)",
          "random_forest": "RandomForestClassifier(max_depth=8, min_samples_leaf=5, min_samples_split=10,\n                       random_state=42)",
          "logistic_regression": "LogisticRegression(C=0.1, max_iter=1000, random_state=42)"
        },
        "weights": {
          "xgboost": 0.4001754372983308,
          "lightgbm": 0.3521543981896826,
          "random_forest": 0.14800154084236178,
          "logistic_regression": 0.09966862366962478
        },
        "type": "conservative_weighted",
        "training_results": {
          "xgboost": {
            "model": "XGBClassifier(base_score=None, booster=None, callbacks=None,\n              colsample_bylevel=None, colsample_bynode=None,\n              colsample_bytree=0.8, device=None, early_stopping_rounds=None,\n              enable_categorical=False, eval_metric='mlogloss',\n              feature_types=None, feature_weights=None, gamma=None,\n              grow_policy=None, importance_type=None,\n              interaction_constraints=None, learning_rate=0.03, max_bin=None,\n              max_cat_threshold=None, max_cat_to_onehot=None,\n              max_delta_step=None, max_depth=5, max_leaves=None,\n              min_child_weight=5, missing=nan, monotone_constraints=None,\n              multi_strategy=None, n_estimators=150, n_jobs=None,\n              num_parallel_tree=None, ...)",
            "train_accuracy": 1.0,
            "val_accuracy": 1.0,
            "train_logloss": 0.019125269932739202,
            "val_logloss": 0.01904938496786702,
            "overfitting": 0.0,
            "success": true
          },
          "lightgbm": {
            "model": "LGBMClassifier(bagging_fraction=0.8, bagging_freq=5, feature_fraction=0.8,\n               lambda_l1=0.2, lambda_l2=1.5, learning_rate=0.03, max_depth=5,\n               metric='multi_logloss', min_data_in_leaf=10, n_estimators=150,\n               num_class=3, num_leaves=30, objective='multiclass',\n               random_state=42, verbose=-1)",
            "train_accuracy": 1.0,
            "val_accuracy": 1.0,
            "train_logloss": 0.007724634666161266,
            "val_logloss": 0.007729591544659295,
            "overfitting": 0.0,
            "success": true
          },
          "random_forest": {
            "model": "RandomForestClassifier(max_depth=8, min_samples_leaf=5, min_samples_split=10,\n                       random_state=42)",
            "train_accuracy": 1.0,
            "val_accuracy": 1.0,
            "train_logloss": 0.0331503380843572,
            "val_logloss": 0.04629859119306572,
            "overfitting": 0.0,
            "success": true
          },
          "logistic_regression": {
            "model": "LogisticRegression(C=0.1, max_iter=1000, random_state=42)",
            "train_accuracy": 1.0,
            "val_accuracy": 1.0,
            "train_logloss": 0.021689907893286876,
            "val_logloss": 0.026479359246040435,
            "overfitting": 0.0,
            "success": true
          }
        }
      },
      "ensemble_results": {
        "val_accuracy": 1.0,
        "val_logloss": 0.019627778287645403,
        "success": true,
        "type": "conservative_weighted"
      },
      "calibrated_models": {
        "xgboost": "CalibratedClassifierCV(cv=3,\n                       estimator=XGBClassifier(base_score=None, booster=None,\n                                               callbacks=None,\n                                               colsample_bylevel=None,\n                                               colsample_bynode=None,\n                                               colsample_bytree=0.8,\n                                               device=None,\n                                               early_stopping_rounds=None,\n                                               enable_categorical=False,\n                                               eval_metric='mlogloss',\n                                               feature_types=None,\n                                               feature_weights=None, gamma=None,\n                                               grow_policy=None,\n                                               importance_type=None,\n                                               interaction_constraints=None,\n                                               learning_rate=0.03, max_bin=None,\n                                               max_cat_threshold=None,\n                                               max_cat_to_onehot=None,\n                                               max_delta_step=None, max_depth=5,\n                                               max_leaves=None,\n                                               min_child_weight=5, missing=nan,\n                                               monotone_constraints=None,\n                                               multi_strategy=None,\n                                               n_estimators=150, n_jobs=None,\n                                               num_parallel_tree=None, ...),\n                       method='isotonic')",
        "lightgbm": "CalibratedClassifierCV(cv=3,\n                       estimator=LGBMClassifier(bagging_fraction=0.8,\n                                                bagging_freq=5,\n                                                feature_fraction=0.8,\n                                                lambda_l1=0.2, lambda_l2=1.5,\n                                                learning_rate=0.03, max_depth=5,\n                                                metric='multi_logloss',\n                                                min_data_in_leaf=10,\n                                                n_estimators=150, num_class=3,\n                                                num_leaves=30,\n                                                objective='multiclass',\n                                                random_state=42, verbose=-1),\n                       method='isotonic')",
        "random_forest": "CalibratedClassifierCV(cv=3,\n                       estimator=RandomForestClassifier(max_depth=8,\n                                                        min_samples_leaf=5,\n                                                        min_samples_split=10,\n                                                        random_state=42),\n                       method='isotonic')",
        "logistic_regression": "CalibratedClassifierCV(cv=3,\n                       estimator=LogisticRegression(C=0.1, max_iter=1000,\n                                                    random_state=42),\n                       method='isotonic')"
      },
      "cv_results": {
        "mean_accuracy": 1.0,
        "std_accuracy": 0.0,
        "mean_log_loss": 0.04674941628646468,
        "std_log_loss": 0.022106056440222625,
        "n_folds": 3,
        "scores": {
          "accuracy": [
            1.0,
            1.0,
            1.0
          ],
          "log_loss": [
            0.07722474631808462,
            0.03754973865101127,
            0.02547376389029816
          ],
          "folds": [
            0,
            1,
            2
          ]
        }
      },
      "evaluation_results": {
        "accuracy": 1.0,
        "log_loss": 0.024329742892413127,
        "mean_confidence": 0.976011849321168,
        "classification_report": {
          "Away Win": {
            "precision": 1.0,
            "recall": 1.0,
            "f1-score": 1.0,
            "support": 51.0
          },
          "Draw": {
            "precision": 1.0,
            "recall": 1.0,
            "f1-score": 1.0,
            "support": 32.0
          },
          "Home Win": {
            "precision": 1.0,
            "recall": 1.0,
            "f1-score": 1.0,
            "support": 71.0
          },
          "accuracy": 1.0,
          "macro avg": {
            "precision": 1.0,
            "recall": 1.0,
            "f1-score": 1.0,
            "support": 154.0
          },
          "weighted avg": {
            "precision": 1.0,
            "recall": 1.0,
            "f1-score": 1.0,
            "support": 154.0
          }
        },
        "confusion_matrix": "[[51  0  0]\n [ 0 32  0]\n [ 0  0 71]]",
        "predictions": "[0 2 2 0 2 2 2 1 2 1 1 1 2 2 0 2 0 2 2 0 1 2 2 0 2 2 0 2 0 2 2 1 2 1 2 2 1\n 2 2 2 0 0 1 2 0 2 0 1 0 2 2 2 1 2 1 2 1 0 2 0 0 0 0 2 2 2 2 1 2 2 0 0 1 0\n 1 2 1 2 0 0 2 2 1 0 0 2 2 0 1 0 0 0 2 2 1 2 2 2 0 1 2 0 2 2 2 0 2 2 1 1 0\n 2 0 2 1 2 2 2 2 0 2 0 0 0 0 2 2 1 2 2 0 0 0 0 2 1 2 2 2 0 1 1 0 1 0 0 0 1\n 0 0 0 2 1 1]",
        "probabilities": "[[0.96038815 0.02030811 0.01930375]\n [0.00651443 0.00780865 0.98567692]\n [0.02315563 0.01945212 0.95739224]\n [0.96369966 0.02034735 0.015953  ]\n [0.00870771 0.00820246 0.98308984]\n [0.00939555 0.02051443 0.97009003]\n [0.00771039 0.01126958 0.98102005]\n [0.0126045  0.97076129 0.01663419]\n [0.00823913 0.00773049 0.98403037]\n [0.01574732 0.97125017 0.01300251]\n [0.01212544 0.9733366  0.01453795]\n [0.01644963 0.96016549 0.02338488]\n [0.00997736 0.01629136 0.97373128]\n [0.00796719 0.00650931 0.98552352]\n [0.97769259 0.0139895  0.00831789]\n [0.01294718 0.00641703 0.9806358 ]\n [0.98172879 0.01005027 0.00822094]\n [0.01185168 0.01452432 0.97362402]\n [0.0083861  0.00969872 0.98191517]\n [0.97040406 0.01897771 0.01061825]\n [0.03497017 0.92098063 0.04404921]\n [0.00694937 0.00754776 0.9855029 ]\n [0.00928627 0.01072965 0.97998409]\n [0.98036427 0.00935076 0.01028498]\n [0.00951121 0.00699885 0.98348993]\n [0.00840167 0.01415136 0.97744697]\n [0.97602008 0.01503181 0.00894812]\n [0.00696015 0.00888772 0.98415216]\n [0.97380613 0.01789113 0.00830276]\n [0.00610877 0.00773835 0.98615287]\n [0.01258772 0.01291087 0.97450141]\n [0.01156608 0.97766681 0.01076711]\n [0.00986588 0.01584128 0.97429283]\n [0.014688   0.97011365 0.01519834]\n [0.00995248 0.00735742 0.98269012]\n [0.00804918 0.00686584 0.98508497]\n [0.01095337 0.98038281 0.00866382]\n [0.01244955 0.00803103 0.97951944]\n [0.01185651 0.01519586 0.97294762]\n [0.00959082 0.01104204 0.97936715]\n [0.98316752 0.00985778 0.00697471]\n [0.98007033 0.01123426 0.0086954 ]\n [0.0082803  0.98280345 0.00891624]\n [0.00706341 0.0091028  0.98383378]\n [0.96541983 0.02074721 0.01383297]\n [0.0089791  0.011646   0.97937489]\n [0.97841847 0.01412405 0.0074575 ]\n [0.01249449 0.97745908 0.01004643]\n [0.95145463 0.02708093 0.02146445]\n [0.00820507 0.00705414 0.98474081]\n [0.01325963 0.01819936 0.968541  ]\n [0.00800616 0.00762024 0.9843736 ]\n [0.00877927 0.97879906 0.01242167]\n [0.00609769 0.0072648  0.9866375 ]\n [0.016117   0.96814223 0.01574076]\n [0.00896779 0.00858411 0.98244812]\n [0.01477202 0.9743761  0.0108519 ]\n [0.97891069 0.01037594 0.01071336]\n [0.01397901 0.01195694 0.97406406]\n [0.97863314 0.00981238 0.01155449]\n [0.97753442 0.01235244 0.01011314]\n [0.98039019 0.01090957 0.00870026]\n [0.98144126 0.00842132 0.01013744]\n [0.00681729 0.00649771 0.98668499]\n [0.00826021 0.00796837 0.98377143]\n [0.01322862 0.01515829 0.97161308]\n [0.00985395 0.02078005 0.96936599]\n [0.01791242 0.96691866 0.01516889]\n [0.0082731  0.01652537 0.97520153]\n [0.0117465  0.0245898  0.9636637 ]\n [0.95463114 0.02701928 0.01834958]\n [0.98402263 0.00867234 0.00730505]\n [0.01037871 0.97746187 0.0121594 ]\n [0.97583876 0.01555465 0.00860655]\n [0.01278456 0.96957423 0.01764121]\n [0.00784253 0.01190919 0.9802483 ]\n [0.01730386 0.96994624 0.0127499 ]\n [0.01401061 0.01001356 0.97597584]\n [0.98069501 0.01080642 0.00849857]\n [0.98341238 0.0085593  0.00802833]\n [0.00864839 0.0073152  0.98403643]\n [0.00754652 0.00673281 0.98572066]\n [0.02242355 0.94030638 0.03727004]\n [0.98356789 0.00883616 0.00759597]\n [0.96791421 0.01789938 0.01418642]\n [0.00728376 0.0087965  0.98391976]\n [0.01446334 0.01433335 0.97120331]\n [0.98209657 0.01046083 0.00744263]\n [0.01048578 0.97420783 0.01530636]\n [0.97587903 0.01572969 0.00839126]\n [0.9844267  0.00830385 0.00726948]\n [0.97566867 0.01399002 0.01034127]\n [0.0145969  0.0112832  0.97411991]\n [0.00857077 0.00785226 0.98357698]\n [0.01479496 0.97251366 0.01269135]\n [0.00771437 0.00993916 0.9823465 ]\n [0.00989786 0.00810951 0.98199261]\n [0.00815207 0.00909871 0.98274924]\n [0.9822199  0.01016214 0.00761794]\n [0.02204996 0.95634987 0.02160015]\n [0.00889976 0.01587016 0.97523009]\n [0.94979683 0.02723708 0.02296609]\n [0.00671002 0.00794476 0.98534522]\n [0.01352367 0.00847951 0.9779968 ]\n [0.00629172 0.00642318 0.98728511]\n [0.97922242 0.01165381 0.00912378]\n [0.00951757 0.01330139 0.97718103]\n [0.00741454 0.00751335 0.98507213]\n [0.02017441 0.9592599  0.0205657 ]\n [0.01092969 0.97738671 0.0116836 ]\n [0.98332132 0.00964151 0.00703719]\n [0.00997994 0.01319133 0.97682873]\n [0.98002999 0.00864444 0.01132554]\n [0.00780108 0.00717055 0.98502839]\n [0.01085758 0.9746706  0.01447182]\n [0.00874629 0.00979057 0.98146313]\n [0.01285513 0.00726655 0.97987832]\n [0.0118856  0.01054366 0.97757073]\n [0.00949903 0.01138503 0.97911596]\n [0.97159258 0.01881201 0.00959537]\n [0.00715247 0.00712234 0.98572518]\n [0.98402835 0.00700618 0.00896545]\n [0.96823777 0.01628213 0.01548009]\n [0.97900498 0.01196332 0.00903171]\n [0.95382363 0.02179573 0.02438067]\n [0.00866541 0.0112449  0.98008968]\n [0.00598271 0.00627553 0.98774179]\n [0.01471936 0.96417061 0.02111004]\n [0.01174276 0.00765265 0.98060458]\n [0.00967075 0.01260319 0.97772606]\n [0.96406713 0.02065221 0.01528068]\n [0.9703315  0.01982959 0.00983887]\n [0.98103217 0.00961981 0.009348  ]\n [0.97929549 0.01234289 0.00836163]\n [0.00700412 0.00754193 0.98545397]\n [0.00891109 0.97778829 0.01330061]\n [0.00719896 0.00892148 0.98387958]\n [0.00775826 0.00684735 0.9853944 ]\n [0.0110584  0.01387348 0.9750681 ]\n [0.97592259 0.01430001 0.00977738]\n [0.02108602 0.96579307 0.01312089]\n [0.01229476 0.97411131 0.01359392]\n [0.98135594 0.0089953  0.00964875]\n [0.01208749 0.97853949 0.00937301]\n [0.98140606 0.00964701 0.00894696]\n [0.9842184  0.00825437 0.00752725]\n [0.98065267 0.01131047 0.00803683]\n [0.01184278 0.97823164 0.00992555]\n [0.97857083 0.01088089 0.01054829]\n [0.97635415 0.01510382 0.00854202]\n [0.95182706 0.02236372 0.0258092 ]\n [0.02166972 0.0161407  0.9621896 ]\n [0.00948246 0.97677943 0.01373809]\n [0.01675317 0.96763443 0.0156124 ]]",
        "confidence_scores": "[0.96038815 0.98567692 0.95739224 0.96369966 0.98308984 0.97009003\n 0.98102005 0.97076129 0.98403037 0.97125017 0.9733366  0.96016549\n 0.97373128 0.98552352 0.97769259 0.9806358  0.98172879 0.97362402\n 0.98191517 0.97040406 0.92098063 0.9855029  0.97998409 0.98036427\n 0.98348993 0.97744697 0.97602008 0.98415216 0.97380613 0.98615287\n 0.97450141 0.97766681 0.97429283 0.97011365 0.98269012 0.98508497\n 0.98038281 0.97951944 0.97294762 0.97936715 0.98316752 0.98007033\n 0.98280345 0.98383378 0.96541983 0.97937489 0.97841847 0.97745908\n 0.95145463 0.98474081 0.968541   0.9843736  0.97879906 0.9866375\n 0.96814223 0.98244812 0.9743761  0.97891069 0.97406406 0.97863314\n 0.97753442 0.98039019 0.98144126 0.98668499 0.98377143 0.97161308\n 0.96936599 0.96691866 0.97520153 0.9636637  0.95463114 0.98402263\n 0.97746187 0.97583876 0.96957423 0.9802483  0.96994624 0.97597584\n 0.98069501 0.98341238 0.98403643 0.98572066 0.94030638 0.98356789\n 0.96791421 0.98391976 0.97120331 0.98209657 0.97420783 0.97587903\n 0.9844267  0.97566867 0.97411991 0.98357698 0.97251366 0.9823465\n 0.98199261 0.98274924 0.9822199  0.95634987 0.97523009 0.94979683\n 0.98534522 0.9779968  0.98728511 0.97922242 0.97718103 0.98507213\n 0.9592599  0.97738671 0.98332132 0.97682873 0.98002999 0.98502839\n 0.9746706  0.98146313 0.97987832 0.97757073 0.97911596 0.97159258\n 0.98572518 0.98402835 0.96823777 0.97900498 0.95382363 0.98008968\n 0.98774179 0.96417061 0.98060458 0.97772606 0.96406713 0.9703315\n 0.98103217 0.97929549 0.98545397 0.97778829 0.98387958 0.9853944\n 0.9750681  0.97592259 0.96579307 0.97411131 0.98135594 0.97853949\n 0.98140606 0.9842184  0.98065267 0.97823164 0.97857083 0.97635415\n 0.95182706 0.9621896  0.97677943 0.96763443]"
      },
      "predictions": {
        "predictions": "[0 2 2 0 2 2 2 1 2 1 1 1 2 2 0 2 0 2 2 0 1 2 2 0 2 2 0 2 0 2 2 1 2 1 2 2 1\n 2 2 2 0 0 1 2 0 2 0 1 0 2 2 2 1 2 1 2 1 0 2 0 0 0 0 2 2 2 2 1 2 2 0 0 1 0\n 1 2 1 2 0 0 2 2 1 0 0 2 2 0 1 0 0 0 2 2 1 2 2 2 0 1 2 0 2 2 2 0 2 2 1 1 0\n 2 0 2 1 2 2 2 2 0 2 0 0 0 0 2 2 1 2 2 0 0 0 0 2 1 2 2 2 0 1 1 0 1 0 0 0 1\n 0 0 0 2 1 1]",
        "probabilities": "[[0.96038815 0.02030811 0.01930375]\n [0.00651443 0.00780865 0.98567692]\n [0.02315563 0.01945212 0.95739224]\n [0.96369966 0.02034735 0.015953  ]\n [0.00870771 0.00820246 0.98308984]\n [0.00939555 0.02051443 0.97009003]\n [0.00771039 0.01126958 0.98102005]\n [0.0126045  0.97076129 0.01663419]\n [0.00823913 0.00773049 0.98403037]\n [0.01574732 0.97125017 0.01300251]\n [0.01212544 0.9733366  0.01453795]\n [0.01644963 0.96016549 0.02338488]\n [0.00997736 0.01629136 0.97373128]\n [0.00796719 0.00650931 0.98552352]\n [0.97769259 0.0139895  0.00831789]\n [0.01294718 0.00641703 0.9806358 ]\n [0.98172879 0.01005027 0.00822094]\n [0.01185168 0.01452432 0.97362402]\n [0.0083861  0.00969872 0.98191517]\n [0.97040406 0.01897771 0.01061825]\n [0.03497017 0.92098063 0.04404921]\n [0.00694937 0.00754776 0.9855029 ]\n [0.00928627 0.01072965 0.97998409]\n [0.98036427 0.00935076 0.01028498]\n [0.00951121 0.00699885 0.98348993]\n [0.00840167 0.01415136 0.97744697]\n [0.97602008 0.01503181 0.00894812]\n [0.00696015 0.00888772 0.98415216]\n [0.97380613 0.01789113 0.00830276]\n [0.00610877 0.00773835 0.98615287]\n [0.01258772 0.01291087 0.97450141]\n [0.01156608 0.97766681 0.01076711]\n [0.00986588 0.01584128 0.97429283]\n [0.014688   0.97011365 0.01519834]\n [0.00995248 0.00735742 0.98269012]\n [0.00804918 0.00686584 0.98508497]\n [0.01095337 0.98038281 0.00866382]\n [0.01244955 0.00803103 0.97951944]\n [0.01185651 0.01519586 0.97294762]\n [0.00959082 0.01104204 0.97936715]\n [0.98316752 0.00985778 0.00697471]\n [0.98007033 0.01123426 0.0086954 ]\n [0.0082803  0.98280345 0.00891624]\n [0.00706341 0.0091028  0.98383378]\n [0.96541983 0.02074721 0.01383297]\n [0.0089791  0.011646   0.97937489]\n [0.97841847 0.01412405 0.0074575 ]\n [0.01249449 0.97745908 0.01004643]\n [0.95145463 0.02708093 0.02146445]\n [0.00820507 0.00705414 0.98474081]\n [0.01325963 0.01819936 0.968541  ]\n [0.00800616 0.00762024 0.9843736 ]\n [0.00877927 0.97879906 0.01242167]\n [0.00609769 0.0072648  0.9866375 ]\n [0.016117   0.96814223 0.01574076]\n [0.00896779 0.00858411 0.98244812]\n [0.01477202 0.9743761  0.0108519 ]\n [0.97891069 0.01037594 0.01071336]\n [0.01397901 0.01195694 0.97406406]\n [0.97863314 0.00981238 0.01155449]\n [0.97753442 0.01235244 0.01011314]\n [0.98039019 0.01090957 0.00870026]\n [0.98144126 0.00842132 0.01013744]\n [0.00681729 0.00649771 0.98668499]\n [0.00826021 0.00796837 0.98377143]\n [0.01322862 0.01515829 0.97161308]\n [0.00985395 0.02078005 0.96936599]\n [0.01791242 0.96691866 0.01516889]\n [0.0082731  0.01652537 0.97520153]\n [0.0117465  0.0245898  0.9636637 ]\n [0.95463114 0.02701928 0.01834958]\n [0.98402263 0.00867234 0.00730505]\n [0.01037871 0.97746187 0.0121594 ]\n [0.97583876 0.01555465 0.00860655]\n [0.01278456 0.96957423 0.01764121]\n [0.00784253 0.01190919 0.9802483 ]\n [0.01730386 0.96994624 0.0127499 ]\n [0.01401061 0.01001356 0.97597584]\n [0.98069501 0.01080642 0.00849857]\n [0.98341238 0.0085593  0.00802833]\n [0.00864839 0.0073152  0.98403643]\n [0.00754652 0.00673281 0.98572066]\n [0.02242355 0.94030638 0.03727004]\n [0.98356789 0.00883616 0.00759597]\n [0.96791421 0.01789938 0.01418642]\n [0.00728376 0.0087965  0.98391976]\n [0.01446334 0.01433335 0.97120331]\n [0.98209657 0.01046083 0.00744263]\n [0.01048578 0.97420783 0.01530636]\n [0.97587903 0.01572969 0.00839126]\n [0.9844267  0.00830385 0.00726948]\n [0.97566867 0.01399002 0.01034127]\n [0.0145969  0.0112832  0.97411991]\n [0.00857077 0.00785226 0.98357698]\n [0.01479496 0.97251366 0.01269135]\n [0.00771437 0.00993916 0.9823465 ]\n [0.00989786 0.00810951 0.98199261]\n [0.00815207 0.00909871 0.98274924]\n [0.9822199  0.01016214 0.00761794]\n [0.02204996 0.95634987 0.02160015]\n [0.00889976 0.01587016 0.97523009]\n [0.94979683 0.02723708 0.02296609]\n [0.00671002 0.00794476 0.98534522]\n [0.01352367 0.00847951 0.9779968 ]\n [0.00629172 0.00642318 0.98728511]\n [0.97922242 0.01165381 0.00912378]\n [0.00951757 0.01330139 0.97718103]\n [0.00741454 0.00751335 0.98507213]\n [0.02017441 0.9592599  0.0205657 ]\n [0.01092969 0.97738671 0.0116836 ]\n [0.98332132 0.00964151 0.00703719]\n [0.00997994 0.01319133 0.97682873]\n [0.98002999 0.00864444 0.01132554]\n [0.00780108 0.00717055 0.98502839]\n [0.01085758 0.9746706  0.01447182]\n [0.00874629 0.00979057 0.98146313]\n [0.01285513 0.00726655 0.97987832]\n [0.0118856  0.01054366 0.97757073]\n [0.00949903 0.01138503 0.97911596]\n [0.97159258 0.01881201 0.00959537]\n [0.00715247 0.00712234 0.98572518]\n [0.98402835 0.00700618 0.00896545]\n [0.96823777 0.01628213 0.01548009]\n [0.97900498 0.01196332 0.00903171]\n [0.95382363 0.02179573 0.02438067]\n [0.00866541 0.0112449  0.98008968]\n [0.00598271 0.00627553 0.98774179]\n [0.01471936 0.96417061 0.02111004]\n [0.01174276 0.00765265 0.98060458]\n [0.00967075 0.01260319 0.97772606]\n [0.96406713 0.02065221 0.01528068]\n [0.9703315  0.01982959 0.00983887]\n [0.98103217 0.00961981 0.009348  ]\n [0.97929549 0.01234289 0.00836163]\n [0.00700412 0.00754193 0.98545397]\n [0.00891109 0.97778829 0.01330061]\n [0.00719896 0.00892148 0.98387958]\n [0.00775826 0.00684735 0.9853944 ]\n [0.0110584  0.01387348 0.9750681 ]\n [0.97592259 0.01430001 0.00977738]\n [0.02108602 0.96579307 0.01312089]\n [0.01229476 0.97411131 0.01359392]\n [0.98135594 0.0089953  0.00964875]\n [0.01208749 0.97853949 0.00937301]\n [0.98140606 0.00964701 0.00894696]\n [0.9842184  0.00825437 0.00752725]\n [0.98065267 0.01131047 0.00803683]\n [0.01184278 0.97823164 0.00992555]\n [0.97857083 0.01088089 0.01054829]\n [0.97635415 0.01510382 0.00854202]\n [0.95182706 0.02236372 0.0258092 ]\n [0.02166972 0.0161407  0.9621896 ]\n [0.00948246 0.97677943 0.01373809]\n [0.01675317 0.96763443 0.0156124 ]]",
        "confidence_scores": "[0.96038815 0.98567692 0.95739224 0.96369966 0.98308984 0.97009003\n 0.98102005 0.97076129 0.98403037 0.97125017 0.9733366  0.96016549\n 0.97373128 0.98552352 0.97769259 0.9806358  0.98172879 0.97362402\n 0.98191517 0.97040406 0.92098063 0.9855029  0.97998409 0.98036427\n 0.98348993 0.97744697 0.97602008 0.98415216 0.97380613 0.98615287\n 0.97450141 0.97766681 0.97429283 0.97011365 0.98269012 0.98508497\n 0.98038281 0.97951944 0.97294762 0.97936715 0.98316752 0.98007033\n 0.98280345 0.98383378 0.96541983 0.97937489 0.97841847 0.97745908\n 0.95145463 0.98474081 0.968541   0.9843736  0.97879906 0.9866375\n 0.96814223 0.98244812 0.9743761  0.97891069 0.97406406 0.97863314\n 0.97753442 0.98039019 0.98144126 0.98668499 0.98377143 0.97161308\n 0.96936599 0.96691866 0.97520153 0.9636637  0.95463114 0.98402263\n 0.97746187 0.97583876 0.96957423 0.9802483  0.96994624 0.97597584\n 0.98069501 0.98341238 0.98403643 0.98572066 0.94030638 0.98356789\n 0.96791421 0.98391976 0.97120331 0.98209657 0.97420783 0.97587903\n 0.9844267  0.97566867 0.97411991 0.98357698 0.97251366 0.9823465\n 0.98199261 0.98274924 0.9822199  0.95634987 0.97523009 0.94979683\n 0.98534522 0.9779968  0.98728511 0.97922242 0.97718103 0.98507213\n 0.9592599  0.97738671 0.98332132 0.97682873 0.98002999 0.98502839\n 0.9746706  0.98146313 0.97987832 0.97757073 0.97911596 0.97159258\n 0.98572518 0.98402835 0.96823777 0.97900498 0.95382363 0.98008968\n 0.98774179 0.96417061 0.98060458 0.97772606 0.96406713 0.9703315\n 0.98103217 0.97929549 0.98545397 0.97778829 0.98387958 0.9853944\n 0.9750681  0.97592259 0.96579307 0.97411131 0.98135594 0.97853949\n 0.98140606 0.9842184  0.98065267 0.97823164 0.97857083 0.97635415\n 0.95182706 0.9621896  0.97677943 0.96763443]",
        "high_confidence_mask": "[ True  True  True  True  True  True  True  True  True  True  True  True\n  True  True  True  True  True  True  True  True  True  True  True  True\n  True  True  True  True  True  True  True  True  True  True  True  True\n  True  True  True  True  True  True  True  True  True  True  True  True\n  True  True  True  True  True  True  True  True  True  True  True  True\n  True  True  True  True  True  True  True  True  True  True  True  True\n  True  True  True  True  True  True  True  True  True  True  True  True\n  True  True  True  True  True  True  True  True  True  True  True  True\n  True  True  True  True  True  True  True  True  True  True  True  True\n  True  True  True  True  True  True  True  True  True  True  True  True\n  True  True  True  True  True  True  True  True  True  True  True  True\n  True  True  True  True  True  True  True  True  True  True  True  True\n  True  True  True  True  True  True  True  True  True  True]",
        "low_confidence_mask": "[False False False False False False False False False False False False\n False False False False False False False False False False False False\n False False False False False False False False False False False False\n False False False False False False False False False False False False\n False False False False False False False False False False False False\n False False False False False False False False False False False False\n False False False False False False False False False False False False\n False False False False False False False False False False False False\n False False False False False False False False False False False False\n False False False False False False False False False False False False\n False False False False False False False False False False False False\n False False False False False False False False False False False False\n False False False False False False False False False False]",
        "high_confidence_count": "154",
        "low_confidence_count": "0",
        "confidence_threshold": 0.6
      },
      "feature_importance": {
        "xgboost": "                 feature  importance\n120       result_encoded    0.363995\n98              home_win    0.124899\n99                  draw    0.113327\n100             away_win    0.104969\n104       home_advantage    0.098458\n..                   ...         ...\n236   team_strength_diff    0.000000\n237   home_team_win_rate    0.000000\n238   away_team_win_rate    0.000000\n239  home_team_avg_goals    0.000000\n240  away_team_avg_goals    0.000000\n\n[241 rows x 2 columns]",
        "lightgbm": "                 feature  importance\n100             away_win         127\n98              home_win         125\n99                  draw         117\n101      goal_difference          42\n232        match_balance          27\n..                   ...         ...\n236   team_strength_diff           0\n237   home_team_win_rate           0\n238   away_team_win_rate           0\n239  home_team_avg_goals           0\n240  away_team_avg_goals           0\n\n[241 rows x 2 columns]",
        "random_forest": "                      feature  importance\n100                  away_win    0.110646\n120            result_encoded    0.100103\n99                       draw    0.092436\n104            home_advantage    0.087865\n101           goal_difference    0.087605\n..                        ...         ...\n215      away_position_change    0.000000\n211   away_clean_sheet_streak    0.000000\n210   home_clean_sheet_streak    0.000000\n214      home_position_change    0.000000\n225  transfer_learning_factor    0.000000\n\n[241 rows x 2 columns]",
        "ensemble": "                      feature  importance\n100                  away_win   44.883344\n98                   home_win   44.176342\n99                       draw   41.354470\n101           goal_difference   14.874905\n232             match_balance    9.579641\n..                        ...         ...\n215      away_position_change    0.000000\n211   away_clean_sheet_streak    0.000000\n210   home_clean_sheet_streak    0.000000\n214      home_position_change    0.000000\n225  transfer_learning_factor    0.000000\n\n[241 rows x 2 columns]"
      },
      "uncertainty_estimates": {
        "mean_prediction": "[[0.3967778  0.16554402 0.43767818]\n [0.35860468 0.16430054 0.47709478]\n [0.39462445 0.16651452 0.43886104]\n [0.30121162 0.26156623 0.43722215]\n [0.31842962 0.26245876 0.41911162]\n [0.32111961 0.20299904 0.47588134]\n [0.35797268 0.24178559 0.40024172]\n [0.33938199 0.18412442 0.47649359]\n [0.35751474 0.10680298 0.53568228]\n [0.32106318 0.18413351 0.4948033 ]\n [0.37692368 0.20395545 0.41912088]\n [0.22413434 0.26076153 0.51510414]\n [0.29909086 0.22345552 0.47745362]\n [0.35955268 0.22295987 0.41748746]\n [0.26202062 0.28008713 0.45789225]\n [0.39677667 0.12625092 0.47697241]\n [0.31944712 0.14668251 0.53387037]\n [0.3197769  0.20380009 0.47642301]\n [0.32086636 0.18296902 0.49616463]\n [0.24304412 0.18430246 0.57265343]\n [0.29992319 0.22337106 0.47670575]\n [0.32012023 0.2605059  0.41937386]\n [0.31932515 0.22279741 0.45787744]\n [0.3776635  0.18315981 0.43917669]\n [0.22247555 0.25985301 0.51767144]\n [0.3598854  0.27861688 0.36149773]\n [0.28240683 0.29838709 0.41920607]\n [0.30121712 0.24024608 0.45853679]\n [0.28119989 0.30045452 0.41834558]\n [0.26195808 0.2983395  0.43970242]\n [0.33888675 0.18399609 0.47711717]\n [0.24353573 0.27832416 0.4781401 ]\n [0.35647659 0.1076177  0.53590571]\n [0.22252023 0.22302698 0.5544528 ]\n [0.22523956 0.29788577 0.47687468]\n [0.20478539 0.29855553 0.49665907]\n [0.35852683 0.14657596 0.49489721]\n [0.3781794  0.18475636 0.43706424]\n [0.31984323 0.28060687 0.3995499 ]\n [0.35862269 0.20318786 0.43818945]\n [0.47298063 0.18615265 0.34086672]\n [0.3589471  0.14552542 0.49552748]\n [0.3013574  0.20241303 0.49622957]\n [0.39720604 0.16612841 0.43666556]\n [0.35751344 0.2039532  0.43853337]\n [0.37706677 0.20377831 0.41915492]\n [0.26165282 0.37655666 0.36179052]\n [0.35709492 0.08960611 0.55329898]\n [0.31966095 0.14670815 0.53363089]\n [0.29938468 0.18595763 0.51465768]\n [0.28134349 0.18205835 0.53659816]\n [0.28095828 0.24249904 0.47654268]\n [0.37656212 0.18477884 0.43865904]\n [0.31974119 0.18453565 0.49572315]\n [0.39553168 0.22385913 0.38060918]\n [0.39608485 0.20504105 0.3988741 ]\n [0.37603881 0.10719801 0.51676319]\n [0.3584853  0.18405479 0.45745991]\n [0.3380678  0.20387464 0.45805756]\n [0.41508261 0.30147649 0.2834409 ]\n [0.30009244 0.30049086 0.3994167 ]\n [0.35868198 0.22315863 0.4181594 ]\n [0.32024412 0.2036337  0.47612218]\n [0.41571252 0.14567259 0.43861488]\n [0.30136797 0.29888592 0.39974611]\n [0.30076712 0.20425355 0.49497933]\n [0.39878714 0.25965692 0.34155595]\n [0.41682441 0.24224229 0.3409333 ]\n [0.37671923 0.20480937 0.41847141]\n [0.35850458 0.10757739 0.53391803]\n [0.33960758 0.20215483 0.4582376 ]\n [0.33799451 0.28035215 0.38165334]\n [0.35872402 0.16514029 0.47613569]\n [0.47307888 0.14657038 0.38035073]\n [0.49296603 0.18516486 0.32186911]\n [0.2819533  0.27906551 0.43898119]\n [0.41567887 0.24163837 0.34268275]\n [0.41623402 0.18516882 0.39859716]\n [0.26147267 0.10709532 0.63143201]\n [0.33906386 0.24002287 0.42091327]\n [0.27975958 0.20327675 0.51696366]\n [0.18449563 0.26106896 0.55443541]\n [0.26033967 0.28224637 0.45741396]\n [0.32018499 0.24141231 0.4384027 ]\n [0.24349567 0.260748   0.49575634]\n [0.33957825 0.16433131 0.49609045]\n [0.35753951 0.16537751 0.47708298]\n [0.20423455 0.29885249 0.49691297]\n [0.31945845 0.16618875 0.5143528 ]\n [0.37749279 0.1267827  0.49572452]\n [0.37684872 0.20451387 0.41863741]\n [0.28204987 0.31851686 0.39943327]\n [0.33988737 0.20411281 0.45599982]\n [0.35942714 0.20359158 0.43698127]\n [0.39702215 0.12691478 0.47606307]\n [0.31994322 0.29999106 0.38006572]\n [0.32054069 0.25908613 0.42037318]\n [0.28090491 0.24234662 0.47674847]\n [0.37765917 0.20362034 0.41872049]\n [0.34016486 0.16451882 0.49531632]\n [0.35902094 0.22270247 0.41827658]\n [0.24131551 0.26296484 0.49571965]\n [0.31899094 0.24354983 0.43745923]\n [0.34021565 0.20247975 0.45730461]\n [0.22366694 0.28062263 0.49571043]\n [0.32052544 0.33799024 0.34148432]\n [0.319467   0.1836362  0.4968968 ]\n [0.33941441 0.31831536 0.34227023]\n [0.45469105 0.1452101  0.40009885]\n [0.43461359 0.20442768 0.36095874]\n [0.37743551 0.16567939 0.4568851 ]\n [0.31986826 0.20321962 0.47691213]\n [0.30018745 0.18412409 0.51568846]\n [0.2618287  0.22263672 0.51553458]\n [0.35677551 0.32074195 0.32248255]\n [0.35867595 0.20445554 0.43686852]\n [0.39779694 0.22232304 0.37988003]\n [0.35917377 0.22135055 0.41947568]\n [0.29922247 0.2607009  0.44007663]\n [0.4161915  0.20413535 0.37967316]\n [0.28093714 0.22153773 0.49752513]\n [0.39777857 0.20334728 0.39887415]\n [0.41573663 0.14657397 0.43768939]\n [0.29999352 0.14623703 0.55376945]\n [0.31822506 0.10784285 0.57393209]\n [0.31969674 0.22316863 0.45713463]\n [0.41705385 0.14559938 0.43734677]\n [0.22478208 0.29814762 0.4770703 ]\n [0.24288712 0.22246507 0.5346478 ]\n [0.26116541 0.30102608 0.43780851]\n [0.28102851 0.2036713  0.51530019]\n [0.30140163 0.2411363  0.45746207]\n [0.28130178 0.25950045 0.45919776]\n [0.26194513 0.1837117  0.55434317]\n [0.49263428 0.16617576 0.34118995]\n [0.28081997 0.26109016 0.45808987]\n [0.26083587 0.28160131 0.45756281]\n [0.28019596 0.18331298 0.53649107]\n [0.31735646 0.18578348 0.49686007]\n [0.28184942 0.2423214  0.47582918]\n [0.26139136 0.14536195 0.59324669]\n [0.2822512  0.22084385 0.49690496]\n [0.37724023 0.16294841 0.45981135]\n [0.32103251 0.3370856  0.3418819 ]\n [0.30112633 0.20356223 0.49531145]\n [0.16591725 0.29913214 0.53495061]\n [0.3205865  0.2787765  0.400637  ]\n [0.29946402 0.26192439 0.43861158]\n [0.3772589  0.18538888 0.43735222]\n [0.32184637 0.22088528 0.45726835]\n [0.37773769 0.18413207 0.43813024]\n [0.31972968 0.12621292 0.5540574 ]\n [0.35744036 0.16533282 0.47722682]\n [0.3770014  0.16677246 0.45622614]]",
        "prediction_variance": "[[0.2236373  0.12353617 0.23060885]\n [0.21394999 0.12200323 0.23288564]\n [0.22200891 0.1246825  0.23131657]\n [0.19510298 0.17653505 0.22971029]\n [0.19992034 0.17629638 0.22660236]\n [0.2026429  0.1459695  0.23225403]\n [0.21288723 0.16658525 0.22441843]\n [0.2083664  0.13547762 0.2331847 ]\n [0.21454454 0.08145128 0.23319369]\n [0.20432435 0.13685437 0.23482901]\n [0.21890564 0.14807995 0.22913995]\n [0.16136449 0.17852994 0.23424734]\n [0.19450803 0.15822043 0.2338603 ]\n [0.21478384 0.15748244 0.2273951 ]\n [0.17807071 0.18474336 0.23140232]\n [0.22341912 0.09619903 0.23362144]\n [0.20262309 0.11070004 0.23218601]\n [0.20284909 0.14786281 0.2335671 ]\n [0.2045014  0.13584079 0.23451461]\n [0.1708564  0.13705504 0.22823748]\n [0.1940298  0.15746751 0.23297759]\n [0.20334236 0.17721108 0.22895884]\n [0.202006   0.15790938 0.23222204]\n [0.21918695 0.13416153 0.23110367]\n [0.15801335 0.17555496 0.23246188]\n [0.2140791  0.18334874 0.21474384]\n [0.18755353 0.19168628 0.22677427]\n [0.19537859 0.16611059 0.23208281]\n [0.18633684 0.19240384 0.22593212]\n [0.17834158 0.19202827 0.22996833]\n [0.2088115  0.13544921 0.23376555]\n [0.16983358 0.18458195 0.23339763]\n [0.21431579 0.08302108 0.23375478]\n [0.15914259 0.15850656 0.23058685]\n [0.1602739  0.19220819 0.2324845 ]\n [0.14803328 0.19150042 0.23217112]\n [0.214802   0.11157395 0.23400748]\n [0.21946362 0.13596344 0.23057595]\n [0.2024064  0.18482822 0.22412502]\n [0.21431801 0.14660484 0.23059943]\n [0.23139211 0.13665495 0.20996242]\n [0.21397301 0.10909426 0.23190477]\n [0.19669302 0.1472471  0.23456866]\n [0.223387   0.1246117  0.23055681]\n [0.21273515 0.14559726 0.22927712]\n [0.21800839 0.14613973 0.2271168 ]\n [0.17687247 0.21671273 0.21559604]\n [0.21431367 0.06809545 0.23003847]\n [0.20284741 0.11166885 0.23237479]\n [0.1952896  0.13676841 0.23367783]\n [0.18753433 0.1332212  0.23192575]\n [0.18728571 0.1685104  0.23350133]\n [0.21702285 0.13427624 0.22914933]\n [0.20169792 0.13559133 0.23302939]\n [0.2224388  0.15839049 0.2205513 ]\n [0.22043735 0.14566507 0.2225041 ]\n [0.21835344 0.08249949 0.23421511]\n [0.21503149 0.13537275 0.23259672]\n [0.20823454 0.14717623 0.23283881]\n [0.22450023 0.19339605 0.1886702 ]\n [0.19471981 0.1933486  0.22447784]\n [0.213144   0.15698961 0.22680381]\n [0.2026249  0.14608088 0.23256298]\n [0.22671711 0.10990873 0.23108487]\n [0.19491247 0.19173951 0.22347414]\n [0.19640129 0.14771724 0.23458473]\n [0.22300365 0.17486661 0.21002761]\n [0.2266078  0.16864643 0.21020437]\n [0.21858538 0.14786934 0.22847263]\n [0.21390835 0.08153218 0.23133167]\n [0.20931572 0.14539387 0.2321018 ]\n [0.2068477  0.18325812 0.21958078]\n [0.21571989 0.12462876 0.23437812]\n [0.23134463 0.1095918  0.22066744]\n [0.23293809 0.13588105 0.20455003]\n [0.18582113 0.18217452 0.22920351]\n [0.22535486 0.16654064 0.21098311]\n [0.22729204 0.13607241 0.2249837 ]\n [0.18062332 0.08282372 0.21685314]\n [0.20774403 0.16476431 0.22695047]\n [0.18613173 0.14668763 0.23380227]\n [0.13646708 0.17787235 0.23093506]\n [0.17672467 0.18498213 0.23073945]\n [0.20355806 0.16858828 0.23220866]\n [0.17111692 0.1774641  0.23387171]\n [0.20766487 0.12144774 0.23273956]\n [0.21333993 0.12264301 0.23339241]\n [0.14941772 0.19329597 0.23435085]\n [0.20298458 0.12414138 0.23389673]\n [0.21943109 0.09727646 0.23422846]\n [0.21915477 0.14800456 0.22840057]\n [0.18726089 0.19930345 0.22398592]\n [0.20990489 0.14785099 0.23259065]\n [0.21479254 0.14702531 0.2302196 ]\n [0.22396578 0.09682999 0.23411854]\n [0.2019425  0.19369657 0.22094659]\n [0.20096631 0.17284326 0.22612594]\n [0.18700697 0.1681473  0.23301598]\n [0.21892639 0.14749866 0.22832188]\n [0.20966916 0.12342553 0.23386184]\n [0.21492474 0.15800779 0.22787866]\n [0.16869171 0.17824888 0.23407007]\n [0.20255659 0.16954502 0.23112676]\n [0.20850089 0.14601987 0.23177191]\n [0.15854624 0.18399466 0.23107004]\n [0.20219011 0.20643593 0.20963215]\n [0.20317836 0.13558183 0.23438496]\n [0.20812184 0.1988584  0.2095813 ]\n [0.22970748 0.10738081 0.22376463]\n [0.22871849 0.14677024 0.21638464]\n [0.22006162 0.1241506  0.23279941]\n [0.20254484 0.14621688 0.23336023]\n [0.19474639 0.13512191 0.23309897]\n [0.17914677 0.15704999 0.23252326]\n [0.21275393 0.20118529 0.20376572]\n [0.21481418 0.14780027 0.23058039]\n [0.22364432 0.1577729  0.22095478]\n [0.21348359 0.15518145 0.22680874]\n [0.19368634 0.17671484 0.2304076 ]\n [0.22631065 0.14616016 0.21961258]\n [0.1858274  0.15535935 0.23273495]\n [0.22505709 0.14764083 0.22549282]\n [0.22715212 0.111121   0.23074684]\n [0.19526931 0.11108484 0.23120531]\n [0.20131148 0.08288895 0.22767622]\n [0.20207869 0.15726334 0.23196408]\n [0.22683625 0.10991888 0.2300072 ]\n [0.15979873 0.19175221 0.2323062 ]\n [0.16969203 0.15800378 0.23188718]\n [0.17821401 0.19402013 0.22972471]\n [0.18742989 0.14659348 0.23286522]\n [0.19582587 0.16685713 0.23203773]\n [0.18588968 0.17254598 0.23025342]\n [0.17842557 0.13470312 0.23002598]\n [0.23218252 0.1235332  0.20974363]\n [0.18733622 0.17684597 0.23257303]\n [0.17776603 0.18694529 0.23203378]\n [0.18793132 0.13523755 0.23304054]\n [0.20096806 0.13622746 0.23390255]\n [0.18842737 0.16891621 0.23413783]\n [0.17930008 0.11027082 0.22498356]\n [0.18853884 0.15625111 0.23335005]\n [0.21849463 0.12016786 0.23246113]\n [0.2035544  0.20639918 0.20983574]\n [0.19598137 0.14720922 0.23392632]\n [0.12405753 0.1931948  0.23085686]\n [0.20314604 0.18487183 0.22497154]\n [0.19569508 0.17858295 0.23140071]\n [0.21952417 0.13646403 0.23058498]\n [0.20439271 0.15707076 0.23216502]\n [0.21785082 0.13402741 0.22944073]\n [0.2033624  0.09615313 0.23098529]\n [0.21542683 0.12511042 0.23533459]\n [0.21828465 0.12225446 0.2305999 ]]",
        "confidence_intervals": {
          "lower": "[[0.00706239 0.00724768 0.00735505]\n [0.00727716 0.00648808 0.00777543]\n [0.00684701 0.00701129 0.00760092]\n [0.0071206  0.00643779 0.00726948]\n [0.00697503 0.00735979 0.0082526 ]\n [0.00632357 0.00704316 0.00731178]\n [0.00712616 0.0065596  0.00733601]\n [0.00738065 0.00658537 0.00761794]\n [0.00687267 0.00727614 0.00809667]\n [0.00629172 0.00641842 0.00713176]\n [0.00629172 0.00641842 0.00709746]\n [0.00684943 0.00643995 0.00708946]\n [0.00610235 0.00632552 0.00809008]\n [0.00661229 0.0066211  0.00760092]\n [0.00727716 0.00675858 0.00726948]\n [0.0067663  0.00685151 0.00744263]\n [0.00731403 0.0069344  0.0074575 ]\n [0.00690317 0.00680511 0.00716292]\n [0.00645201 0.00675858 0.00716786]\n [0.00626819 0.00690923 0.00809008]\n [0.00694937 0.00673281 0.00807826]\n [0.00610877 0.00655271 0.00749359]\n [0.00643969 0.00658049 0.00744597]\n [0.00673415 0.00661047 0.00752725]\n [0.00696169 0.00652278 0.00854395]\n [0.00719896 0.00654959 0.00748207]\n [0.00630033 0.00654794 0.00732748]\n [0.00690317 0.00650032 0.00744597]\n [0.00794103 0.00735342 0.00807167]\n [0.00702461 0.00699885 0.00866382]\n [0.00694937 0.00685151 0.00757632]\n [0.00701746 0.00648808 0.00761794]\n [0.00629172 0.00642318 0.00733935]\n [0.00671002 0.00679267 0.00754278]\n [0.0067663  0.00689577 0.00727748]\n [0.00661471 0.00706436 0.00785326]\n [0.00697005 0.0071544  0.00698877]\n [0.00685933 0.00694167 0.00708946]\n [0.00673415 0.00689388 0.00709746]\n [0.0071215  0.00658657 0.00760092]\n [0.00697005 0.00696695 0.00754271]\n [0.00706704 0.00751978 0.00727748]\n [0.00684701 0.00701129 0.00763999]\n [0.00600858 0.00640419 0.00763999]\n [0.00643969 0.00688309 0.00732748]\n [0.00623546 0.00648808 0.00703719]\n [0.0070375  0.00654959 0.00823935]\n [0.00680957 0.00680511 0.00714746]\n [0.00643969 0.00658537 0.00760092]\n [0.00690317 0.00655061 0.00768333]\n [0.00610018 0.00641703 0.00709746]\n [0.00700412 0.00676274 0.00721369]\n [0.00695167 0.00650032 0.00824275]\n [0.0067663  0.0070005  0.00831789]\n [0.00610877 0.00658537 0.00737545]\n [0.00651443 0.00723437 0.00761775]\n [0.00684701 0.0065596  0.00835854]\n [0.00661471 0.00643779 0.00712841]\n [0.00698338 0.00680511 0.00803024]\n [0.00620263 0.00630737 0.00822094]\n [0.00674665 0.0065596  0.00817791]\n [0.0074463  0.00700618 0.00771027]\n [0.00708345 0.00715479 0.0074697 ]\n [0.00687267 0.00655061 0.00761794]\n [0.00629791 0.00706436 0.00760092]\n [0.00620005 0.00685151 0.00727748]\n [0.0070375  0.00701129 0.00744597]\n [0.008103   0.00699885 0.00754271]\n [0.00687267 0.00690821 0.00746167]\n [0.00677619 0.00705908 0.00746167]\n [0.00619145 0.00688309 0.00758593]\n [0.00678953 0.0073152  0.0084154 ]\n [0.00702461 0.00684735 0.00733935]\n [0.00655843 0.00732851 0.00760092]\n [0.00705403 0.00688309 0.00730844]\n [0.00661229 0.00674296 0.00733935]\n [0.00614135 0.00702664 0.00731178]\n [0.00709208 0.00675858 0.00735505]\n [0.00614994 0.00643995 0.00708946]\n [0.00601107 0.00647691 0.00762927]\n [0.00620005 0.00675858 0.00799258]\n [0.00640997 0.00643995 0.00779193]\n [0.00701388 0.00692007 0.00763616]\n [0.00634547 0.00673281 0.00759597]\n [0.00684701 0.00685151 0.00735505]\n [0.00711299 0.00643518 0.00836832]\n [0.0070375  0.00652278 0.00803683]\n [0.00617049 0.00630737 0.00746167]\n [0.0069518  0.00665809 0.00734294]\n [0.00679429 0.00644256 0.00708946]\n [0.00708345 0.00658953 0.00730844]\n [0.00661182 0.00651862 0.00749359]\n [0.00722939 0.0066319  0.00727748]\n [0.00715247 0.00692355 0.00734294]\n [0.00673415 0.00655061 0.00730844]\n [0.00623546 0.00643779 0.00747319]\n [0.00662461 0.00679267 0.00809008]\n [0.00729841 0.00702664 0.0083683 ]\n [0.00709391 0.00701129 0.00771027]\n [0.00661229 0.00680511 0.00698877]\n [0.0071738  0.00735742 0.00747713]\n [0.00643969 0.00685151 0.0083344 ]\n [0.00638584 0.00701697 0.00830617]\n [0.00661229 0.00708033 0.00754271]\n [0.00722958 0.00712104 0.00875576]\n [0.00689271 0.00657638 0.00744597]\n [0.00676387 0.00692355 0.00733601]\n [0.00705403 0.0073152  0.00732748]\n [0.00771437 0.00650931 0.00759597]\n [0.00638584 0.00641842 0.00749359]\n [0.00629172 0.00659134 0.00832773]\n [0.00706704 0.0069344  0.00754271]\n [0.00708345 0.00651801 0.00831789]\n [0.00634183 0.00644256 0.00752725]\n [0.00626819 0.00658054 0.00733935]\n [0.00711299 0.00752109 0.00731178]\n [0.00700412 0.00705869 0.00727748]\n [0.00684943 0.00655061 0.00698877]\n [0.00698338 0.00644256 0.00754765]\n [0.00727716 0.00675858 0.00807826]\n [0.00663133 0.00641842 0.00703719]\n [0.00696169 0.00689741 0.00704904]\n [0.00678953 0.00686584 0.00751113]\n [0.00614135 0.00651862 0.0074575 ]\n [0.00614135 0.00642318 0.00734294]\n [0.00655843 0.0065596  0.00823935]\n [0.00696015 0.00651385 0.00769325]\n [0.00709391 0.0068313  0.00714746]\n [0.00718201 0.00689741 0.00744022]\n [0.00739053 0.00658537 0.00763999]\n [0.00620005 0.00690821 0.00768333]\n [0.00787058 0.00650931 0.00758785]\n [0.00634183 0.0071544  0.00785326]\n [0.00651443 0.00679267 0.00709746]\n [0.00668753 0.0069556  0.00746167]\n [0.00629174 0.00658537 0.00771219]\n [0.00719896 0.00658953 0.00803683]\n [0.00598271 0.00627553 0.00748207]\n [0.00651443 0.00667969 0.00803683]\n [0.00605224 0.00630875 0.00748207]\n [0.00598271 0.00627553 0.00761794]\n [0.00655843 0.00658537 0.00734788]\n [0.00671002 0.00688144 0.00803024]\n [0.00697005 0.00721675 0.00730505]\n [0.00704796 0.00701129 0.00823935]\n [0.00661229 0.00679267 0.0084154 ]\n [0.00629174 0.0068525  0.00727748]\n [0.00624405 0.00689577 0.00748865]\n [0.00772116 0.00719215 0.00735505]\n [0.0070375  0.00650931 0.00731178]\n [0.00619145 0.0071544  0.00749359]\n [0.00609769 0.00649771 0.00758593]\n [0.00676387 0.00690821 0.00803683]\n [0.0077308  0.00727173 0.00813413]]",
          "upper": "[[0.98392031 0.97814347 0.98528377]\n [0.98362145 0.9776207  0.98561335]\n [0.98339189 0.97805844 0.98643223]\n [0.98417799 0.97813189 0.98551888]\n [0.98149619 0.97867139 0.98546742]\n [0.98386927 0.97760379 0.98589986]\n [0.98292656 0.97676541 0.9856763 ]\n [0.9822199  0.9776207  0.98516472]\n [0.98144126 0.96691866 0.98628529]\n [0.98339189 0.97760379 0.98728511]\n [0.98386483 0.97848128 0.98728511]\n [0.9844267  0.97854431 0.98646903]\n [0.98287683 0.97762007 0.98750401]\n [0.98402835 0.97738671 0.9856424 ]\n [0.9844267  0.97813189 0.98564725]\n [0.98209657 0.97746124 0.98549446]\n [0.98143334 0.97828011 0.98507224]\n [0.98332132 0.97823164 0.98643121]\n [0.98328671 0.97847022 0.98643121]\n [0.98296096 0.97804154 0.98652846]\n [0.98166409 0.97828011 0.98572066]\n [0.9821094  0.97996806 0.98652846]\n [0.98304575 0.97760379 0.98693413]\n [0.9842184  0.97743041 0.9866743 ]\n [0.98062642 0.97834314 0.98549189]\n [0.98386927 0.97854431 0.98426245]\n [0.98417564 0.97828011 0.98575253]\n [0.98326459 0.97600599 0.98642366]\n [0.98134656 0.97874066 0.98052441]\n [0.98135594 0.98002647 0.98540597]\n [0.98177169 0.9782964  0.9855029 ]\n [0.9822199  0.98002647 0.98566065]\n [0.98392031 0.97375184 0.98728511]\n [0.98327391 0.97805782 0.98572518]\n [0.98402706 0.97829702 0.98534522]\n [0.98202551 0.9798988  0.98538367]\n [0.98351241 0.97805844 0.98566416]\n [0.98388532 0.97829702 0.98544056]\n [0.98339189 0.98190246 0.98563776]\n [0.98314407 0.97829702 0.98539613]\n [0.98407204 0.97979904 0.98537094]\n [0.98402263 0.97622307 0.98536805]\n [0.98403705 0.97738671 0.98600663]\n [0.98403705 0.9798988  0.98749333]\n [0.98437983 0.97979904 0.98638222]\n [0.98332132 0.97769793 0.98643121]\n [0.98351095 0.97847022 0.98535821]\n [0.9835329  0.97456648 0.9856763 ]\n [0.98295431 0.97874066 0.98638686]\n [0.98402835 0.97813189 0.98646801]\n [0.98386483 0.97395705 0.98728511]\n [0.98277119 0.97834314 0.98545397]\n [0.98139478 0.9733366  0.9856763 ]\n [0.98320533 0.97744496 0.98538333]\n [0.98383023 0.97829702 0.98615287]\n [0.98304575 0.97630494 0.98571432]\n [0.98344182 0.97650735 0.98572416]\n [0.98328671 0.9733366  0.9856424 ]\n [0.98284381 0.97867139 0.9855649 ]\n [0.98172879 0.98225881 0.98722703]\n [0.98059361 0.98038281 0.98571082]\n [0.98388976 0.97744496 0.98492491]\n [0.98326439 0.97746715 0.98554084]\n [0.9822199  0.97725007 0.98646903]\n [0.98392474 0.9782964  0.98601126]\n [0.98417435 0.98002647 0.98604578]\n [0.98311632 0.9743761  0.98550787]\n [0.98392474 0.97776095 0.98483909]\n [0.98292656 0.97972497 0.98554373]\n [0.98358926 0.97676757 0.9854295 ]\n [0.98339189 0.97677559 0.98643121]\n [0.98062059 0.97420783 0.98528377]\n [0.98402706 0.97834464 0.9853944 ]\n [0.98326459 0.97718368 0.98562675]\n [0.98433707 0.97730832 0.98551888]\n [0.98261121 0.97762007 0.9856424 ]\n [0.98401656 0.97854431 0.9866375 ]\n [0.98392031 0.97746715 0.98564725]\n [0.98356789 0.973901   0.98715009]\n [0.98165617 0.97847022 0.98738428]\n [0.98335122 0.97664975 0.98605664]\n [0.9818768  0.97847022 0.98668499]\n [0.98164066 0.97867139 0.98532189]\n [0.9835329  0.97829702 0.98643121]\n [0.98388532 0.97805782 0.98605664]\n [0.9828529  0.96814223 0.98572066]\n [0.9816449  0.97756129 0.98572518]\n [0.98209657 0.97823164 0.98750401]\n [0.98423346 0.97996806 0.98551888]\n [0.98401656 0.97678666 0.9856424 ]\n [0.98390242 0.97776095 0.9856798 ]\n [0.98202551 0.97867139 0.98598221]\n [0.98417435 0.97762007 0.98535821]\n [0.9835329  0.97745908 0.98571432]\n [0.98292656 0.97197366 0.98571082]\n [0.98388976 0.97867139 0.98638686]\n [0.98314407 0.97677559 0.98571082]\n [0.98143334 0.97867139 0.98541211]\n [0.98314407 0.97746124 0.98496368]\n [0.98339189 0.97769793 0.98571082]\n [0.9835329  0.97787016 0.98544056]\n [0.97978323 0.97874066 0.98600663]\n [0.98165617 0.98225881 0.98643223]\n [0.98407204 0.97762007 0.98563776]\n [0.98143334 0.9776207  0.98521088]\n [0.98403705 0.97762007 0.98605664]\n [0.9830735  0.97708681 0.98571082]\n [0.98398195 0.97867139 0.98540886]\n [0.9834778  0.97329568 0.98549446]\n [0.98165617 0.97771484 0.98684864]\n [0.98127335 0.9774428  0.98703036]\n [0.98392031 0.97837047 0.98537094]\n [0.98144126 0.97730832 0.98558113]\n [0.98403705 0.97677943 0.98692327]\n [0.98383023 0.97996806 0.98652846]\n [0.98407204 0.98160159 0.98506229]\n [0.98433578 0.98177479 0.98566065]\n [0.98328671 0.97769793 0.98646801]\n [0.98417564 0.97854431 0.98572416]\n [0.98284381 0.9769264  0.9856763 ]\n [0.98332132 0.97746187 0.98679008]\n [0.98417564 0.97823164 0.98549189]\n [0.98350651 0.97677943 0.98561335]\n [0.98277911 0.97736415 0.9871394 ]\n [0.98351241 0.97315144 0.98728511]\n [0.98166409 0.97778829 0.98572066]\n [0.9835329  0.97275715 0.98511489]\n [0.98332132 0.97849757 0.98506229]\n [0.98388976 0.97762007 0.98571082]\n [0.98402706 0.97837047 0.98549446]\n [0.98142177 0.98177479 0.98604578]\n [0.98069501 0.97683383 0.98552352]\n [0.98172717 0.97867139 0.98643223]\n [0.98386483 0.97664975 0.98571082]\n [0.98341238 0.97667147 0.98554373]\n [0.98195266 0.97746715 0.98643223]\n [0.98142206 0.97813189 0.98542484]\n [0.98219215 0.9776207  0.98774179]\n [0.98142206 0.98158469 0.98604578]\n [0.98314407 0.97857164 0.98763904]\n [0.9822199  0.97676757 0.98774179]\n [0.98295431 0.98002647 0.98643223]\n [0.98120723 0.97275715 0.98567517]\n [0.98402263 0.97837047 0.98566416]\n [0.98166409 0.97771421 0.98535821]\n [0.98124607 0.97771484 0.98571082]\n [0.98433578 0.97879906 0.98643121]\n [0.98315409 0.98190246 0.98652846]\n [0.98388532 0.97776095 0.9848052 ]\n [0.98417564 0.97766681 0.9856763 ]\n [0.98203679 0.97688854 0.98643223]\n [0.9819412  0.97708681 0.98668499]\n [0.98144126 0.98184406 0.98534522]\n [0.98294968 0.97776095 0.98432378]]",
          "level": 0.95
        },
        "bootstrap_predictions": "[[[0.01397901 0.01195694 0.97406406]\n  [0.95382363 0.02179573 0.02438067]\n  [0.98356789 0.00883616 0.00759597]\n  ...\n  [0.00870771 0.00820246 0.98308984]\n  [0.00784253 0.01190919 0.9802483 ]\n  [0.00877927 0.97879906 0.01242167]]\n\n [[0.97857083 0.01088089 0.01054829]\n  [0.98402835 0.00700618 0.00896545]\n  [0.00866541 0.0112449  0.98008968]\n  ...\n  [0.97380613 0.01789113 0.00830276]\n  [0.95182706 0.02236372 0.0258092 ]\n  [0.00959082 0.01104204 0.97936715]]\n\n [[0.00997736 0.01629136 0.97373128]\n  [0.97857083 0.01088089 0.01054829]\n  [0.01037871 0.97746187 0.0121594 ]\n  ...\n  [0.00939555 0.02051443 0.97009003]\n  [0.00826021 0.00796837 0.98377143]\n  [0.00985395 0.02078005 0.96936599]]\n\n ...\n\n [[0.00928627 0.01072965 0.97998409]\n  [0.01037871 0.97746187 0.0121594 ]\n  [0.98007033 0.01123426 0.0086954 ]\n  ...\n  [0.00857077 0.00785226 0.98357698]\n  [0.02017441 0.9592599  0.0205657 ]\n  [0.00889976 0.01587016 0.97523009]]\n\n [[0.00896779 0.00858411 0.98244812]\n  [0.98036427 0.00935076 0.01028498]\n  [0.94979683 0.02723708 0.02296609]\n  ...\n  [0.0082731  0.01652537 0.97520153]\n  [0.97922242 0.01165381 0.00912378]\n  [0.00826021 0.00796837 0.98377143]]\n\n [[0.0083861  0.00969872 0.98191517]\n  [0.00985395 0.02078005 0.96936599]\n  [0.98039019 0.01090957 0.00870026]\n  ...\n  [0.00866541 0.0112449  0.98008968]\n  [0.98065267 0.01131047 0.00803683]\n  [0.9703315  0.01982959 0.00983887]]]"
      },
      "success": true
    },
    "model_validation": {
      "validation_results": {
        "time_series_cv": {
          "scores": [
            1.0,
            1.0,
            1.0
          ],
          "train_scores": [
            1.0,
            1.0,
            1.0
          ],
          "overfitting_gaps": [
            0.0,
            0.0,
            0.0
          ],
          "fold_details": [
            {
              "fold": 1,
              "train_size": 194,
              "val_size": 191,
              "train_accuracy": 1.0,
              "val_accuracy": 1.0,
              "train_logloss": 0.021131832577601987,
              "val_logloss": 0.05906946057005878,
              "overfitting_gap": 0.0,
              "success": true
            },
            {
              "fold": 2,
              "train_size": 385,
              "val_size": 191,
              "train_accuracy": 1.0,
              "val_accuracy": 1.0,
              "train_logloss": 0.010346159414796765,
              "val_logloss": 0.03955406941464675,
              "overfitting_gap": 0.0,
              "success": true
            },
            {
              "fold": 3,
              "train_size": 576,
              "val_size": 191,
              "train_accuracy": 1.0,
              "val_accuracy": 1.0,
              "train_logloss": 0.008813937554395645,
              "val_logloss": 0.02754603049793664,
              "overfitting_gap": 0.0,
              "success": true
            }
          ],
          "successful_folds": 3,
          "failed_folds": 0,
          "mean_score": 1.0,
          "std_score": 0.0,
          "mean_overfitting": 0.0,
          "max_overfitting": 0.0,
          "coefficient_of_variation": 0.0
        },
        "walk_forward": {
          "scores": [
            1.0,
            1.0,
            1.0
          ],
          "train_sizes": [
            191,
            382,
            573
          ],
          "val_sizes": [
            191,
            191,
            191
          ],
          "fold_details": [
            {
              "fold": 1,
              "train_size": 191,
              "val_size": 191,
              "val_accuracy": 1.0,
              "val_logloss": 0.06264508402232245,
              "success": true
            },
            {
              "fold": 2,
              "train_size": 382,
              "val_size": 191,
              "val_accuracy": 1.0,
              "val_logloss": 0.04479573792018792,
              "success": true
            },
            {
              "fold": 3,
              "train_size": 573,
              "val_size": 191,
              "val_accuracy": 1.0,
              "val_logloss": 0.029647262044423264,
              "success": true
            }
          ],
          "successful_folds": 3,
          "failed_folds": 0,
          "mean_score": 1.0,
          "std_score": 0.0,
          "coefficient_of_variation": 0.0
        },
        "bootstrap": {
          "scores": [
            1.0,
            1.0,
            1.0,
            1.0,
            1.0,
            1.0,
            1.0,
            1.0,
            1.0,
            1.0,
            1.0,
            1.0,
            1.0,
            1.0,
            1.0,
            1.0,
            1.0,
            1.0,
            1.0,
            1.0,
            1.0,
            1.0,
            1.0,
            1.0,
            1.0,
            1.0,
            1.0,
            1.0,
            1.0,
            1.0,
            1.0,
            1.0,
            1.0,
            1.0,
            1.0,
            1.0,
            1.0,
            1.0,
            1.0,
            1.0,
            1.0,
            1.0,
            1.0,
            1.0,
            1.0,
            1.0,
            1.0,
            1.0,
            1.0,
            1.0
          ],
          "out_of_bag_scores": [
            1.0,
            1.0,
            1.0,
            1.0,
            1.0,
            1.0,
            1.0,
            1.0,
            1.0,
            1.0,
            1.0,
            1.0,
            1.0,
            1.0,
            1.0,
            1.0,
            1.0,
            1.0,
            1.0,
            1.0,
            1.0,
            1.0,
            1.0,
            1.0,
            1.0,
            1.0,
            1.0,
            1.0,
            1.0,
            1.0,
            1.0,
            1.0,
            1.0,
            1.0,
            1.0,
            1.0,
            1.0,
            1.0,
            1.0,
            1.0,
            1.0,
            1.0,
            1.0,
            1.0,
            1.0,
            1.0,
            1.0,
            1.0,
            1.0,
            1.0
          ],
          "bootstrap_details": [
            {
              "bootstrap": 1,
              "bootstrap_size": 613,
              "oob_size": 345,
              "bootstrap_accuracy": 1.0,
              "oob_accuracy": 1.0,
              "bootstrap_logloss": 0.006685534073050898,
              "oob_logloss": 0.03147659884521001,
              "success": true
            },
            {
              "bootstrap": 2,
              "bootstrap_size": 613,
              "oob_size": 346,
              "bootstrap_accuracy": 1.0,
              "oob_accuracy": 1.0,
              "bootstrap_logloss": 0.006379906260783054,
              "oob_logloss": 0.032451086159346464,
              "success": true
            },
            {
              "bootstrap": 3,
              "bootstrap_size": 613,
              "oob_size": 339,
              "bootstrap_accuracy": 1.0,
              "oob_accuracy": 1.0,
              "bootstrap_logloss": 0.005485896072064275,
              "oob_logloss": 0.029033622269946362,
              "success": true
            },
            {
              "bootstrap": 4,
              "bootstrap_size": 613,
              "oob_size": 368,
              "bootstrap_accuracy": 1.0,
              "oob_accuracy": 1.0,
              "bootstrap_logloss": 0.004794836822614417,
              "oob_logloss": 0.026829394441652644,
              "success": true
            },
            {
              "bootstrap": 5,
              "bootstrap_size": 613,
              "oob_size": 335,
              "bootstrap_accuracy": 1.0,
              "oob_accuracy": 1.0,
              "bootstrap_logloss": 0.006169583476005362,
              "oob_logloss": 0.027362578670141247,
              "success": true
            },
            {
              "bootstrap": 6,
              "bootstrap_size": 613,
              "oob_size": 342,
              "bootstrap_accuracy": 1.0,
              "oob_accuracy": 1.0,
              "bootstrap_logloss": 0.005727426619907035,
              "oob_logloss": 0.022372165210075314,
              "success": true
            },
            {
              "bootstrap": 7,
              "bootstrap_size": 613,
              "oob_size": 346,
              "bootstrap_accuracy": 1.0,
              "oob_accuracy": 1.0,
              "bootstrap_logloss": 0.005893320386796385,
              "oob_logloss": 0.02625457251529012,
              "success": true
            },
            {
              "bootstrap": 8,
              "bootstrap_size": 613,
              "oob_size": 352,
              "bootstrap_accuracy": 1.0,
              "oob_accuracy": 1.0,
              "bootstrap_logloss": 0.006551393926681578,
              "oob_logloss": 0.030328685527397915,
              "success": true
            },
            {
              "bootstrap": 9,
              "bootstrap_size": 613,
              "oob_size": 343,
              "bootstrap_accuracy": 1.0,
              "oob_accuracy": 1.0,
              "bootstrap_logloss": 0.005885477321385654,
              "oob_logloss": 0.026093860295858688,
              "success": true
            },
            {
              "bootstrap": 10,
              "bootstrap_size": 613,
              "oob_size": 354,
              "bootstrap_accuracy": 1.0,
              "oob_accuracy": 1.0,
              "bootstrap_logloss": 0.005820233407203623,
              "oob_logloss": 0.023927759538384472,
              "success": true
            },
            {
              "bootstrap": 11,
              "bootstrap_size": 613,
              "oob_size": 341,
              "bootstrap_accuracy": 1.0,
              "oob_accuracy": 1.0,
              "bootstrap_logloss": 0.005816605058983332,
              "oob_logloss": 0.029290800083548677,
              "success": true
            },
            {
              "bootstrap": 12,
              "bootstrap_size": 613,
              "oob_size": 347,
              "bootstrap_accuracy": 1.0,
              "oob_accuracy": 1.0,
              "bootstrap_logloss": 0.006179358114290325,
              "oob_logloss": 0.028111720580322526,
              "success": true
            },
            {
              "bootstrap": 13,
              "bootstrap_size": 613,
              "oob_size": 355,
              "bootstrap_accuracy": 1.0,
              "oob_accuracy": 1.0,
              "bootstrap_logloss": 0.005085954693518423,
              "oob_logloss": 0.02328409274861756,
              "success": true
            },
            {
              "bootstrap": 14,
              "bootstrap_size": 613,
              "oob_size": 346,
              "bootstrap_accuracy": 1.0,
              "oob_accuracy": 1.0,
              "bootstrap_logloss": 0.007029452550597916,
              "oob_logloss": 0.031646350246810555,
              "success": true
            },
            {
              "bootstrap": 15,
              "bootstrap_size": 613,
              "oob_size": 343,
              "bootstrap_accuracy": 1.0,
              "oob_accuracy": 1.0,
              "bootstrap_logloss": 0.005471692784467059,
              "oob_logloss": 0.020064530758840828,
              "success": true
            },
            {
              "bootstrap": 16,
              "bootstrap_size": 613,
              "oob_size": 346,
              "bootstrap_accuracy": 1.0,
              "oob_accuracy": 1.0,
              "bootstrap_logloss": 0.006125025040280944,
              "oob_logloss": 0.0222901030355215,
              "success": true
            },
            {
              "bootstrap": 17,
              "bootstrap_size": 613,
              "oob_size": 355,
              "bootstrap_accuracy": 1.0,
              "oob_accuracy": 1.0,
              "bootstrap_logloss": 0.00666438412283261,
              "oob_logloss": 0.02931969629233512,
              "success": true
            },
            {
              "bootstrap": 18,
              "bootstrap_size": 613,
              "oob_size": 342,
              "bootstrap_accuracy": 1.0,
              "oob_accuracy": 1.0,
              "bootstrap_logloss": 0.00572682186742734,
              "oob_logloss": 0.03103975755281476,
              "success": true
            },
            {
              "bootstrap": 19,
              "bootstrap_size": 613,
              "oob_size": 347,
              "bootstrap_accuracy": 1.0,
              "oob_accuracy": 1.0,
              "bootstrap_logloss": 0.005182895245357778,
              "oob_logloss": 0.025258625100605024,
              "success": true
            },
            {
              "bootstrap": 20,
              "bootstrap_size": 613,
              "oob_size": 337,
              "bootstrap_accuracy": 1.0,
              "oob_accuracy": 1.0,
              "bootstrap_logloss": 0.006581149288444765,
              "oob_logloss": 0.026586299588433452,
              "success": true
            },
            {
              "bootstrap": 21,
              "bootstrap_size": 613,
              "oob_size": 351,
              "bootstrap_accuracy": 1.0,
              "oob_accuracy": 1.0,
              "bootstrap_logloss": 0.006281422004902439,
              "oob_logloss": 0.027646842703214613,
              "success": true
            },
            {
              "bootstrap": 22,
              "bootstrap_size": 613,
              "oob_size": 342,
              "bootstrap_accuracy": 1.0,
              "oob_accuracy": 1.0,
              "bootstrap_logloss": 0.00568785689552868,
              "oob_logloss": 0.028527389873623472,
              "success": true
            },
            {
              "bootstrap": 23,
              "bootstrap_size": 613,
              "oob_size": 346,
              "bootstrap_accuracy": 1.0,
              "oob_accuracy": 1.0,
              "bootstrap_logloss": 0.005758582469581023,
              "oob_logloss": 0.02707141022332031,
              "success": true
            },
            {
              "bootstrap": 24,
              "bootstrap_size": 613,
              "oob_size": 346,
              "bootstrap_accuracy": 1.0,
              "oob_accuracy": 1.0,
              "bootstrap_logloss": 0.006116823871439057,
              "oob_logloss": 0.03057374907872713,
              "success": true
            },
            {
              "bootstrap": 25,
              "bootstrap_size": 613,
              "oob_size": 333,
              "bootstrap_accuracy": 1.0,
              "oob_accuracy": 1.0,
              "bootstrap_logloss": 0.0062683588251875155,
              "oob_logloss": 0.02267927770564697,
              "success": true
            },
            {
              "bootstrap": 26,
              "bootstrap_size": 613,
              "oob_size": 343,
              "bootstrap_accuracy": 1.0,
              "oob_accuracy": 1.0,
              "bootstrap_logloss": 0.005682766305261538,
              "oob_logloss": 0.027348913133397795,
              "success": true
            },
            {
              "bootstrap": 27,
              "bootstrap_size": 613,
              "oob_size": 341,
              "bootstrap_accuracy": 1.0,
              "oob_accuracy": 1.0,
              "bootstrap_logloss": 0.005814703128446733,
              "oob_logloss": 0.027764369351022142,
              "success": true
            },
            {
              "bootstrap": 28,
              "bootstrap_size": 613,
              "oob_size": 351,
              "bootstrap_accuracy": 1.0,
              "oob_accuracy": 1.0,
              "bootstrap_logloss": 0.005615239810586684,
              "oob_logloss": 0.024058976680042615,
              "success": true
            },
            {
              "bootstrap": 29,
              "bootstrap_size": 613,
              "oob_size": 348,
              "bootstrap_accuracy": 1.0,
              "oob_accuracy": 1.0,
              "bootstrap_logloss": 0.006867592416383635,
              "oob_logloss": 0.029838590496837167,
              "success": true
            },
            {
              "bootstrap": 30,
              "bootstrap_size": 613,
              "oob_size": 350,
              "bootstrap_accuracy": 1.0,
              "oob_accuracy": 1.0,
              "bootstrap_logloss": 0.005235661733001682,
              "oob_logloss": 0.02226989349661187,
              "success": true
            },
            {
              "bootstrap": 31,
              "bootstrap_size": 613,
              "oob_size": 347,
              "bootstrap_accuracy": 1.0,
              "oob_accuracy": 1.0,
              "bootstrap_logloss": 0.0059530766083124,
              "oob_logloss": 0.0323878355127989,
              "success": true
            },
            {
              "bootstrap": 32,
              "bootstrap_size": 613,
              "oob_size": 349,
              "bootstrap_accuracy": 1.0,
              "oob_accuracy": 1.0,
              "bootstrap_logloss": 0.006022964037093902,
              "oob_logloss": 0.029203475092036073,
              "success": true
            },
            {
              "bootstrap": 33,
              "bootstrap_size": 613,
              "oob_size": 337,
              "bootstrap_accuracy": 1.0,
              "oob_accuracy": 1.0,
              "bootstrap_logloss": 0.005752645525922664,
              "oob_logloss": 0.024059583861648397,
              "success": true
            },
            {
              "bootstrap": 34,
              "bootstrap_size": 613,
              "oob_size": 358,
              "bootstrap_accuracy": 1.0,
              "oob_accuracy": 1.0,
              "bootstrap_logloss": 0.004785273407411799,
              "oob_logloss": 0.026279772580940036,
              "success": true
            },
            {
              "bootstrap": 35,
              "bootstrap_size": 613,
              "oob_size": 347,
              "bootstrap_accuracy": 1.0,
              "oob_accuracy": 1.0,
              "bootstrap_logloss": 0.006750953709304914,
              "oob_logloss": 0.028784715952768408,
              "success": true
            },
            {
              "bootstrap": 36,
              "bootstrap_size": 613,
              "oob_size": 356,
              "bootstrap_accuracy": 1.0,
              "oob_accuracy": 1.0,
              "bootstrap_logloss": 0.005424245745177371,
              "oob_logloss": 0.03217819628177859,
              "success": true
            },
            {
              "bootstrap": 37,
              "bootstrap_size": 613,
              "oob_size": 352,
              "bootstrap_accuracy": 1.0,
              "oob_accuracy": 1.0,
              "bootstrap_logloss": 0.006288083377627122,
              "oob_logloss": 0.030466264998673984,
              "success": true
            },
            {
              "bootstrap": 38,
              "bootstrap_size": 613,
              "oob_size": 355,
              "bootstrap_accuracy": 1.0,
              "oob_accuracy": 1.0,
              "bootstrap_logloss": 0.00591066591577865,
              "oob_logloss": 0.026963688826316338,
              "success": true
            },
            {
              "bootstrap": 39,
              "bootstrap_size": 613,
              "oob_size": 347,
              "bootstrap_accuracy": 1.0,
              "oob_accuracy": 1.0,
              "bootstrap_logloss": 0.006712148172186245,
              "oob_logloss": 0.03217976770284368,
              "success": true
            },
            {
              "bootstrap": 40,
              "bootstrap_size": 613,
              "oob_size": 343,
              "bootstrap_accuracy": 1.0,
              "oob_accuracy": 1.0,
              "bootstrap_logloss": 0.005979196334973484,
              "oob_logloss": 0.02591406040735959,
              "success": true
            },
            {
              "bootstrap": 41,
              "bootstrap_size": 613,
              "oob_size": 354,
              "bootstrap_accuracy": 1.0,
              "oob_accuracy": 1.0,
              "bootstrap_logloss": 0.005636088173570969,
              "oob_logloss": 0.030255583498600542,
              "success": true
            },
            {
              "bootstrap": 42,
              "bootstrap_size": 613,
              "oob_size": 360,
              "bootstrap_accuracy": 1.0,
              "oob_accuracy": 1.0,
              "bootstrap_logloss": 0.005680188493507964,
              "oob_logloss": 0.027629820571992893,
              "success": true
            },
            {
              "bootstrap": 43,
              "bootstrap_size": 613,
              "oob_size": 341,
              "bootstrap_accuracy": 1.0,
              "oob_accuracy": 1.0,
              "bootstrap_logloss": 0.007314746969386564,
              "oob_logloss": 0.02979496239620961,
              "success": true
            },
            {
              "bootstrap": 44,
              "bootstrap_size": 613,
              "oob_size": 338,
              "bootstrap_accuracy": 1.0,
              "oob_accuracy": 1.0,
              "bootstrap_logloss": 0.006924432258886162,
              "oob_logloss": 0.030443160596624233,
              "success": true
            },
            {
              "bootstrap": 45,
              "bootstrap_size": 613,
              "oob_size": 334,
              "bootstrap_accuracy": 1.0,
              "oob_accuracy": 1.0,
              "bootstrap_logloss": 0.006344577511718186,
              "oob_logloss": 0.02272482269414457,
              "success": true
            },
            {
              "bootstrap": 46,
              "bootstrap_size": 613,
              "oob_size": 351,
              "bootstrap_accuracy": 1.0,
              "oob_accuracy": 1.0,
              "bootstrap_logloss": 0.00540440861821513,
              "oob_logloss": 0.02446610222117215,
              "success": true
            },
            {
              "bootstrap": 47,
              "bootstrap_size": 613,
              "oob_size": 355,
              "bootstrap_accuracy": 1.0,
              "oob_accuracy": 1.0,
              "bootstrap_logloss": 0.006062210026294128,
              "oob_logloss": 0.02776574260517601,
              "success": true
            },
            {
              "bootstrap": 48,
              "bootstrap_size": 613,
              "oob_size": 346,
              "bootstrap_accuracy": 1.0,
              "oob_accuracy": 1.0,
              "bootstrap_logloss": 0.005488488548182014,
              "oob_logloss": 0.02984095300096755,
              "success": true
            },
            {
              "bootstrap": 49,
              "bootstrap_size": 613,
              "oob_size": 334,
              "bootstrap_accuracy": 1.0,
              "oob_accuracy": 1.0,
              "bootstrap_logloss": 0.006730643010387874,
              "oob_logloss": 0.031277985445868024,
              "success": true
            },
            {
              "bootstrap": 50,
              "bootstrap_size": 613,
              "oob_size": 367,
              "bootstrap_accuracy": 1.0,
              "oob_accuracy": 1.0,
              "bootstrap_logloss": 0.006147898107689532,
              "oob_logloss": 0.030228093975371302,
              "success": true
            }
          ],
          "successful_bootstraps": 50,
          "failed_bootstraps": 0,
          "mean_score": 1.0,
          "std_score": 0.0,
          "mean_oob_score": 1.0,
          "std_oob_score": 0.0,
          "optimism": 0.0
        },
        "calibration": {
          "original": {
            "brier_score": 0.002025974025974026,
            "reliability": 0.0,
            "sharpness": 0.20904473304473306,
            "calibration_error": 0.020779220779220786
          },
          "calibrated": {
            "brier_score": 1.0543760445201431e-05,
            "reliability": 0.0,
            "sharpness": 0.22201388948379752,
            "calibration_error": 0.00031777098785973346
          },
          "improvement": {
            "brier_score": 0.0020154302655288245,
            "calibration_error": 0.020461449791361053
          }
        },
        "overfitting": {
          "detected": false,
          "severity": "none",
          "metrics": {
            "mean_overfitting": 0.0,
            "max_overfitting": 0.0
          },
          "recommendations": []
        },
        "stability": {
          "stable": true,
          "metrics": {
            "coefficient_of_variation": 0.0,
            "std_score": 0.0,
            "walk_forward_cv": 0.0,
            "optimism": 0.0
          },
          "issues": []
        },
        "overall": {
          "validation_passed": true,
          "overall_score": 1.0,
          "issues": [],
          "recommendations": [
            "Model validation passed successfully"
          ]
        }
      },
      "report": "================================================================================\nNON-MAJOR LEAGUE MODEL VALIDATION REPORT\n================================================================================\n\nOVERALL ASSESSMENT:\n  Validation Status: PASSED\n  Overall Score: 1.0000\n\n  Recommendations:\n    - Model validation passed successfully\n\nDETAILED VALIDATION RESULTS:\n----------------------------------------\n\nTime Series Cross-Validation:\n  Successful folds: 3\n  Failed folds: 0\n  Mean accuracy: 1.0000\n  Std accuracy: 0.0000\n  Coefficient of variation: 0.0000\n  Mean overfitting: 0.0000\n  Max overfitting: 0.0000\n\nWalk-Forward Validation:\n  Successful folds: 3\n  Failed folds: 0\n  Mean accuracy: 1.0000\n  Std accuracy: 0.0000\n  Coefficient of variation: 0.0000\n\nBootstrap Validation:\n  Successful bootstraps: 50\n  Failed bootstraps: 0\n  Mean accuracy: 1.0000\n  Mean OOB accuracy: 1.0000\n  Optimism: 0.0000\n\nModel Calibration:\n  Original Brier score: 0.0020\n  Original calibration error: 0.0208\n  Calibrated Brier score: 0.0000\n  Calibrated calibration error: 0.0003\n  Brier score improvement: +0.0020\n  Calibration error improvement: +0.0205\n\nOverfitting Detection:\n  Overfitting detected: NO\n  Severity: none\n\nStability Analysis:\n  Model stable: YES\n\n================================================================================",
      "success": true
    }
  },
  "timestamp": "2025-10-16T23:39:24.465750"
}